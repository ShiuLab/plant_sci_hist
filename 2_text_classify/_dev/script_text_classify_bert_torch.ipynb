{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 2c: BERT models - pytorch__\n",
    "\n",
    "Follow:\n",
    "- [How to train a BERT model from scratch](https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6)\n",
    "- [Huggintface ByteLevelBPETokenizer tutorial](https://github.com/huggingface/blog/blob/main/how-to-train.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Establish the environment_\n",
    "\n",
    "This is the pytorch version.\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create -n bert_classify python=3.9\n",
    "conda activate bert_classify\n",
    "conda install ipykernel --update-deps --force-reinstall\n",
    "```\n",
    "\n",
    "In addition, install:\n",
    "- numpy, pandas, scikit-learn, ,transformers, tokenizers, datasets, pytorch, tensorflow-gpu\n",
    "\n",
    "Issues:\n",
    "- datasets cannot be installed through conda successfully. Did pip.\n",
    "- Python 3.10 as of 6/16/22 is not fully supported by pytorch and lead to the following error when calling `transfomer.Trainer`:\n",
    "  - TypeError: Instance and class checks can only be used with @runtime_checkable protocols\n",
    "- pytorch installed by default when transformers is installed but without CUDA support. following instructions from [here](https://pytorch.org/):\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Imports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For building text classification model based on embedding of Word2Vec and BERT\n",
    "'''\n",
    "\n",
    "## for data\n",
    "#import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "#import torch\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'bert_param':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, valid, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  corpus = corpus_combo[['label','txt','txt_clean']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  print(f\"    train={train.shape}, valid={valid.shape},\" +\\\n",
    "        f\" test={test.shape}\")\n",
    "\n",
    "  return [train, valid, test]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(w2v_param):\n",
    "  ''' Return a list with hyperparameters based on the passed dictionary\n",
    "  Adopted from:\n",
    "    https://stackoverflow.com/questions/38721847/how-to-generate-all-combination-from-values-in-dict-of-lists-in-python\n",
    "  Args:\n",
    "    param (dict): a dictionary specified in the config.txt file.\n",
    "  Return:\n",
    "    param_list (list): a nested list of hyperparameters in the order of\n",
    "      max_feature, ngram_range, and p_threshold\n",
    "  '''\n",
    "  print(w2v_param)\n",
    "  keys, values = zip(*w2v_param.items())\n",
    "  param_list = [v for v in itertools.product(*values)]\n",
    "  \n",
    "  return keys, param_list\n",
    "\n",
    "def get_unigram(corpus):\n",
    "  unigram = []\n",
    "  for txt in corpus:\n",
    "    lst_words = txt.split()\n",
    "    unigram.append(lst_words)\n",
    "\n",
    "  return unigram\n",
    "\n",
    "def get_ngram(X_train, X_valid, X_test, ngram):\n",
    "\n",
    "  uni_train = get_unigram(X_train)\n",
    "  uni_valid = get_unigram(X_valid)\n",
    "  uni_test  = get_unigram(X_test)\n",
    "\n",
    "  if ngram == 1:\n",
    "    return uni_train, uni_valid, uni_test\n",
    "  # ngram >1\n",
    "  else:\n",
    "    # Get bigrams\n",
    "    bigrams_detector  = gensim.models.phrases.Phrases(\n",
    "                          uni_train, delimiter=\" \", min_count=5, threshold=10)\n",
    "    bigrams_detector  = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "    bi_train = list(bigrams_detector[uni_train])\n",
    "    bi_valid = list(bigrams_detector[uni_valid])\n",
    "    bi_test  = list(bigrams_detector[uni_test])\n",
    "\n",
    "    # Return bigrams\n",
    "    if ngram == 2:\n",
    "      return bi_train, bi_valid, bi_test\n",
    "\n",
    "    # Get trigrams and return them\n",
    "    elif ngram == 3:\n",
    "      trigrams_detector = gensim.models.phrases.Phrases(\n",
    "                          bigrams_detector[uni_train], delimiter=\" \", \n",
    "                          min_count=5, threshold=10)\n",
    "      trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "      tri_train = list(trigrams_detector[bi_train])\n",
    "      tri_valid = list(trigrams_detector[bi_valid])\n",
    "      tri_test  = list(trigrams_detector[bi_test])\n",
    "      return tri_train, tri_valid, tri_test\n",
    "    \n",
    "    else:\n",
    "      print('ERR: ngram cannot be larger than 3. QUIT!')\n",
    "      sys.exit(0)\n",
    "\n",
    "\n",
    "def get_w2v_model(X_train, X_valid, X_test, param, rand_state):\n",
    "  '''Get ngram lists and w2v model\n",
    "  Args:\n",
    "  Return:\n",
    "  '''\n",
    "  [min_count, window, ngram] = param\n",
    "\n",
    "  ngram_train, ngram_valid, ngram_test = get_ngram(X_train, X_valid, X_test, \n",
    "                                                  ngram)\n",
    "\n",
    "  # Check if w2v model is already generated\n",
    "  model_w2v_name = work_dir / f\"model_cln_w2v_{min_count}-{window}-{ngram}\"\n",
    "\n",
    "  if model_w2v_name.is_file():\n",
    "    print(\"  load the w2v model\")\n",
    "    with open(work_dir / model_w2v_name, \"rb\") as f:\n",
    "        model_w2v = pickle.load(f)\n",
    "  else:\n",
    "    print(\"  geneate and save w2v model\")\n",
    "    model_w2v = gensim.models.Word2Vec(ngram_train, vector_size=300, \n",
    "                                      window=window, min_count=min_count, \n",
    "                                      sg=1, epochs=30, seed=rand_state)\n",
    "    \n",
    "    with open(model_w2v_name, \"wb\") as f:\n",
    "      pickle.dump(model_w2v, f)\n",
    "\n",
    "  return model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test\n",
    "\n",
    "\n",
    "def train_tokenizer(corpus):\n",
    "  '''Train a tokenizer\n",
    "  Args:\n",
    "    corpus (list): a nested list of word lists\n",
    "  Return:\n",
    "    tokenizer (keras.preprocessing.text.Tokenizer): trained tokenizer\n",
    "    dic_vocab_token (dict): token as key, index as value\n",
    "  '''\n",
    "\n",
    "  # intialize tokenizer\n",
    "  # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "  # This is replaced by tf.keras.layers.TextVectorization\n",
    "  tokenizer = preprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "  # tokenize corpus \n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "  # get token dictionary, with token as key, index number as value\n",
    "  dic_vocab_token = tokenizer.word_index\n",
    "\n",
    "  return tokenizer, dic_vocab_token\n",
    "\n",
    "\n",
    "def get_embeddings(corpus, model_w2v, tokenizer, dic_vocab_token):\n",
    "\n",
    "  # Transforms each text in texts to a sequence of integers.\n",
    "  lst_text2seq = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "  # pad or trucate sequence\n",
    "  X_w2v = preprocessing.sequence.pad_sequences(\n",
    "                    lst_text2seq,      # List of sequences, each a list of ints \n",
    "                    maxlen=500,        # maximum length of all sequences\n",
    "                    padding=\"post\",    # 'pre' or 'post' \n",
    "                    truncating=\"post\") # remove values from sequences > maxlen\n",
    "\n",
    "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "\n",
    "  embeddings = np.zeros((len(dic_vocab_token)+1, 300))\n",
    "  not_in_emb = {}\n",
    "  for word, idx in dic_vocab_token.items():\n",
    "      ## update the row with vector\n",
    "      try:\n",
    "          embeddings[idx] =  model_w2v.wv[word]\n",
    "      ## if word not in model then skip and the row stays all 0s\n",
    "      except KeyError:\n",
    "          not_in_emb[word] = 1\n",
    "\n",
    "  return embeddings, X_w2v\n",
    "\n",
    "\n",
    "def get_w2v_emb_model(embeddings):\n",
    "  '''Build a deep learning model with Word2Vec embeddings\n",
    "  Args:\n",
    "    embeddings\n",
    "  '''\n",
    "\n",
    "  ## code attention layer\n",
    "  def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "  ## input\n",
    "  x_in = layers.Input(shape=(500,)) ## embedding\n",
    "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                      output_dim=embeddings.shape[1], \n",
    "                      weights=[embeddings],\n",
    "                      input_length=500, trainable=False)(x_in)\n",
    "\n",
    "  ## apply attention\n",
    "  x = attention_layer(x, neurons=500)\n",
    "\n",
    "  ## 2 layers of bidirectional lstm\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                          return_sequences=True))(x)\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "  ## final dense layers\n",
    "  x = layers.Dense(64, activation='relu')(x)\n",
    "  y_out = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "  ## Initialize and compile model\n",
    "  model = models.Model(x_in, y_out)\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def run_main_function():\n",
    "\n",
    "  # get w2c parameter list\n",
    "  #   [min_count, window, ngram]\n",
    "  param_keys, param_list  = get_hyperparameters(w2v_param)\n",
    "\n",
    "  # iterate through different parameters\n",
    "  with open(work_dir / f\"scores_cln_w2v\", \"w\") as f:\n",
    "    f.write(\"run\\ttxt_flag\\tlang_model\\tparameters\\tvalidate_f1\\t\" +\\\n",
    "            \"test_f1\\tmodel_dir\\n\")\n",
    "    run_num = 0\n",
    "    for param in param_list:\n",
    "      print(f\"\\n## param: {param}\")\n",
    "      best_score, model_dir, test_score = run_pipeline(param, subsets)\n",
    "\n",
    "      f.write(f\"{run_num}\\tcln\\t{lang_model}\\t{str(param)}\\t\"+\\\n",
    "              f\"{best_score}\\t{test_score}\\t{model_dir}\\n\")\n",
    "\n",
    "      run_num += 1\n",
    "\n",
    "\n",
    "def run_pipeline(param, subsets):\n",
    "  '''Carry out the major steps'''\n",
    "\n",
    "  rand_state = config_dict['rand_state']\n",
    "\n",
    "  [X_train, X_valid, X_test, y_train, y_valid, y_test] = subsets\n",
    "\n",
    "  # Get list of ngrams and w2v model\n",
    "  print(\"  get list of ngrams and w2v model\")\n",
    "  model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test = \\\n",
    "                      get_w2v_model(X_train, X_valid, X_test, param, rand_state)\n",
    "  \n",
    "  # Train tokenizer\n",
    "  print(\"  train tokenizer\")\n",
    "  tokenizer, dic_vocab_token = train_tokenizer(ngram_train)\n",
    "\n",
    "  # Get embeddings\n",
    "  print(\"  get embeddings\")\n",
    "  embeddings, X_train_w2v = get_embeddings(ngram_train, model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "  _, X_valid_w2v = get_embeddings(ngram_valid, model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "  _ , X_test_w2v  = get_embeddings(ngram_test , model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "\n",
    "  # Model checkpoint path and output model file name\n",
    "  cp_filepath  = Path(str(model_w2v_name) + \"_dnn\")\n",
    "\n",
    "  # Load model if exists\n",
    "  if cp_filepath.is_dir():\n",
    "    print(\"  load model in:\", cp_filepath)\n",
    "    model_emb = models.load_model(cp_filepath)\n",
    "\n",
    "  # Train and save model if not\n",
    "  else:\n",
    "    print(\"  train model\")\n",
    "    model_emb    = get_w2v_emb_model(embeddings)\n",
    "\n",
    "    # setup check points\n",
    "    callback_es  = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callback_mcp = callbacks.ModelCheckpoint(filepath=cp_filepath, mode='max', \n",
    "            save_weights_only=False, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_emb.fit(x=X_train_w2v, y=y_train, batch_size=256, \n",
    "                            epochs=20, shuffle=True, verbose=1, \n",
    "                            validation_data=(X_valid_w2v, y_valid), \n",
    "                            callbacks=[callback_es, callback_mcp])\n",
    "\n",
    "  print(\"  get validation f1 score\")\n",
    "  y_valid_pred_prob = model_emb.predict(X_valid_w2v)\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_valid))}\n",
    "  y_valid_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_valid_pred_prob]\n",
    "  best_score = metrics.f1_score(y_valid, y_valid_pred)\n",
    "  print(\"    \", best_score)\n",
    "\n",
    "  print(\"  get testing f1 score\")\n",
    "  y_test_pred_prob = model_emb.predict(X_test_w2v)\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_test))}\n",
    "  y_test_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred_prob]\n",
    "  test_score = metrics.f1_score(y_test, y_test_pred)\n",
    "  print(\"    \", test_score)\n",
    "\n",
    "  # provide some space between runs\n",
    "  print('\\n')\n",
    "\n",
    "  return best_score, cp_filepath, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get training/testing split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read configuration file...\n",
      "   lang_model = bert\n",
      "   proj_dir = /home/shius/projects/plant_sci_hist\n",
      "   work_dir = 2_text_classify\n",
      "   corpus_combo_file = corpus_combo\n",
      "   rand_state = 20220609\n",
      "   bert_param = {}\n",
      "  all config available\n"
     ]
    }
   ],
   "source": [
    "config_file = Path(\"config_bert.txt\")\n",
    "\n",
    "print(\"\\nRead configuration file...\")\n",
    "config_dict = read_configs(config_file)\n",
    "\n",
    "# Set up working directory and corpus file location\n",
    "proj_dir          = Path(config_dict['proj_dir'])\n",
    "work_dir          = proj_dir / config_dict['work_dir']\n",
    "corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "\n",
    "# For reproducibility\n",
    "rand_state = config_dict['rand_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read file and split train/validate/test...\n",
      "    train=(51987, 3), valid=(17329, 3), test=(17330, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split train/validate/test for cleaned text\n",
    "#   Will not focus on original due to issues with non-alphanumeric characters\n",
    "#   and stop words.\n",
    "print(\"\\nRead file and split train/validate/test...\")\n",
    "[train, valid, test] = split_train_validate_test(corpus_combo_file, rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to Datasets\n",
    "dataset_train = Dataset.from_pandas(train)\n",
    "dataset_valid = Dataset.from_pandas(valid)\n",
    "dataset_test  = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'txt', 'txt_clean', '__index_level_0__'],\n",
       "    num_rows: 51987\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Tokenization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 52_000\n",
    "\n",
    "# maximum sequence length, lowering will result to faster training (when \n",
    "# increasing batch size)\n",
    "max_length = 512\n",
    "\n",
    "min_frequency=2\n",
    "\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Write training texts to files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training texts to a folder where each file has 5000 entries.\n",
    "corpus_train_path = work_dir / \"corpus_train\"\n",
    "corpus_train_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note that I use the original text for training tokenizer\n",
    "txts  = train['txt'].values\n",
    "\n",
    "# list of training corpus files\n",
    "files = []             \n",
    "for idx in range(0,len(txts),5000):\n",
    "  subset = txts[idx:idx+5000]\n",
    "  subset_file = corpus_train_path / f\"txt_{idx}\"\n",
    "\n",
    "  # force posix path to be string, otherwize the training step below will fail\n",
    "  files.append(str(subset_file))\n",
    "  with open(subset_file, \"w\") as f:\n",
    "    subset_txts = '\\n'.join(subset)\n",
    "    f.write(subset_txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Train tokeinzier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intialize and train tokenizer\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, min_frequency=min_frequency, \n",
    "                special_tokens=special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Save tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/shius/projects/plant_sci_hist/2_text_classify/model_bert/vocab.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = work_dir / \"model_bert\"\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the tokenizer  \n",
    "tokenizer.save_model(str(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(model_path / \"config.json\", \"w\") as f:\n",
    "    tokenizer_cfg = {\n",
    "        \"do_lower_case\": True,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"model_max_length\": max_length,\n",
    "        \"max_len\": max_length,}\n",
    "    json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is critical: the trained tokenizer object cannot be called directly.\n",
    "tokenizer_loaded = transformers.BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Text classification with transfer learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Set up train, valid, and test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here use pre-processed text data for classification\n",
    "X_train = dataset_train['txt_clean']\n",
    "y_train = dataset_train['label']\n",
    "X_valid = dataset_valid['txt_clean']\n",
    "y_valid = dataset_valid['label']\n",
    "X_test  = dataset_test['txt_clean']\n",
    "y_test  = dataset_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Encode corpus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to encode text data in batches\n",
    "def batch_encode(tokenizer, texts, batch_size=256):\n",
    "  \"\"\"\"\"\"\"\"\"\n",
    "  A function that encodes a batch of texts and returns the texts'\n",
    "  corresponding encodings and attention masks that are ready to be fed \n",
    "  into a pre-trained transformer model.\n",
    "  \n",
    "  Input:\n",
    "  - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "  - texts:       List of strings where each string represents a text\n",
    "  - batch_size:  Integer controlling number of texts in a batch\n",
    "  - max_length:  Integer controlling max number of words to tokenize in a\n",
    "    given text\n",
    "  Output:\n",
    "  - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "  - attention_mask: the texts' attention mask encoded as a tf.Tensor obj\n",
    "  \"\"\"\"\"\"\"\"\"\n",
    "  # Define the maximum number of words to tokenize (up to 512)\n",
    "  max_length = 50\n",
    "  input_ids = []\n",
    "  attention_mask = []\n",
    "  \n",
    "  for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    inputs = tokenizer.batch_encode_plus(batch,\n",
    "                                          max_length=max_length,\n",
    "                                          padding='max_length',\n",
    "                                          truncation=True,\n",
    "                                          return_attention_mask=True,\n",
    "                                          return_token_type_ids=False\n",
    "                                          )\n",
    "    input_ids.extend(inputs['input_ids'])\n",
    "    attention_mask.extend(inputs['attention_mask'])\n",
    "  \n",
    "  return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bc73294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 08:38:03.700413: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-17 08:38:03.700834: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-17 08:38:03.702798: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Encode corpus\n",
    "X_train_ids, X_train_attn = batch_encode(tokenizer_loaded, X_train)\n",
    "X_valid_ids, X_valid_attn = batch_encode(tokenizer_loaded, X_valid)\n",
    "X_test_ids , X_test_attn  = batch_encode(tokenizer_loaded, X_test)\n",
    "\n",
    "X_train_bert = [np.asarray(X_train_ids, dtype='int32'),\n",
    "                np.asarray(X_train_attn, dtype='int32')]\n",
    "X_valid_bert = [np.asarray(X_valid_ids, dtype='int32'),\n",
    "                np.asarray(X_valid_attn, dtype='int32')]\n",
    "X_test_bert  = [np.asarray(X_test_ids, dtype='int32'),\n",
    "                np.asarray(X_test_attn, dtype='int32')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2604605",
   "metadata": {},
   "source": [
    "### _Classification model training_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inputs\n",
    "idx = layers.Input((50), dtype=\"int32\", name=\"input_idx\")\n",
    "masks = layers.Input((50), dtype=\"int32\", name=\"input_masks\")\n",
    "\n",
    "## pre-trained bert with config\n",
    "config = transformers.DistilBertConfig(dropout=0.2, \n",
    "           attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "nlp = transformers.TFDistilBertModel.from_pretrained(\n",
    "                                    'distilbert-base-uncased', config=config)\n",
    "bert_out = nlp(idx, attention_mask=masks)[0]\n",
    "\n",
    "## fine-tuning\n",
    "x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "y_out = layers.Dense(len(np.unique(y_train)), \n",
    "                     activation='softmax')(x)\n",
    "                     \n",
    "## compile\n",
    "model = models.Model([idx, masks], y_out)\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "              \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y\n",
    "dic_y_mapping = {n:label for n,label in \n",
    "                 enumerate(np.unique(y_train_bert))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train_label_bert = np.array([inverse_dic[y] for y in y_train_bert])\n",
    "\n",
    "## train\n",
    "history = model.fit(x=X_train_bert, y=y_train_label_bert, batch_size=64, \n",
    "                     epochs=10, shuffle=True, verbose=1, \n",
    "                     validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Batch encode texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify_bert.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify_bert.ipynb#ch0000071vscode-remote?line=0'>1</a>\u001b[0m dataset_train\u001b[39m.\u001b[39;49mhead(\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "X_train = train['txt_clean']\n",
    "y_train = train['label']\n",
    "X_valid = valid['txt_clean']\n",
    "y_valid = valid['label']\n",
    "X_test  = train['txt_clean']\n",
    "y_test = train['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Text classification with retrained model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Tokenize dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226d4a4-df25-4210-a34a-258d1aef5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer_loaded(examples[\"txt\"], truncation=True, padding=\"max_length\", \n",
    "                            max_length=max_length, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function encode at 0x7f2c02128310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 52/52 [00:17<00:00,  2.94ba/s]\n",
      "100%|██████████| 18/18 [00:05<00:00,  3.16ba/s]\n",
      "100%|██████████| 18/18 [00:05<00:00,  3.28ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the subsets\n",
    "train_tokenized = dataset_train.map(encode, batched=True)\n",
    "valid_tokenized = dataset_valid.map(encode, batched=True)\n",
    "test_tokenized  = dataset_test.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fcaaa-3df9-49d9-9dfe-b7932ee989e6",
   "metadata": {},
   "source": [
    "### _Reformat columns_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat two columns into type `torch`:\n",
    "#  Setting to torch leda to trucation of longer samples\n",
    "train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "valid_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "test_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Load existing model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = transformers.BertConfig(vocab_size=vocab_size, \n",
    "                                       max_position_embeddings=max_length)\n",
    "\n",
    "#  Bert Model with a `language modeling` head on top\n",
    "model = transformers.BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Randomly mask tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e82f2f93-b372-4d3e-99fd-c27d23344773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the \n",
    "# tokens for the Masked Language Modeling (MLM) task\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(\n",
    "                                                tokenizer=tokenizer_loaded, \n",
    "                                                mlm=True, \n",
    "                                                mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Initialize training arguments_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Changed:\n",
    "# per_device_train_batch_size from 8 to lower, like 2\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=model_path,          # where to save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs\n",
    "    per_device_train_batch_size=8, # the training batch size\n",
    "    gradient_accumulation_steps=8,  # accumulae gradients before weight update\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=500,              # steps before valuate/log/sav checkpoints\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,    # load the best model (loss) at the end\n",
    "    # save_total_limit=3,           # save 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ab13e-c331-4736-b1ca-4b63fca70eba",
   "metadata": {},
   "source": [
    "### _Training_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1f14dad-5eb6-4a7e-ba2d-854a96c0ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=valid_tokenized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16df1344-51de-4aba-aaaa-7dcae72a10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: txt_clean, special_tokens_mask, txt, __index_level_0__. If txt_clean, special_tokens_mask, txt, __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 51987\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 8120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='8120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/8120 14:03 < 3:34:35, 0.59 it/s, Epoch 0.62/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/271 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: txt_clean, special_tokens_mask, txt, __index_level_0__. If txt_clean, special_tokens_mask, txt, __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17329\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.35 GiB (GPU 0; 24.00 GiB total capacity; 10.27 GiB already allocated; 4.04 GiB free; 17.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify_bert.ipynb Cell 41'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify_bert.ipynb#ch0000040vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify_bert.ipynb#ch0000040vscode-remote?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/trainer.py:1497\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1495\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1497\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1622\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 1624\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   1625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   1627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2281\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2283\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2284\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2285\u001b[0m     eval_dataloader,\n\u001b[1;32m   2286\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2287\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2288\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2289\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2290\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2291\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2292\u001b[0m )\n\u001b[1;32m   2294\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2295\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   2296\u001b[0m     speed_metrics(\n\u001b[1;32m   2297\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2301\u001b[0m     )\n\u001b[1;32m   2302\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/trainer.py:2458\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2455\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   2457\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 2458\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[1;32m   2460\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2461\u001b[0m     xm\u001b[39m.\u001b[39mmark_step()\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/trainer.py:2671\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[39mif\u001b[39;00m has_labels:\n\u001b[1;32m   2670\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 2671\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   2672\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m   2674\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2042\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2043\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2044\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1363\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1362\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()  \u001b[39m# -100 index = padding token\u001b[39;00m\n\u001b[0;32m-> 1363\u001b[0m     masked_lm_loss \u001b[39m=\u001b[39m loss_fct(prediction_scores\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvocab_size), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1366\u001b[0m     output \u001b[39m=\u001b[39m (prediction_scores,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1164\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1165\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_classify/lib/python3.9/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.35 GiB (GPU 0; 24.00 GiB total capacity; 10.27 GiB already allocated; 4.04 GiB free; 17.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __FROM PREVIOUS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Get prediction f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_prob = model_emb.predict(X_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_train_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_train_label, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ec201",
   "metadata": {},
   "source": [
    "#### _Evaluate model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of n-grams for test set\n",
    "X_test = test_cln['txt_clean']\n",
    "y_test = test_cln['label']\n",
    "\n",
    "lst_corpus_test = []\n",
    "for text in X_test:\n",
    "    lst_words = text.split()\n",
    "    lst_corpus_test.append(lst_words)\n",
    "\n",
    "## Detect common bigram and trigram with fitted detectors\n",
    "lst_corpus_test_bi = list(bigrams_detector[lst_corpus_test])\n",
    "lst_corpus_test_tr = list(trigrams_detector[lst_corpus_test_bi])\n",
    "\n",
    "len(lst_corpus_test_bi), len(lst_corpus_test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq_test = tokenizer.texts_to_sequences(lst_corpus_test_tr)\n",
    "\n",
    "## padding sequence\n",
    "X_test_w2v = keras.preprocessing.sequence.pad_sequences(lst_text2seq_test, \n",
    "                                maxlen=500, padding=\"post\", truncating=\"post\")\n",
    "X_test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a31ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_w2v = model_emb.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_w2v[1:4,] # Why are there 3 columns??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q: Why use dic_y_mapping instead of inverse_dic???\n",
    "y_pred_w2v      = [dic_y_mapping[np.argmax(pred)] for pred in y_pred_prob_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = metrics.f1_score(y_test, y_pred_w2v)\n",
    "print(test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('bert_classify': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c603e4c891dfdb4358d4c4294dc4cc17d9c2fcc038773a36be1d411e252d5e27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
