{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 5: Make predictions and interpret model__\n",
    "\n",
    "The best model is Word2Vec:\n",
    "- Because the performance is all very similar, for interpretability purpose, choose to focus on the Word2Vec-based model with ` [min_count, window, n_gram] = [20, 8, 3]`. \n",
    "- This way there is a smaller set of eature (because min_count is high) that include tri-grams (so 3 word combinations that help with interpretation).\n",
    "\n",
    "Also for interpretability purpose:\n",
    "- Tf-Idf with `[max_features, ngram_range, p_threshold]=[10000.0, (1, 3), 0.01]` are also examined.\n",
    "- This is because the Tf-Idf values is immediately interpretable and the model is based on XGBoost and I can get feature importance using SHAP.\n",
    "\n",
    "Goals: I am interested in:\n",
    "- Model interpretation\n",
    "  - Figure out how top words that define plant sciences are related to each other. \n",
    "  - See if there is certain date range, journal are more challenging to predict.\n",
    "- Make predictions of the entire corupus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "## for word embedding with w2v\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras import models, layers, callbacks, preprocessing\n",
    "\n",
    "import script_2_3_text_classify_w2v as script23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting paths\n",
    "work_dir   = Path.home() / \"projects/plant_sci_hist/2_text_classify\"\n",
    "\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# Training data for interpretation purpose\n",
    "corpus_train = work_dir / \"corpus_train.json\"\n",
    "\n",
    "# The columns to focus on\n",
    "target_col = 'txt'\n",
    "\n",
    "# Trainded Word2Vec model, tokenizer, and vocab for getting embeddings\n",
    "w2v_name   = work_dir / f\"model_cln_w2v_20-8-3\"\n",
    "tok_name   = work_dir / f\"model_cln_w2v_token_20-8-3\"\n",
    "vocab_name = work_dir / f\"model_cln_w2v_vocab_20-8-3\"\n",
    "\n",
    "# Getting ngrams\n",
    "ngram = 3\n",
    "min_count = 20\n",
    "\n",
    "# DNN checkpoint path\n",
    "cp_filepath = work_dir / f\"model_cln_w2v_20-8-3_dnn\"\n",
    "\n",
    "# Corpus to make predictions for\n",
    "corpus_dir  = Path.home() / \"projects/plant_sci_hist/1_obtaining_corpus\"\n",
    "corpus_file = corpus_dir / \"pubmed_qualified.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Analysis of prediction outcome___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load w2v model, tokenizer, and vocab\n",
    "\n",
    "Need:\n",
    "- W2V model\n",
    "- Tokenizer and vocab\n",
    "- Trained DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f74831da350>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load word2vec model\n",
    "with open(w2v_name, \"rb\") as f:\n",
    "  model_w2v = pickle.load(f)\n",
    "model_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenzier and vocab\n",
    "with open(tok_name, \"rb\") as f:\n",
    "  tokenizer = pickle.load(f)\n",
    "\n",
    "with open(vocab_name, \"rb\") as f:\n",
    "  vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\"'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb#ch0000046vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(corpus_train, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb#ch0000046vscode-remote?line=1'>2</a>\u001b[0m   corpus_train \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\"'."
     ]
    }
   ],
   "source": [
    "with corpus_train.open(\"r+\") as f:\n",
    "  corpus_combo_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Make predictions on the whole dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read corpus that needs to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497511, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_df_raw = pd.read_csv(corpus_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows\n",
    "corpus_df = corpus_df_raw[corpus_df_raw.duplicated() == False]\n",
    "\n",
    "# Rid of all records with NAs\n",
    "corpus_df = corpus_df.dropna(axis=0)\n",
    "\n",
    "# Create a new column 'txt' which is concatenated between 'Title' and 'Abstract'\n",
    "corpus_df['txt'] = corpus_df['Title'] + \" \" + corpus_df['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>QualifiedName</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>1975-11-01</td>\n",
       "      <td>The British journal of nutrition</td>\n",
       "      <td>The effects of processing of barley-based supp...</td>\n",
       "      <td>1. In one experiment the effect on rumen pH of...</td>\n",
       "      <td>barley</td>\n",
       "      <td>The effects of processing of barley-based supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1975-12-02</td>\n",
       "      <td>Biochemistry</td>\n",
       "      <td>Evidence of the involvement of a 50S ribosomal...</td>\n",
       "      <td>The functional role of the Bacillus stearother...</td>\n",
       "      <td>rose</td>\n",
       "      <td>Evidence of the involvement of a 50S ribosomal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>1975-12-11</td>\n",
       "      <td>Biochimica et biophysica acta</td>\n",
       "      <td>The reaction between the superoxide anion radi...</td>\n",
       "      <td>1. The superoxide anion radical (O2-) reacts w...</td>\n",
       "      <td>tuna</td>\n",
       "      <td>The reaction between the superoxide anion radi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PMID        Date                           Journal  \\\n",
       "0   36  1975-11-01  The British journal of nutrition   \n",
       "1   52  1975-12-02                      Biochemistry   \n",
       "2   60  1975-12-11     Biochimica et biophysica acta   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The effects of processing of barley-based supp...   \n",
       "1  Evidence of the involvement of a 50S ribosomal...   \n",
       "2  The reaction between the superoxide anion radi...   \n",
       "\n",
       "                                            Abstract QualifiedName  \\\n",
       "0  1. In one experiment the effect on rumen pH of...        barley   \n",
       "1  The functional role of the Bacillus stearother...          rose   \n",
       "2  1. The superoxide anion radical (O2-) reacts w...          tuna   \n",
       "\n",
       "                                                 txt  \n",
       "0  The effects of processing of barley-based supp...  \n",
       "1  Evidence of the involvement of a 50S ribosomal...  \n",
       "2  The reaction between the superoxide anion radi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The effects of processing of barley-based supplements on rumen pH, rate of digestion of voluntary intake of dried grass in sheep. 1. In one experiment the effect on rumen pH of feeding with restricted amounts of whole or pelleted barley was studied. With whole barley there was little variation in rumen pH associated with feeding time, but with pelleted barley the pH decreased from about 7-0 before feeding to about 5-3, 2--3 h after feeding. 2. The rate of disappearance of dried grass during incubation in the rumens of sheep receiving either whole or pelleted barley was studied in a second experiment. After 24 h incubation only 423 mg/g incubated had disappeared in the rumen of sheep receiving pelleted barley while 625 mg/g incubated had disappeared when it was incubated in the rumen of sheep receiving whole barley. 3. The voluntary intake of dried grass of lambs was studied in a third experiment when they received supplements of either 25 or 50 g whole or pelleted barley/kg live weight 0-75. At the high level, pelleted barley reduced intake of dried grass by 534 g/kg but whole barley reduced it by only 352 g/kg. The digestibility of acid-detergent fibre was reduced more by pelleted barley than by whole barley but there was a tendency for a small increase in digestibility of the barley due to processing. 4. The implications of these findings on supplementation of roughages with cereals are discussed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_df['txt'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word embeddings, w2v feature matrix using corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ngrams\n",
    "X        = corpus[target_col]\n",
    "X_ngrams = script23.get_ngram(X, ngram, min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "embeddings, X_w2v = script23.get_embeddings(X, model_w2v, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 13:40:00.933422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:00.985457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:00.985950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:00.988692: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-21 13:40:00.990587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:00.991016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:00.991405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:01.923089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:01.923553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:01.923614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-21 13:40:01.924002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 13:40:01.924466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21634 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2022-06-21 13:40:04.512528: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_w2v_20-8-3_dnn: FAILED_PRECONDITION: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_w2v_20-8-3_dnn; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f747a7924d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = script23.get_w2v_emb_model(embeddings)\n",
    "model.load_weights(cp_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb#ch0000018vscode-remote?line=0'>1</a>\u001b[0m y_pred_prob   \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_w2v)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb#ch0000018vscode-remote?line=1'>2</a>\u001b[0m dic_y_mapping \u001b[39m=\u001b[39m {n:label \u001b[39mfor\u001b[39;00m n,label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39munique([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]))}\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_2-5_text_classify_predict.ipynb#ch0000018vscode-remote?line=2'>3</a>\u001b[0m y_pred        \u001b[39m=\u001b[39m [dic_y_mapping[np\u001b[39m.\u001b[39margmax(pred)] \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m y_pred_prob]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1982\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   1981\u001b[0m   callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1982\u001b[0m   tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   1983\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1984\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred_prob   = model.predict(X_w2v)\n",
    "dic_y_mapping = {n:label for n,label in enumerate(np.unique([0,1]))}\n",
    "y_pred        = [dic_y_mapping[np.argmax(pred)] for pred in y_pred_prob]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd3af54f5fe992bccbd23931b262c263c643af7052ca64c3b616d552ec510a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
