{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For creating Word2Vec embedding-based text classification model\n",
    "\n",
    "6/22/22 [Shiu] Move to juputer (call this v.3) to get the prediction dataframes\n",
    "        to be written properly.\n",
    "6/18/22 [Shiu] When getting bi and trigrams, min_count was hard coded to 5,\n",
    "        instead of using the config file values. Rerun.\n",
    "6/15/22 Created by Shiu.\n",
    "'''\n",
    "\n",
    "## for data\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "## for word embedding with w2v\n",
    "import gensim\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, callbacks, preprocessing\n",
    "\n",
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'w2v_param':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def write_df_as_json(df, file_name):\n",
    "  json_file_name = work_dir / file_name\n",
    "\n",
    "  if not json_file_name.is_file():\n",
    "    json_file = df.to_json()\n",
    "    with json_file_name.open(\"w+\") as f:\n",
    "      json.dump(json_file, f)\n",
    "\n",
    "\n",
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, valid, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus = corpus_combo[['label','txt_clean']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  # Output train, valid, and test sets as jsons\n",
    "  print(\"  write train, valid, test data to json\")\n",
    "  write_df_as_json(train, \"corpus_train.json\")\n",
    "  write_df_as_json(valid, \"corpus_valid.json\")\n",
    "  write_df_as_json(test , \"corpus_test.json\")\n",
    "\n",
    "  X_train = train['txt_clean']\n",
    "  X_valid = valid['txt_clean']\n",
    "  X_test  = test['txt_clean']\n",
    "  y_train = train['label']\n",
    "  y_valid = valid['label']\n",
    "  y_test  = test['label']\n",
    "\n",
    "  print(f\"    size: train={X_train.shape}, valid={X_valid.shape},\" +\\\n",
    "        f\" test={X_test.shape}\")\n",
    "\n",
    "  return [X_train, X_valid, X_test, y_train, y_valid, y_test]\n",
    "  \n",
    "def get_hyperparameters(w2v_param):\n",
    "  ''' Return a list with hyperparameters based on the passed dictionary\n",
    "  Adopted from:\n",
    "    https://stackoverflow.com/questions/38721847/how-to-generate-all-combination-from-values-in-dict-of-lists-in-python\n",
    "  Args:\n",
    "    param (dict): a dictionary specified in the config.txt file.\n",
    "  Return:\n",
    "    param_list (list): a nested list of hyperparameters \n",
    "  '''\n",
    "  print(w2v_param)\n",
    "  keys, values = zip(*w2v_param.items())\n",
    "  param_list = [v for v in itertools.product(*values)]\n",
    "  \n",
    "  return keys, param_list\n",
    "\n",
    "def get_unigram(corpus):\n",
    "  unigram = []\n",
    "  for txt in corpus:\n",
    "    lst_words = txt.split()\n",
    "    unigram.append(lst_words)\n",
    "\n",
    "  return unigram\n",
    "\n",
    "def get_ngram(X_corpus, ngram, min_count, subset):\n",
    "  '''Check if ngrams files exisit, if not get ngrams based on passed parameters\n",
    "  Args:\n",
    "    X_corpus (pandas series): texts to get ngrams from\n",
    "    ngram (int): uni (1), bi (2), or tri (3) grams\n",
    "    min_count (int): minmumal number of term occurence in corpus\n",
    "    subset (str): train, valid, or test; for file name\n",
    "  Output:\n",
    "    ngram_file (pickle): model_cln_ngrams_{subset}_{min_count}-{ngram}\n",
    "  Return:\n",
    "    unigrams, bigrams, or trigrams\n",
    "  '''\n",
    "\n",
    "  # Check if ngram file exist\n",
    "  ngram_file = work_dir / f\"model_cln_ngrams_{subset}_{min_count}-{ngram}\"\n",
    "  if ngram_file.is_file():\n",
    "    print(\"    load ngrams\")\n",
    "    with open(ngram_file, \"rb\") as f:\n",
    "        ngrams = pickle.load(f)\n",
    "    return ngrams\n",
    "\n",
    "  # ngrams file does not exist, generate it\n",
    "  print(\"    generate ngrams\")\n",
    "  unigrams = get_unigram(X_corpus)\n",
    "\n",
    "  if ngram == 1:\n",
    "    return unigrams\n",
    "  # ngram >1\n",
    "  else:\n",
    "    # Get bigrams\n",
    "    bigrams_detector  = gensim.models.phrases.Phrases(\n",
    "                    uni_X, delimiter=\" \", min_count=min_count, threshold=10)\n",
    "    bigrams_detector  = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "    bigrams = list(bigrams_detector[unigrams])\n",
    "\n",
    "    # Return bigrams\n",
    "    if ngram == 2:\n",
    "      return bigrams\n",
    "\n",
    "    # Get trigrams and return them\n",
    "    elif ngram == 3:\n",
    "      trigrams_detector = gensim.models.phrases.Phrases(\n",
    "                      bigrams_detector[unigrams], delimiter=\" \", \n",
    "                      min_count=min_count, threshold=10)\n",
    "      trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "      trigrams = list(trigrams_detector[bigrams])\n",
    "      return trigrams\n",
    "    \n",
    "    else:\n",
    "      print('ERR: ngram cannot be larger than 3. QUIT!')\n",
    "      sys.exit(0)\n",
    "\n",
    "\n",
    "def get_w2v_model(X_train, X_valid, X_test, param, rand_state):\n",
    "  '''Get ngram lists and w2v model\n",
    "  Args:\n",
    "  Return:\n",
    "  '''\n",
    "  [min_count, window, ngram] = param\n",
    "\n",
    "  print(\"     ngrams for training\")\n",
    "  ngram_train = get_ngram(X_train, ngram, min_count, \"train\") \n",
    "  print(\"     ngrams for validation\")\n",
    "  ngram_valid = get_ngram(X_valid, ngram, min_count, \"valid\")\n",
    "  print(\"     ngrams for testing\")\n",
    "  ngram_test  = get_ngram(X_test , ngram, min_count, \"test\")\n",
    "\n",
    "  # Check if w2v model is already generated\n",
    "  model_w2v_name = work_dir / f\"model_cln_w2v_{min_count}-{window}-{ngram}\"\n",
    "\n",
    "  if model_w2v_name.is_file():\n",
    "    print(\"   load the w2v model\")\n",
    "    with open(work_dir / model_w2v_name, \"rb\") as f:\n",
    "        model_w2v = pickle.load(f)\n",
    "  else:\n",
    "    print(\"   geneate and save w2v model\")\n",
    "    model_w2v = gensim.models.Word2Vec(ngram_train, vector_size=300, \n",
    "                                      window=window, min_count=min_count, \n",
    "                                      sg=1, epochs=30, seed=rand_state)\n",
    "    \n",
    "    with open(model_w2v_name, \"wb\") as f:\n",
    "      pickle.dump(model_w2v, f)\n",
    "\n",
    "  return model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test\n",
    "\n",
    "\n",
    "def train_tokenizer(corpus, param):\n",
    "  '''Train a tokenizer\n",
    "  Args:\n",
    "    corpus (list): a nested list of word lists\n",
    "    param (list): for tokenizer and vocab output file names\n",
    "  Return:\n",
    "    tokenizer (keras.preprocessing.text.Tokenizer): trained tokenizer\n",
    "    dic_vocab_token (dict): token as key, index as value\n",
    "  '''\n",
    "\n",
    "  # intialize tokenizer\n",
    "  # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "  # This is replaced by tf.keras.layers.TextVectorization\n",
    "  tokenizer = preprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "  # tokenize corpus \n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "  # get token dictionary, with token as key, index number as value\n",
    "  dic_vocab_token = tokenizer.word_index\n",
    "\n",
    "  # Save tokenizer and vocab\n",
    "  [min_count, window, ngram] = param\n",
    "  tok_name   = work_dir / f\"model_cln_w2v_token_{min_count}-{window}-{ngram}\"\n",
    "  vocab_name = work_dir / f\"model_cln_w2v_vocab_{min_count}-{window}-{ngram}\"\n",
    "\n",
    "  if not tok_name.is_file():\n",
    "    with open(tok_name, \"wb\") as f:\n",
    "      pickle.dump(tokenizer, f)\n",
    "\n",
    "  if not vocab_name.is_file():\n",
    "    with open(vocab_name, \"wb\") as f:\n",
    "      pickle.dump(dic_vocab_token, f)\n",
    "\n",
    "  return tokenizer, dic_vocab_token\n",
    "\n",
    "\n",
    "def get_embeddings(corpus, model_w2v, tokenizer, dic_vocab_token):\n",
    "\n",
    "  # Transforms each text in texts to a sequence of integers.\n",
    "  lst_text2seq = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "  # pad or trucate sequence\n",
    "  X_w2v = preprocessing.sequence.pad_sequences(\n",
    "                    lst_text2seq,      # List of sequences, each a list of ints \n",
    "                    maxlen=500,        # maximum length of all sequences\n",
    "                    padding=\"post\",    # 'pre' or 'post' \n",
    "                    truncating=\"post\") # remove values from sequences > maxlen\n",
    "\n",
    "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "\n",
    "  embeddings = np.zeros((len(dic_vocab_token)+1, 300))\n",
    "  not_in_emb = {}\n",
    "  for word, idx in dic_vocab_token.items():\n",
    "      ## update the row with vector\n",
    "      try:\n",
    "          embeddings[idx] =  model_w2v.wv[word]\n",
    "      ## if word not in model then skip and the row stays all 0s\n",
    "      except KeyError:\n",
    "          not_in_emb[word] = 1\n",
    "\n",
    "  return embeddings, X_w2v\n",
    "\n",
    "\n",
    "def get_w2v_emb_model(embeddings):\n",
    "  '''Build a deep learning model with Word2Vec embeddings\n",
    "  Args:\n",
    "    embeddings\n",
    "  '''\n",
    "\n",
    "  ## code attention layer\n",
    "  def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "  ## input\n",
    "  x_in = layers.Input(shape=(500,)) ## embedding\n",
    "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                      output_dim=embeddings.shape[1], \n",
    "                      weights=[embeddings],\n",
    "                      input_length=500, trainable=False)(x_in)\n",
    "\n",
    "  ## apply attention\n",
    "  x = attention_layer(x, neurons=500)\n",
    "\n",
    "  ## 2 layers of bidirectional lstm\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                          return_sequences=True))(x)\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "  ## final dense layers\n",
    "  x = layers.Dense(64, activation='relu')(x)\n",
    "  y_out = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "  ## Initialize and compile model\n",
    "  model = models.Model(x_in, y_out)\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def run_main_function():\n",
    "\n",
    "  # Split train/validate/test for cleaned text\n",
    "  #   Will not focus on original due to issues with non-alphanumeric characters\n",
    "  #   and stop words.\n",
    "  print(\"\\nRead file and split train/validate/test...\")\n",
    "  subsets = split_train_validate_test(corpus_combo_file, rand_state)\n",
    "\n",
    "  # get w2c parameter list\n",
    "  #   [min_count, window, ngram]\n",
    "  param_keys, param_list  = get_hyperparameters(w2v_param)\n",
    "\n",
    "  # iterate through different parameters\n",
    "  with open(work_dir / f\"scores_cln_w2v\", \"w\") as f:\n",
    "    f.write(\"run\\ttxt_flag\\tlang_model\\tparameters\\tvalidate_f1\\t\" +\\\n",
    "            \"test_f1\\tmodel_dir\\n\")\n",
    "    run_num = 0\n",
    "    for param in param_list:\n",
    "      print(f\"\\n## param: {param}\")\n",
    "      best_score, model_dir, test_score = run_pipeline(param, subsets)\n",
    "\n",
    "      f.write(f\"{run_num}\\tcln\\t{lang_model}\\t{str(param)}\\t\"+\\\n",
    "              f\"{best_score}\\t{test_score}\\t{model_dir}\\n\")\n",
    "\n",
    "      run_num += 1\n",
    "\n",
    "\n",
    "def run_pipeline(param, subsets):\n",
    "  '''Carry out the major steps'''\n",
    "\n",
    "  rand_state = config_dict['rand_state']\n",
    "\n",
    "  [X_train, X_valid, X_test, y_train, y_valid, y_test] = subsets\n",
    "\n",
    "  # Get list of ngrams and w2v model\n",
    "  print(\"  get list of ngrams and w2v model\")\n",
    "  model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test = \\\n",
    "                      get_w2v_model(X_train, X_valid, X_test, param, rand_state)\n",
    "  \n",
    "  # Train tokenizer\n",
    "  print(\"  train tokenizer\")\n",
    "  tokenizer, dic_vocab_token = train_tokenizer(ngram_train, param)\n",
    "\n",
    "  # Get embeddings\n",
    "  print(\"  get embeddings\")\n",
    "  embeddings, X_train_w2v = get_embeddings(ngram_train, model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "  _, X_valid_w2v = get_embeddings(ngram_valid, model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "  _ , X_test_w2v  = get_embeddings(ngram_test , model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "\n",
    "  # Model checkpoint path and output model file name\n",
    "  cp_filepath  = Path(str(model_w2v_name) + \"_dnn\")\n",
    "\n",
    "  # Load model if exists\n",
    "  if cp_filepath.is_dir():\n",
    "    print(\"  load model in:\", cp_filepath)\n",
    "    model_emb = models.load_model(cp_filepath)\n",
    "\n",
    "  # Train and save model if not\n",
    "  else:\n",
    "    print(\"  train model\")\n",
    "    model_emb    = get_w2v_emb_model(embeddings)\n",
    "\n",
    "    # setup check points\n",
    "    callback_es  = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callback_mcp = callbacks.ModelCheckpoint(filepath=cp_filepath, mode='max', \n",
    "            save_weights_only=False, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_emb.fit(x=X_train_w2v, y=y_train, batch_size=256, \n",
    "                            epochs=20, shuffle=True, verbose=1, \n",
    "                            validation_data=(X_valid_w2v, y_valid), \n",
    "                            callbacks=[callback_es, callback_mcp])\n",
    "\n",
    "  def predict_and_output(corpus_pred_file, X_w2v, X, y):\n",
    "\n",
    "    # prediction probability\n",
    "    y_prob  = model_emb.predict(X_w2v)\n",
    "    # label mapping\n",
    "    y_map   = {n:label for n,label in enumerate(np.unique(y))}\n",
    "    # prediction\n",
    "    y_pred  = [y_map[np.argmax(pred)] for pred in y_prob]\n",
    "    # dataframe with everything\n",
    "    pred_df = pd.DataFrame([y, y_pred, y_prob, X],\n",
    "                           ['y', \"y_pred\", \"y_prob\", \"X\"])\n",
    "    pred_df.to_csv(corpus_pred_file, sep=\"\\t\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "  print(\"  output predictions of training data\")\n",
    "  train_pred_file = work_dir / \"corpus_train_pred\"\n",
    "  predict_and_output(train_pred_file, X_train_w2v, X_train, y_train)\n",
    "\n",
    "  print(\"  output validation predictions and f1 score\")\n",
    "  valid_pred_file = work_dir / \"corpus_valid_pred\"\n",
    "  y_valid_pred    = predict_and_output(valid_pred_file, X_valid_w2v, X_valid, \n",
    "                                       y_valid)\n",
    "  valid_score     = metrics.f1_score(y_valid, y_valid_pred)\n",
    "  print(\"    \", valid_score)\n",
    "\n",
    "  print(\"  output test predictions and f1 score\")\n",
    "  test_pred_file = work_dir / \"corpus_test_pred\"\n",
    "  y_test_pred    = predict_and_output(test_pred_file, X_test_w2v, X_test, \n",
    "                                      y_test)\n",
    "  test_score     = metrics.f1_score(y_valid, y_valid_pred)\n",
    "  print(\"    \", test_score)\n",
    "\n",
    "  # provide some space between runs\n",
    "  print('\\n')\n",
    "\n",
    "  return best_score, cp_filepath, test_score\n",
    "\n",
    "################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  argparser = argparse.ArgumentParser()\n",
    "  argparser.add_argument('-c', '--config',\n",
    "                        help='Configuration file', required=True)\n",
    "  args = argparser.parse_args()\n",
    "\n",
    "  config_file = Path(args.config)\n",
    "\n",
    "  print(\"\\nRead configuration file...\")\n",
    "  config_dict = read_configs(config_file)\n",
    "\n",
    "  # Declare config parameters as global variables\n",
    "  proj_dir          = Path(config_dict['proj_dir'])\n",
    "  work_dir          = proj_dir / config_dict['work_dir']\n",
    "  corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "  lang_model        = config_dict['lang_model']\n",
    "  rand_state        = config_dict['rand_state']\n",
    "  w2v_param         = config_dict['w2v_param']\n",
    "\n",
    "  run_main_function()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
