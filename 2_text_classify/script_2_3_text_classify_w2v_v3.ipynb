{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For creating Word2Vec embedding-based text classification model\n",
    "\n",
    "6/30/22 [Shiu] Saving major files that require substantial run time to help\n",
    "        with reruns.\n",
    "6/18/22 [Shiu] When getting bi and trigrams, min_count was hard coded to 5,\n",
    "        instead of using the config file values. Rerun.\n",
    "6/15/22 Created by Shin-Han Shiu.\n",
    "'''\n",
    "\n",
    "## for data\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "## for word embedding with w2v\n",
    "import gensim\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, callbacks, preprocessing\n",
    "\n",
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'w2v_param':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def write_df_as_json(df, file_name):\n",
    "  json_file_name = work_dir / file_name\n",
    "\n",
    "  if not json_file_name.is_file():\n",
    "    json_file = df.to_json()\n",
    "    with json_file_name.open(\"w+\") as f:\n",
    "      json.dump(json_file, f)\n",
    "\n",
    "\n",
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, valid, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus = corpus_combo[['label','txt_clean']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  # Output train, valid, and test sets as jsons\n",
    "  print(\"  write train, valid, test data to json\")\n",
    "  write_df_as_json(train, \"corpus_train.json\")\n",
    "  write_df_as_json(valid, \"corpus_valid.json\")\n",
    "  write_df_as_json(test , \"corpus_test.json\")\n",
    "\n",
    "  X_train = train['txt_clean']\n",
    "  X_valid = valid['txt_clean']\n",
    "  X_test  = test['txt_clean']\n",
    "  y_train = train['label']\n",
    "  y_valid = valid['label']\n",
    "  y_test  = test['label']\n",
    "\n",
    "  print(f\"    size: train={X_train.shape}, valid={X_valid.shape},\" +\\\n",
    "        f\" test={X_test.shape}\")\n",
    "\n",
    "  return [X_train, X_valid, X_test, y_train, y_valid, y_test], corpus_combo\n",
    "  \n",
    "def get_hyperparameters(w2v_param):\n",
    "  ''' Return a list with hyperparameters based on the passed dictionary\n",
    "  Adopted from:\n",
    "    https://stackoverflow.com/questions/38721847/how-to-generate-all-combination-from-values-in-dict-of-lists-in-python\n",
    "  Args:\n",
    "    param (dict): a dictionary specified in the config.txt file.\n",
    "  Return:\n",
    "    param_list (list): a nested list of hyperparameters \n",
    "  '''\n",
    "  print(w2v_param)\n",
    "  keys, values = zip(*w2v_param.items())\n",
    "  param_list = [v for v in itertools.product(*values)]\n",
    "  \n",
    "  return keys, param_list\n",
    "\n",
    "def get_unigram(corpus):\n",
    "  unigram = []\n",
    "  for txt in corpus:\n",
    "    lst_words = txt.split()\n",
    "    unigram.append(lst_words)\n",
    "\n",
    "  return unigram\n",
    "\n",
    "def get_ngram(X_corpus, ngram, min_count, subset, work_dir):\n",
    "  '''Check if ngrams files exisit, if not get ngrams based on passed parameters\n",
    "  Args:\n",
    "    X_corpus (pandas series): texts to get ngrams from\n",
    "    ngram (int): uni (1), bi (2), or tri (3) grams\n",
    "    min_count (int): minmumal number of term occurence in corpus\n",
    "    subset (str): train, valid, or test; for file name\n",
    "    work_dir (Path): does not really need this for call within this script, but\n",
    "      if called as module, this needs to be passed. So make this required.\n",
    "  Output:\n",
    "    ngram_file (pickle): model_cln_ngrams_{subset}_{min_count}-{ngram}\n",
    "  Return:\n",
    "    unigrams, bigrams, or trigrams\n",
    "  '''\n",
    "\n",
    "  # Check if ngram file exist\n",
    "  ngram_file = work_dir / f\"model_cln_ngrams_{subset}_{min_count}-{ngram}\"\n",
    "  if ngram_file.is_file():\n",
    "    print(\"    load ngrams\")\n",
    "    with open(ngram_file, \"rb\") as f:\n",
    "        ngrams = pickle.load(f)\n",
    "    return ngrams\n",
    "\n",
    "  else:\n",
    "    # ngrams file does not exist, generate it\n",
    "    print(\"    generate ngrams\")\n",
    "    ngrams   = \"\"\n",
    "\n",
    "    unigrams = get_unigram(X_corpus)\n",
    "    if ngram == 1:\n",
    "      ngrams = unigrams\n",
    "    # ngram >1\n",
    "    else:\n",
    "      # Get bigrams\n",
    "      bigrams_detector  = gensim.models.phrases.Phrases(\n",
    "                      unigrams, delimiter=\" \", min_count=min_count, threshold=10)\n",
    "      bigrams_detector  = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "      bigrams = list(bigrams_detector[unigrams])\n",
    "\n",
    "      # Return bigrams\n",
    "      if ngram == 2:\n",
    "        ngrams = bigrams\n",
    "      # Get trigrams and return them\n",
    "      elif ngram == 3:\n",
    "        trigrams_detector = gensim.models.phrases.Phrases(\n",
    "                        bigrams_detector[unigrams], delimiter=\" \", \n",
    "                        min_count=min_count, threshold=10)\n",
    "        trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "        trigrams = list(trigrams_detector[bigrams])\n",
    "        ngrams = trigrams\n",
    "      else:\n",
    "        print('ERR: ngram cannot be larger than 3. QUIT!')\n",
    "        sys.exit(0)\n",
    "\n",
    "      # write ngram file\n",
    "      with open(ngram_file, \"wb\") as f:\n",
    "          pickle.dump(ngrams, f)      \n",
    "\n",
    "      return ngrams\n",
    "\n",
    "def get_w2v_model(X_train, X_valid, X_test, param, rand_state):\n",
    "  '''Get ngram lists and w2v model\n",
    "  Args:\n",
    "  Return:\n",
    "  '''\n",
    "  [min_count, window, ngram] = param\n",
    "\n",
    "  print(\"    ngrams for training\")\n",
    "  ngram_train = get_ngram(X_train, ngram, min_count, \"train\", work_dir) \n",
    "  print(\"    ngrams for validation\")\n",
    "  ngram_valid = get_ngram(X_valid, ngram, min_count, \"valid\", work_dir)\n",
    "  print(\"    ngrams for testing\")\n",
    "  ngram_test  = get_ngram(X_test , ngram, min_count, \"test\", work_dir)\n",
    "\n",
    "  # Check if w2v model is already generated\n",
    "  model_w2v_name = work_dir / f\"model_cln_w2v_{min_count}-{window}-{ngram}\"\n",
    "\n",
    "  if model_w2v_name.is_file():\n",
    "    print(\"   load the w2v model\")\n",
    "    with open(work_dir / model_w2v_name, \"rb\") as f:\n",
    "        model_w2v = pickle.load(f)\n",
    "  else:\n",
    "    print(\"   geneate and save w2v model\")\n",
    "    model_w2v = gensim.models.Word2Vec(ngram_train, vector_size=300, \n",
    "                                      window=window, min_count=min_count, \n",
    "                                      sg=1, epochs=30, seed=rand_state)\n",
    "    \n",
    "    with open(model_w2v_name, \"wb\") as f:\n",
    "      pickle.dump(model_w2v, f)\n",
    "\n",
    "  return model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test\n",
    "\n",
    "\n",
    "def train_tokenizer(corpus, param):\n",
    "  '''Train a tokenizer\n",
    "  Args:\n",
    "    corpus (list): a nested list of word lists\n",
    "    param (list): for tokenizer and vocab output file names\n",
    "  Return:\n",
    "    tokenizer (keras.preprocessing.text.Tokenizer): trained tokenizer\n",
    "    dic_vocab_token (dict): token as key, index as value\n",
    "  '''\n",
    "\n",
    "  # intialize tokenizer\n",
    "  # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "  # This is replaced by tf.keras.layers.TextVectorization\n",
    "  tokenizer = preprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "  # tokenize corpus \n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "  # get token dictionary, with token as key, index number as value\n",
    "  dic_vocab_token = tokenizer.word_index\n",
    "\n",
    "  # Save tokenizer and vocab\n",
    "  [min_count, window, ngram] = param\n",
    "  tok_name   = work_dir / f\"model_cln_w2v_token_{min_count}-{window}-{ngram}\"\n",
    "  vocab_name = work_dir / f\"model_cln_w2v_vocab_{min_count}-{window}-{ngram}\"\n",
    "\n",
    "  if not tok_name.is_file():\n",
    "    with open(tok_name, \"wb\") as f:\n",
    "      pickle.dump(tokenizer, f)\n",
    "\n",
    "  if not vocab_name.is_file():\n",
    "    with open(vocab_name, \"wb\") as f:\n",
    "      pickle.dump(dic_vocab_token, f)\n",
    "    \n",
    "  return tokenizer, dic_vocab_token\n",
    "\n",
    "\n",
    "def get_embeddings(corpus, model_w2v, tokenizer, dic_vocab_token):\n",
    "\n",
    "  # Transforms each text in texts to a sequence of integers.\n",
    "  lst_text2seq = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "  # pad or trucate sequence\n",
    "  X_w2v = preprocessing.sequence.pad_sequences(\n",
    "                    lst_text2seq,      # List of sequences, each a list of ints \n",
    "                    maxlen=500,        # maximum length of all sequences\n",
    "                    padding=\"post\",    # 'pre' or 'post' \n",
    "                    truncating=\"post\") # remove values from sequences > maxlen\n",
    "\n",
    "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "\n",
    "  embeddings = np.zeros((len(dic_vocab_token)+1, 300))\n",
    "  not_in_emb = {}\n",
    "  for word, idx in dic_vocab_token.items():\n",
    "      ## update the row with vector\n",
    "      try:\n",
    "          embeddings[idx] =  model_w2v.wv[word]\n",
    "      ## if word not in model then skip and the row stays all 0s\n",
    "      except KeyError:\n",
    "          not_in_emb[word] = 1\n",
    "\n",
    "  return embeddings, X_w2v\n",
    "\n",
    "\n",
    "def get_w2v_emb_model(embeddings):\n",
    "  '''Build a deep learning model with Word2Vec embeddings\n",
    "  Args:\n",
    "    embeddings\n",
    "  '''\n",
    "\n",
    "  ## code attention layer\n",
    "  def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "  ## input\n",
    "  x_in = layers.Input(shape=(500,)) ## embedding\n",
    "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                      output_dim=embeddings.shape[1], \n",
    "                      weights=[embeddings],\n",
    "                      input_length=500, trainable=False)(x_in)\n",
    "\n",
    "  ## apply attention\n",
    "  x = attention_layer(x, neurons=500)\n",
    "\n",
    "  ## 2 layers of bidirectional lstm\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                          return_sequences=True))(x)\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "  ## final dense layers\n",
    "  x = layers.Dense(64, activation='relu')(x)\n",
    "  y_out = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "  ## Initialize and compile model\n",
    "  model = models.Model(x_in, y_out)\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read configuration file...\n",
      "   lang_model = w2v\n",
      "   proj_dir = /home/shinhan/projects/plant_sci_hist\n",
      "   work_dir = 2_text_classify\n",
      "   corpus_combo_file = corpus_combo\n",
      "   rand_state = 20220609\n",
      "   w2v_param = {'min_count': [20], 'window': [8], 'ngram': [3]}\n",
      "  all config available\n",
      "\n",
      "Read file and split train/validate/test...\n",
      "  write train, valid, test data to json\n",
      "    size: train=(51987,), valid=(17329,), test=(17330,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "config_file = Path('config_w2v_run1.txt')\n",
    "\n",
    "print(\"\\nRead configuration file...\")\n",
    "config_dict = read_configs(config_file)\n",
    "\n",
    "# Declare config parameters as global variables\n",
    "proj_dir          = Path(config_dict['proj_dir'])\n",
    "work_dir          = proj_dir / config_dict['work_dir']\n",
    "corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "lang_model        = config_dict['lang_model']\n",
    "rand_state        = config_dict['rand_state']\n",
    "w2v_param         = config_dict['w2v_param']\n",
    "\n",
    "# Split train/validate/test for cleaned text\n",
    "#   Will not focus on original due to issues with non-alphanumeric characters\n",
    "#   and stop words.\n",
    "print(\"\\nRead file and split train/validate/test...\")\n",
    "subsets, corpus_combo = split_train_validate_test(corpus_combo_file, rand_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>QualifiedName</th>\n",
       "      <th>txt</th>\n",
       "      <th>label</th>\n",
       "      <th>txt_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1137623</th>\n",
       "      <td>28316606</td>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>Frontiers in plant science</td>\n",
       "      <td>A Comprehensive Phenotypic Investigation of th...</td>\n",
       "      <td>Seed shattering in crops is a key domesticatio...</td>\n",
       "      <td>bean</td>\n",
       "      <td>A Comprehensive Phenotypic Investigation of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>comprehensive phenotypic investigation podshat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790768</th>\n",
       "      <td>22369516</td>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>Plant biotechnology journal</td>\n",
       "      <td>Enhanced polyhydroxybutyrate production in tra...</td>\n",
       "      <td>Polyhydroxybutyrate (PHB) is a bacterial polye...</td>\n",
       "      <td>sugarcane</td>\n",
       "      <td>Enhanced polyhydroxybutyrate production in tra...</td>\n",
       "      <td>1</td>\n",
       "      <td>enhanced polyhydroxybutyrate production transg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PMID        Date                      Journal  \\\n",
       "1137623  28316606  2017-03-21   Frontiers in plant science   \n",
       "790768   22369516  2012-03-01  Plant biotechnology journal   \n",
       "\n",
       "                                                     Title  \\\n",
       "1137623  A Comprehensive Phenotypic Investigation of th...   \n",
       "790768   Enhanced polyhydroxybutyrate production in tra...   \n",
       "\n",
       "                                                  Abstract QualifiedName  \\\n",
       "1137623  Seed shattering in crops is a key domesticatio...          bean   \n",
       "790768   Polyhydroxybutyrate (PHB) is a bacterial polye...     sugarcane   \n",
       "\n",
       "                                                       txt  label  \\\n",
       "1137623  A Comprehensive Phenotypic Investigation of th...      1   \n",
       "790768   Enhanced polyhydroxybutyrate production in tra...      1   \n",
       "\n",
       "                                                 txt_clean  \n",
       "1137623  comprehensive phenotypic investigation podshat...  \n",
       "790768   enhanced polyhydroxybutyrate production transg...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##DEBUG\n",
    "# Figure out how the subsets and the original corpus indices are corresponding\n",
    "# to each other.\n",
    "corpus_combo.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'perspective better understanding metabolic integration photorespiration within complex plant primary metabolism network photorespiration essential high flux metabolic pathway found oxygenproducing photosynthetic organism often viewed closed metabolic repair pathway serf detoxify 2phosphoglycolic acid recycle carbon fuel calvinbenson cycle however, view simplistic since photorespiratory cycle known interact several primary metabolic pathways, including photosynthesis, nitrate assimilation, amino acid metabolism, c1 metabolism krebs (tca) cycle review recent advance photorespiration research discus future priority better understand (i) metabolic integration photorespiratory cycle within complex network plant primary metabolism (ii) importance photorespiration response abiotic biotic stress author 2016 published oxford university press behalf society experimental biology right reserved permissions, please email journalspermissionsoupcom'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_train, X_valid, X_test, y_train, y_valid, y_test] = subsets\n",
    "X_train.loc[1066736]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perspectives for a better understanding of the metabolic integration of photorespiration within a complex plant primary metabolism network.. Photorespiration is an essential high flux metabolic pathway that is found in all oxygen-producing photosynthetic organisms. It is often viewed as a closed metabolic repair pathway that serves to detoxify 2-phosphoglycolic acid and to recycle carbon to fuel the Calvin-Benson cycle. However, this view is too simplistic since the photorespiratory cycle is known to interact with several primary metabolic pathways, including photosynthesis, nitrate assimilation, amino acid metabolism, C1 metabolism and the Krebs (TCA) cycle. Here we will review recent advances in photorespiration research and discuss future priorities to better understand (i) the metabolic integration of the photorespiratory cycle within the complex network of plant primary metabolism and (ii) the importance of photorespiration in response to abiotic and biotic stresses.© The Author 2016. Published by Oxford University Press on behalf of the Society for Experimental Biology. All rights reserved. For permissions, please email: journals.permissions@oup.com.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_combo.loc[1066736]['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get w2c parameter list\n",
    "#   [min_count, window, ngram]\n",
    "param_keys, param_list  = get_hyperparameters(w2v_param)\n",
    "\n",
    "# iterate through different parameters\n",
    "with open(work_dir / f\"scores_cln_w2v\", \"w\") as f:\n",
    "  f.write(\"run\\ttxt_flag\\tlang_model\\tparameters\\tvalidate_f1\\t\" +\\\n",
    "          \"test_f1\\tmodel_dir\\n\")\n",
    "run_num = 0\n",
    "\n",
    "# Assuming only one parameter for now\n",
    "param = param_list[0]\n",
    "print(f\"\\n## param: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  get list of ngrams and w2v model\n",
      "    ngrams for training\n",
      "    load ngrams\n",
      "    ngrams for validation\n",
      "    load ngrams\n",
      "    ngrams for testing\n",
      "    load ngrams\n",
      "   load the w2v model\n"
     ]
    }
   ],
   "source": [
    "### def run_pipeline(param, subsets):\n",
    "\n",
    "rand_state = config_dict['rand_state']\n",
    "\n",
    "[X_train, X_valid, X_test, y_train, y_valid, y_test] = subsets\n",
    "\n",
    "# Get list of ngrams and w2v model\n",
    "print(\"  get list of ngrams and w2v model\")\n",
    "model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test = \\\n",
    "                    get_w2v_model(X_train, X_valid, X_test, param, rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Train tokenizer\n",
    "print(\"  train tokenizer\")\n",
    "tokenizer, dic_vocab_token = train_tokenizer(ngram_train, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ngram len: 51987 17329 17330\n",
      "['vivo vitro', 'inhibition', 'catalase', 'leaf', 'nicotiana sylvestris', '3amino1,2,4triazole', 'seedling', 'tobacco (nicotiana', 'sylvestris)', 'treated', 'vivo', '003', '20', 'millimolar', '3amino1,2,4triazole', '(aminotriazole)', 'rapid', 'loss', 'catalase', '(ec', '11116)', 'activity', 'first', '5', 'hour', 'followed', 'slower', 'decrease', 'next', '4 hour', 'level', '15 20', 'initial', 'activity,', 'little', 'change', 'period', '3 day', 'fifty', 'percent', 'loss', 'catalase activity', 'occurred', '010', '015', 'millimolar', 'inhibitor', '(18hour', 'incubation)', 'isozymes', 'tobacco', 'catalase', 'differed', 'sensitivity', 'inhibitor', 'enhancedperoxidatic', 'catalase', '(epcat)', '(havir', 'ea,', 'mchale', 'na,', '1989', 'plant physiol', '91', '812815)', 'decreased', '35', 'condition', 'major', 'isozyme', 'decreased', '85', 'resistance', 'aminotriazole', 'inhibition', 'demonstrated', 'vivo', 'epcat', 'also observed', 'vitro', 'time', '50', 'inhibition', '067,', '333,', '50,', '100,', '15', 'millimolar', 'aminotriazole', '15,', '5,', '26,', '25,', '15', 'minutes,', 'respectively,', 'major', 'isozyme', 'catalase', '60,', '185,', '51,', '4,', '30', 'minutes,', 'respectively,', 'epcat', 'increasing', 'h(2)o(2)', 'concentration', 'change', 'sensitivity', 'epcat', 'aminotriazole', 'major', 'form', 'catalase', 'contained', '40', '+', '04', 'mole', 'heme', 'per mole', 'enzyme', 'epcat', '34', '+', '03', 'thus,', 'resistance', 'epcat', 'aminotriazole', 'probably due', 'lowered', 'affinity', 'h(2)o(2)', 'alteration', 'heme', 'content', 'structural change', 'impair', 'inhibitor', 'binding']\n"
     ]
    }
   ],
   "source": [
    "## DEBUG\n",
    "print(\" ngram len:\", len(ngram_train), len(ngram_valid), len(ngram_test))\n",
    "print(ngram_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  get embeddings\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings\n",
    "print(\"  get embeddings\")\n",
    "embeddings, X_train_w2v = get_embeddings(\n",
    "                          ngram_train, model_w2v, tokenizer, dic_vocab_token)\n",
    "_, X_valid_w2v = get_embeddings(\n",
    "                          ngram_valid, model_w2v, tokenizer, dic_vocab_token)\n",
    "_, X_test_w2v  = get_embeddings(\n",
    "                          ngram_test , model_w2v, tokenizer, dic_vocab_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51987, 500), (17329, 500), (17330, 500))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##DEBUG\n",
    "X_train_w2v.shape, X_valid_w2v.shape, X_test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  load model in: /home/shinhan/projects/plant_sci_hist/2_text_classify/model_cln_w2v_20-8-3_dnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 19:10:50.149129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:50.306489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:50.306732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:50.309105: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-30 19:10:50.311849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:50.312193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:50.312436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:52.026758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:52.027005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:52.027020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-30 19:10:52.027306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-30 19:10:52.028687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5430 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Model checkpoint path and output model file name\n",
    "cp_filepath  = Path(str(model_w2v_name) + \"_dnn\")\n",
    "\n",
    "# Load model if exists\n",
    "if cp_filepath.is_dir():\n",
    "  print(\"  load model in:\", cp_filepath)\n",
    "  model_emb = models.load_model(cp_filepath)\n",
    "\n",
    "# Train and save model if not\n",
    "else:\n",
    "  print(\"  train model\")\n",
    "  model_emb    = get_w2v_emb_model(embeddings)\n",
    "\n",
    "  # setup check points\n",
    "  callback_es  = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "  callback_mcp = callbacks.ModelCheckpoint(filepath=cp_filepath, mode='max', \n",
    "          save_weights_only=False, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "  # Train model\n",
    "  history = model_emb.fit(x=X_train_w2v, y=y_train, batch_size=256, \n",
    "                          epochs=20, shuffle=True, verbose=1, \n",
    "                          validation_data=(X_valid_w2v, y_valid), \n",
    "                          callbacks=[callback_es, callback_mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_output(corpus_pred_file, X_w2v, X, y):\n",
    "\n",
    "  # prediction probability\n",
    "  print(\"    get prediction probability\")\n",
    "  y_prob  = model_emb.predict(X_w2v)\n",
    "\n",
    "  # label mapping\n",
    "  y_map   = {n:label for n,label in enumerate(np.unique(y))}\n",
    "  # prediction\n",
    "  print(\"    get predictions\")\n",
    "  #y_pred  = pd.Series([y_map[np.argmax(pred)] for pred in y_prob])\n",
    "  y_pred  = [y_map[np.argmax(pred)] for pred in y_prob]\n",
    "\n",
    "  # Convert y_prob to series. There are probabilities for two classes. Take\n",
    "  # the column with class=1 (2nd column)\n",
    "  y_prob_series = pd.Series(y_prob[:,1], index=y.index)\n",
    "\n",
    "  # convert y_pred to series\n",
    "  y_pred_series = pd.Series(y_pred, index=y.index)\n",
    "\n",
    "  # dataframe with everything\n",
    "  pred_df = pd.DataFrame({'y': y, \n",
    "                          'y_pred': y_pred_series, \n",
    "                          'y_prob': y_prob_series, \n",
    "                          'X':X})\n",
    "\n",
    "  print(\"    write prediciton dataframe\")\n",
    "  pred_df.to_csv(corpus_pred_file, sep=\"\\t\")\n",
    "\n",
    "  return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  output predictions of training data\n",
      "    get prediction probability\n",
      "    get predictions\n",
      "    write prediciton dataframe\n",
      "     train_pred_df.shape: (51987, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"  output predictions of training data\")\n",
    "train_pred_file = work_dir / \"corpus_train_pred\"\n",
    "train_pred_df = predict_and_output(\n",
    "                            train_pred_file, X_train_w2v, X_train, y_train)\n",
    "print(\"     train_pred_df.shape:\", train_pred_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For working out how to output the prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DEBUG: modified\n",
    "#def predict_and_output(corpus_pred_file, X_w2v, X, y):\n",
    "def predict_and_output(X_w2v, y):\n",
    "\n",
    "  # prediction probability\n",
    "  print(\"    get prediction probability\")\n",
    "  y_prob  = model_emb.predict(X_w2v)\n",
    "  #print(y_prob.shape) # has two columns\n",
    "\n",
    "  # label mapping\n",
    "  y_map   = {n:label for n,label in enumerate(np.unique(y))}\n",
    "  # prediction\n",
    "  print(\"    get predictions\")\n",
    "  #y_pred  = pd.Series([y_map[np.argmax(pred)] for pred in y_prob])\n",
    "  y_pred  = [y_map[np.argmax(pred)] for pred in y_prob]\n",
    "\n",
    "  # convert y_prob column index=1 to pandas series\n",
    "  #y_prob_1= pd.Series(y_prob[:,1])\n",
    "\n",
    "  # get values from X otherwise the index does not match\n",
    "  #X_idx   = pd.Series(X.index)\n",
    "  #X_val   = pd.Series(X.value)\n",
    "\n",
    "  # dataframe with everything\n",
    "  #pred_df = pd.DataFrame({'y': y, \"y_pred\": y_pred, \"y_prob\": y_prob_1, \n",
    "  #                        \"X_idx\":X_idx, \"X_val\": X_val})\n",
    "\n",
    "  #print(\"    write prediciton dataframe\")\n",
    "  #pred_df.to_csv(corpus_pred_file, sep=\"\\t\")\n",
    "\n",
    "  return y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  output predictions of training data\n",
      "    get prediction probability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 19:15:55.149205: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-30 19:15:56.923758: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    get predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"  output predictions of training data\")\n",
    "train_pred_file = work_dir / \"corpus_train_pred\"\n",
    "#predict_and_output(train_pred_file, X_train_w2v, X_train, y_train)\n",
    "y_pred, y_prob = predict_and_output(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n",
      "51987 (51987, 2) (51987,) (51987,)\n"
     ]
    }
   ],
   "source": [
    "##DEBUG\n",
    "print(type(y_pred), type(y_prob), type(X_train), type(y_train))\n",
    "print(len(y_pred), y_prob.shape, X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516651    vivo vitro inhibition catalase leaf nicotiana ...\n",
       "521301    pathway glucose regulation monosaccharide tran...\n",
       "65516     feasibility home treatment diarrhoea packaged ...\n",
       "Name: txt_clean, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516651    1\n",
       "521301    1\n",
       "65516     0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516651    1\n",
       "521301    1\n",
       "65516     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_series = pd.Series(y_pred, index=y_train.index)\n",
    "y_pred_series[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516651    0.982180\n",
       "521301    0.993828\n",
       "65516     0.001417\n",
       "dtype: float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob_series = pd.Series(y_prob[:,1], index=y_train.index)\n",
    "y_prob_series[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'y': y_train, 'y_pred': y_pred_series, \n",
    "                        'y_prob': y_prob_series, 'X':X_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51987, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_prob</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516651</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982180</td>\n",
       "      <td>vivo vitro inhibition catalase leaf nicotiana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521301</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.993828</td>\n",
       "      <td>pathway glucose regulation monosaccharide tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65516</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>feasibility home treatment diarrhoea packaged ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277058</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990589</td>\n",
       "      <td>modulation phosphatidylcholine biosynthesis ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753225</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921157</td>\n",
       "      <td>120yr period dr beals seed viability experimen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        y  y_pred    y_prob                                                  X\n",
       "516651  1       1  0.982180  vivo vitro inhibition catalase leaf nicotiana ...\n",
       "521301  1       1  0.993828  pathway glucose regulation monosaccharide tran...\n",
       "65516   0       0  0.001417  feasibility home treatment diarrhoea packaged ...\n",
       "277058  1       1  0.990589  modulation phosphatidylcholine biosynthesis ce...\n",
       "753225  1       1  0.921157  120yr period dr beals seed viability experimen..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1761d8df3801bd2a70c9560dc6d458568584f11b389930146f99ac99ddb0b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
