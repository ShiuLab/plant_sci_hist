{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 2c: BERT models - distillbert__\n",
    "\n",
    "Follow:\n",
    "- [How to train a BERT model from scratch](https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6)\n",
    "- [Huggintface ByteLevelBPETokenizer tutorial](https://github.com/huggingface/blog/blob/main/how-to-train.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install pytorch for working with SciBERT\n",
    "%conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Imports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For building text classification model based on embedding of Word2Vec and BERT\n",
    "'''\n",
    "\n",
    "\n",
    "## for data\n",
    "#import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'bert_param':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, valid, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  corpus = corpus_combo[['label','txt','txt_clean']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  print(f\"    train={train.shape}, valid={valid.shape},\" +\\\n",
    "        f\" test={test.shape}\")\n",
    "\n",
    "  return [train, valid, test]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get training/testing split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read configuration file...\n",
      "   lang_model = bert\n",
      "   proj_dir = /home/shius/projects/plant_sci_hist\n",
      "   work_dir = 2_text_classify\n",
      "   corpus_combo_file = corpus_combo\n",
      "   rand_state = 20220609\n",
      "   bert_param = {}\n",
      "  all config available\n"
     ]
    }
   ],
   "source": [
    "config_file = Path(\"config_bert.txt\")\n",
    "\n",
    "print(\"\\nRead configuration file...\")\n",
    "config_dict = read_configs(config_file)\n",
    "\n",
    "# Set up working directory and corpus file location\n",
    "proj_dir          = Path(config_dict['proj_dir'])\n",
    "work_dir          = proj_dir / config_dict['work_dir']\n",
    "corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# For reproducibility\n",
    "rand_state = config_dict['rand_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read file and split train/validate/test...\n",
      "    train=(51987, 3), valid=(17329, 3), test=(17330, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split train/validate/test for cleaned text\n",
    "#   Will not focus on original due to issues with non-alphanumeric characters\n",
    "#   and stop words.\n",
    "print(\"\\nRead file and split train/validate/test...\")\n",
    "[train, valid, test] = split_train_validate_test(corpus_combo_file, rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to Datasets\n",
    "dataset_train = Dataset.from_pandas(train)\n",
    "dataset_valid = Dataset.from_pandas(valid)\n",
    "dataset_test  = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'txt', 'txt_clean', '__index_level_0__'],\n",
       "    num_rows: 51987\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Tokenization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 52_000\n",
    "\n",
    "# maximum sequence length, lowering will result to faster training (when \n",
    "# increasing batch size)\n",
    "max_length = 512\n",
    "\n",
    "min_frequency=2\n",
    "\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Write training texts to files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training texts to a folder where each file has 5000 entries.\n",
    "corpus_train_path = work_dir / \"corpus_train\"\n",
    "corpus_train_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note that I use the original text for training tokenizer\n",
    "txts  = train['txt'].values\n",
    "\n",
    "# list of training corpus files\n",
    "files = []             \n",
    "for idx in range(0,len(txts),5000):\n",
    "  subset = txts[idx:idx+5000]\n",
    "  subset_file = corpus_train_path / f\"txt_{idx}\"\n",
    "\n",
    "  # force posix path to be string, otherwize the training step below will fail\n",
    "  files.append(str(subset_file))\n",
    "  with open(subset_file, \"w\") as f:\n",
    "    subset_txts = '\\n'.join(subset)\n",
    "    f.write(subset_txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Train tokeinzier from scratch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intialize and train tokenizer\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, min_frequency=min_frequency, \n",
    "                special_tokens=special_tokens)\n",
    "\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "model_path = work_dir / \"model_cln_bert_test\"\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the tokenizer  \n",
    "tokenizer.save_model(str(model_path))\n",
    "\n",
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(model_path / \"config.json\", \"w\") as f:\n",
    "    tokenizer_cfg = {\n",
    "        \"do_lower_case\": True,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"model_max_length\": max_length,\n",
    "        \"max_len\": max_length,}\n",
    "    json.dump(tokenizer_cfg, f)\n",
    "\n",
    "# This step is critical: the trained tokenizer object cannot be called directly.\n",
    "tokenizer_loaded = transformers.BertTokenizerFast.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2604605",
   "metadata": {},
   "source": [
    "## __Set up model__\n",
    "\n",
    "For scibert:\n",
    "- https://analyticsindiamag.com/guide-to-scibert-a-pre-trained-bert-based-language-model-for-scientific-text/\n",
    "- https://www.kaggle.com/code/gcspkmdr/scibert-wrapped-in-tf2/notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inputs\n",
    "idx   = tf.keras.layers.Input((max_length), dtype=\"int32\", name=\"input_idx\")\n",
    "masks = tf.keras.layers.Input((max_length), dtype=\"int32\", name=\"input_masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _distilbert_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 13:37:25.642924: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-17 13:37:27.293746: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## get pre-trained bert with config\n",
    "#  Below is for distilbert\n",
    "pretrained_name = 'distilbert-base-uncased'\n",
    "config = transformers.DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "pretrained = transformers.TFDistilBertModel.from_pretrained(pretrained_name,\n",
    "                                                              config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_out = pretrained(idx, attention_mask=masks)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## set up additional layers for fine-tuning\n",
    "x     = tf.keras.layers.GlobalAveragePooling1D()(bert_out)\n",
    "x     = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "y_out = tf.keras.layers.Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_idx (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_masks (InputLayer)       [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_idx[0][0]',              \n",
      " BertModel)                     ast_hidden_state=(N               'input_masks[0][0]']            \n",
      "                                one, 512, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           49216       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            130         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,412,226\n",
      "Trainable params: 49,346\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## compile\n",
    "model = tf.keras.models.Model([idx, masks], y_out)\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "loss      ='sparse_categorical_crossentropy'\n",
    "metrics   =['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "              \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Text classification with transfer learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Set up train, valid, and test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datasets.arrow_dataset.Dataset, list)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pre-processed text data is used for encoding purpose. From dataset data\n",
    "# type, the returned object from here are lists.\n",
    "X_train = dataset_train['txt_clean']\n",
    "X_valid = dataset_valid['txt_clean']\n",
    "X_test  = dataset_test['txt_clean']\n",
    "type(dataset_train), type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up labels: cannot get these from dataset type since list cannot be used\n",
    "# to store labels for the model.fit function below. Instead, get them from\n",
    "# the original dataframe\n",
    "y_train = train['label']\n",
    "y_valid = valid['label']\n",
    "y_test  = test['label']\n",
    "type(train), type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Encode corpus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to encode text data in batches\n",
    "def batch_encode(tokenizer, texts, batch_size=256):\n",
    "  \"\"\"\"\"\"\"\"\"\n",
    "  A function that encodes a batch of texts and returns the texts'\n",
    "  corresponding encodings and attention masks that are ready to be fed \n",
    "  into a pre-trained transformer model.\n",
    "  \n",
    "  Input:\n",
    "  - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "  - texts:       List of strings where each string represents a text\n",
    "  - batch_size:  Integer controlling number of texts in a batch\n",
    "  - max_length:  Integer controlling max number of words to tokenize in a\n",
    "    given text\n",
    "  Output:\n",
    "  - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "  - attention_mask: the texts' attention mask encoded as a tf.Tensor obj\n",
    "  \"\"\"\"\"\"\"\"\"\n",
    "  # Define the maximum number of words to tokenize (up to 512)\n",
    "  max_length = 512\n",
    "  input_ids = []\n",
    "  attention_mask = []\n",
    "  \n",
    "  for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    inputs = tokenizer.batch_encode_plus(batch,\n",
    "                                          max_length=max_length,\n",
    "                                          padding='max_length',\n",
    "                                          truncation=True,\n",
    "                                          return_attention_mask=True,\n",
    "                                          return_token_type_ids=False\n",
    "                                          )\n",
    "    input_ids.extend(inputs['input_ids'])\n",
    "    attention_mask.extend(inputs['attention_mask'])\n",
    "  \n",
    "  return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bc73294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode corpus\n",
    "X_train_ids, X_train_attn = batch_encode(tokenizer_loaded, X_train)\n",
    "X_valid_ids, X_valid_attn = batch_encode(tokenizer_loaded, X_valid)\n",
    "X_test_ids , X_test_attn  = batch_encode(tokenizer_loaded, X_test)\n",
    "\n",
    "X_train_bert = [np.asarray(X_train_ids, dtype='int32'),\n",
    "                np.asarray(X_train_attn, dtype='int32')]\n",
    "X_valid_bert = [np.asarray(X_valid_ids, dtype='int32'),\n",
    "                np.asarray(X_valid_attn, dtype='int32')]\n",
    "X_test_bert  = [np.asarray(X_test_ids, dtype='int32'),\n",
    "                np.asarray(X_test_attn, dtype='int32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25bb1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following is done in the original tutorial because the classes are not\n",
    "## in numbers. So this can be skipped.\n",
    "# encode y\n",
    "#dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "#inverse_dic   = {v:k for k,v in dic_y_mapping.items()}\n",
    "#y_train_label = np.array([inverse_dic[y] for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup output dir\n",
    "cp_filepath = work_dir / \"model_cln_bert_test\"\n",
    "\n",
    "# setup callbacks\n",
    "callback_es  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "callback_mcp = tf.keras.callbacks.ModelCheckpoint(filepath=cp_filepath, \n",
    "                  mode='max', save_weights_only=False, monitor='val_accuracy', \n",
    "                  save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "history = model.fit(x=X_train_bert, y=y_train, batch_size=512, \n",
    "                     epochs=20, shuffle=True, verbose=1,\n",
    "                     validation_data=(X_valid_bert, y_valid),\n",
    "                     callbacks=[callback_es, callback_mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.Model([idx, masks], y_out)\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "loss      ='sparse_categorical_crossentropy'\n",
    "metrics   =['accuracy']\n",
    "model_loaded.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "model_loaded.load_weights(cp_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Get validation f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred_prob = model_loaded.predict(X_valid_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_valid))}\n",
    "y_valid_pred  = [dic_y_mapping[np.argmax(pred)] for pred in y_valid_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score = f1_score(y_valid, y_valid_pred)\n",
    "print(valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ec201",
   "metadata": {},
   "source": [
    "#### _Evaluate model with test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_prob = model.predict(X_test_bert)\n",
    "y_test_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred_prob]\n",
    "test_score  = f1_score(y_test, y_test_pred)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd3af54f5fe992bccbd23931b262c263c643af7052ca64c3b616d552ec510a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
