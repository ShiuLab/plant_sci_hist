{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 2c: BERT models - scibert__\n",
    "\n",
    "Follow:\n",
    "- https://www.kaggle.com/code/gcspkmdr/scibert-wrapped-in-tf2/notebook\n",
    "- https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch for working with SciBERT\n",
    "# %conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Imports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For building text classification model based on embedding of Word2Vec and BERT\n",
    "'''\n",
    "\n",
    "## for data\n",
    "#import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'bert_param':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, valid, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  corpus = corpus_combo[['label','txt','txt_clean']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  print(f\"    train={train.shape}, valid={valid.shape},\" +\\\n",
    "        f\" test={test.shape}\")\n",
    "\n",
    "  return [train, valid, test]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get training/testing split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read configuration file...\n",
      "   lang_model = bert\n",
      "   proj_dir = /home/shius/projects/plant_sci_hist\n",
      "   work_dir = 2_text_classify\n",
      "   corpus_combo_file = corpus_combo\n",
      "   rand_state = 20220609\n",
      "   bert_param = {}\n",
      "  all config available\n"
     ]
    }
   ],
   "source": [
    "config_file = Path(\"config_bert.txt\")\n",
    "\n",
    "print(\"\\nRead configuration file...\")\n",
    "config_dict = read_configs(config_file)\n",
    "\n",
    "# Set up working directory and corpus file location\n",
    "proj_dir          = Path(config_dict['proj_dir'])\n",
    "work_dir          = proj_dir / config_dict['work_dir']\n",
    "corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# For reproducibility\n",
    "rand_state = config_dict['rand_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read file and split train/validate/test...\n",
      "    train=(51987, 3), valid=(17329, 3), test=(17330, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split train/validate/test for cleaned text\n",
    "#   Will not focus on original due to issues with non-alphanumeric characters\n",
    "#   and stop words.\n",
    "print(\"\\nRead file and split train/validate/test...\")\n",
    "[train, valid, test] = split_train_validate_test(corpus_combo_file, rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to Datasets\n",
    "dataset_train = Dataset.from_pandas(train)\n",
    "dataset_valid = Dataset.from_pandas(valid)\n",
    "dataset_test  = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Set up SciBERT__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get SciBERT model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\\n!tar -xvf ./scibert_scivocab_uncased.tar.gz\\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\\n!transformers-cli convert --model_type bert   --tf_checkpoint \\'./scibert_scivocab_uncased/bert_model.ckpt\\'   --config \\'./scibert_scivocab_uncased/bert_config.json\\'   --pytorch_dump_output \\'./scibert_scivocab_uncased/pytorch_model.bin\\'\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/tensorflow_models/scibert_scivocab_uncased.tar.gz\n",
    "!tar -xvf ./scibert_scivocab_uncased.tar.gz\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n",
    "!transformers-cli convert --model_type bert \\\n",
    "  --tf_checkpoint './scibert_scivocab_uncased/bert_model.ckpt' \\\n",
    "  --config './scibert_scivocab_uncased/bert_config.json' \\\n",
    "  --pytorch_dump_output './scibert_scivocab_uncased/pytorch_model.bin'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "BATCH_SIZE      = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "NR_EPOCHS       = 1\n",
    "MAX_LEN         = 512 # try diffrent lengths\n",
    "threshold       = 0.4\n",
    "bert_model_name = './scibert_scivocab_uncased'\n",
    "config          = transformers.BertConfig.from_json_file(\n",
    "                                './scibert_scivocab_uncased/bert_config.json')\n",
    "\n",
    "# Missed this line earlier and the loaded model has the outputs so later on when\n",
    "# I am trying to put the layers together, there was an error.\n",
    "config.output_hidden_states = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 14:52:27.616836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:27.621103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:27.621716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:27.624055: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-17 14:52:27.626491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:27.626926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:27.627313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:32.397621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:32.398024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:32.398038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-17 14:52:32.398430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-17 14:52:32.398816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20363 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2022-06-17 14:52:35.316413: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "scibert = transformers.TFBertModel.from_pretrained(bert_model_name, \n",
    "                        from_pt=True, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109918464 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,918,464\n",
      "Trainable params: 109,918,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "scibert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# From:\\n# https://www.kaggle.com/code/gcspkmdr/scibert-wrapped-in-tf2/notebook\\nclass BertClassifier(tf.keras.Model):        \\n  def __init__(self, bert: TFBertModel, num_classes: int):\\n      \\n      super().__init__()\\n      \\n      self.bert = bert\\n      \\n      self.classifier = Dense(num_classes, activation='sigmoid')\\n      \\n  def call(self, input_ids, attention_mask=None, token_type_ids=None, \\n           position_ids=None, head_mask=None):\\n      \\n      outputs = self.bert(input_ids,\\n                          attention_mask=attention_mask,\\n                          token_type_ids=token_type_ids,\\n                          position_ids=position_ids,\\n                          head_mask=head_mask)\\n      \\n      cls_output = outputs[1]\\n      \\n      cls_output = self.classifier(cls_output)\\n              \\n      return cls_output\\n\\nmodel = transformers.BertClassifier(scibert, 2)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# From:\n",
    "# https://www.kaggle.com/code/gcspkmdr/scibert-wrapped-in-tf2/notebook\n",
    "class BertClassifier(tf.keras.Model):        \n",
    "  def __init__(self, bert: TFBertModel, num_classes: int):\n",
    "      \n",
    "      super().__init__()\n",
    "      \n",
    "      self.bert = bert\n",
    "      \n",
    "      self.classifier = Dense(num_classes, activation='sigmoid')\n",
    "      \n",
    "  def call(self, input_ids, attention_mask=None, token_type_ids=None, \n",
    "           position_ids=None, head_mask=None):\n",
    "      \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          position_ids=position_ids,\n",
    "                          head_mask=head_mask)\n",
    "      \n",
    "      cls_output = outputs[1]\n",
    "      \n",
    "      cls_output = self.classifier(cls_output)\n",
    "              \n",
    "      return cls_output\n",
    "\n",
    "model = transformers.BertClassifier(scibert, 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "                                          bert_model_name, do_lower_case=True,\n",
    "                                          config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get a list of list of texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training texts to a folder where each file has 5000 entries.\n",
    "corpus_train_path = work_dir / \"corpus_train\"\n",
    "corpus_train_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note that I use the original text for training tokenizer\n",
    "txts  = train['txt'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Train tokeinzier_\n",
    "\n",
    "- https://huggingface.co/course/chapter6/2?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = 52000\n",
    "# maximum sequence length, lowering will result to faster training (when \n",
    "# increasing batch size)\n",
    "max_length = 512\n",
    "min_frequency=2\n",
    "\n",
    "# take iterator of sequences\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(txts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31090, 52000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_tokenizer), len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer and reload\n",
    "model_path = work_dir / \"model_cln_bert_scibert\"\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the tokenizer  \n",
    "tokenizer.save_pretrained(str(model_path))\n",
    "\n",
    "# This step is critical: the trained tokenizer object cannot be called directly.\n",
    "tokenizer_loaded = transformers.BertTokenizerFast.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2604605",
   "metadata": {},
   "source": [
    "## __Set up model__\n",
    "\n",
    "For scibert:\n",
    "- https://analyticsindiamag.com/guide-to-scibert-a-pre-trained-bert-based-language-model-for-scientific-text/\n",
    "- https://www.kaggle.com/code/gcspkmdr/scibert-wrapped-in-tf2/notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e07730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inputs\n",
    "idx   = tf.keras.layers.Input((max_length), dtype=\"int32\", name=\"input_idx\")\n",
    "masks = tf.keras.layers.Input((max_length), dtype=\"int32\", name=\"input_masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 512, 768) dtype=float32 (created by layer 'tf_bert_model')>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_out = scibert(idx, attention_mask=masks)[0]\n",
    "bert_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up additional layers for fine-tuning\n",
    "x     = tf.keras.layers.GlobalAveragePooling1D()(bert_out)\n",
    "x     = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "y_out = tf.keras.layers.Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile\n",
    "model = tf.keras.models.Model([idx, masks], y_out)\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_idx (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_masks (InputLayer)       [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109918464   ['input_idx[0][0]',              \n",
      "                                thPoolingAndCrossAt               'input_masks[0][0]']            \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           49216       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            130         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,967,810\n",
      "Trainable params: 49,346\n",
      "Non-trainable params: 109,918,464\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "loss      ='sparse_categorical_crossentropy'\n",
    "metrics   =['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "              \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Text classification with transfer learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Set up train, valid, and test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datasets.arrow_dataset.Dataset, list)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pre-processed text data is used for encoding purpose. From dataset data\n",
    "# type, the returned object from here are lists.\n",
    "X_train = dataset_train['txt_clean']\n",
    "X_valid = dataset_valid['txt_clean']\n",
    "X_test  = dataset_test['txt_clean']\n",
    "type(dataset_train), type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up labels: cannot get these from dataset type since list cannot be used\n",
    "# to store labels for the model.fit function below. Instead, get them from\n",
    "# the original dataframe\n",
    "y_train = train['label']\n",
    "y_valid = valid['label']\n",
    "y_test  = test['label']\n",
    "type(train), type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Encode corpus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to encode text data in batches\n",
    "def batch_encode(tokenizer, texts, batch_size=256):\n",
    "  \"\"\"\"\"\"\"\"\"\n",
    "  A function that encodes a batch of texts and returns the texts'\n",
    "  corresponding encodings and attention masks that are ready to be fed \n",
    "  into a pre-trained transformer model.\n",
    "  \n",
    "  Input:\n",
    "  - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "  - texts:       List of strings where each string represents a text\n",
    "  - batch_size:  Integer controlling number of texts in a batch\n",
    "  - max_length:  Integer controlling max number of words to tokenize in a\n",
    "    given text\n",
    "  Output:\n",
    "  - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "  - attention_mask: the texts' attention mask encoded as a tf.Tensor obj\n",
    "  \"\"\"\"\"\"\"\"\"\n",
    "  # Define the maximum number of words to tokenize (up to 512)\n",
    "  max_length = 512\n",
    "  input_ids = []\n",
    "  attention_mask = []\n",
    "  \n",
    "  for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    inputs = tokenizer.batch_encode_plus(batch,\n",
    "                                          max_length=max_length,\n",
    "                                          padding='max_length',\n",
    "                                          truncation=True,\n",
    "                                          return_attention_mask=True,\n",
    "                                          return_token_type_ids=False\n",
    "                                          )\n",
    "    input_ids.extend(inputs['input_ids'])\n",
    "    attention_mask.extend(inputs['attention_mask'])\n",
    "  \n",
    "  return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bc73294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode corpus\n",
    "X_train_ids, X_train_attn = batch_encode(tokenizer_loaded, X_train)\n",
    "X_valid_ids, X_valid_attn = batch_encode(tokenizer_loaded, X_valid)\n",
    "X_test_ids , X_test_attn  = batch_encode(tokenizer_loaded, X_test)\n",
    "\n",
    "X_train_bert = [np.asarray(X_train_ids, dtype='int32'),\n",
    "                np.asarray(X_train_attn, dtype='int32')]\n",
    "X_valid_bert = [np.asarray(X_valid_ids, dtype='int32'),\n",
    "                np.asarray(X_valid_attn, dtype='int32')]\n",
    "X_test_bert  = [np.asarray(X_test_ids, dtype='int32'),\n",
    "                np.asarray(X_test_attn, dtype='int32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25bb1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following is done in the original tutorial because the classes are not\n",
    "## in numbers. So this can be skipped.\n",
    "# encode y\n",
    "#dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "#inverse_dic   = {v:k for k,v in dic_y_mapping.items()}\n",
    "#y_train_label = np.array([inverse_dic[y] for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup callbacks\n",
    "callback_es  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "callback_mcp = tf.keras.callbacks.ModelCheckpoint(filepath=model_path, \n",
    "                  mode='max', save_weights_only=False, monitor='val_accuracy', \n",
    "                  save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.4601 - accuracy: 0.7889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 15:06:42.806207: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 759s 2s/step - loss: 0.4601 - accuracy: 0.7889 - val_loss: 0.3773 - val_accuracy: 0.8338\n",
      "Epoch 2/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.8313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 784s 2s/step - loss: 0.3788 - accuracy: 0.8313 - val_loss: 0.3601 - val_accuracy: 0.8406\n",
      "Epoch 3/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.8393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 772s 2s/step - loss: 0.3627 - accuracy: 0.8393 - val_loss: 0.3359 - val_accuracy: 0.8516\n",
      "Epoch 4/20\n",
      "407/407 [==============================] - 737s 2s/step - loss: 0.3456 - accuracy: 0.8466 - val_loss: 0.3392 - val_accuracy: 0.8509\n",
      "Epoch 5/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.8506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 704s 2s/step - loss: 0.3397 - accuracy: 0.8506 - val_loss: 0.3218 - val_accuracy: 0.8578\n",
      "Epoch 6/20\n",
      "407/407 [==============================] - 668s 2s/step - loss: 0.3278 - accuracy: 0.8567 - val_loss: 0.3301 - val_accuracy: 0.8553\n",
      "Epoch 7/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.8587"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 701s 2s/step - loss: 0.3249 - accuracy: 0.8587 - val_loss: 0.3195 - val_accuracy: 0.8593\n",
      "Epoch 8/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3196 - accuracy: 0.8612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 702s 2s/step - loss: 0.3196 - accuracy: 0.8612 - val_loss: 0.3087 - val_accuracy: 0.8657\n",
      "Epoch 9/20\n",
      "407/407 [==============================] - 669s 2s/step - loss: 0.3175 - accuracy: 0.8633 - val_loss: 0.3203 - val_accuracy: 0.8614\n",
      "Epoch 10/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8633"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 703s 2s/step - loss: 0.3175 - accuracy: 0.8633 - val_loss: 0.3025 - val_accuracy: 0.8683\n",
      "Epoch 11/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 702s 2s/step - loss: 0.3132 - accuracy: 0.8651 - val_loss: 0.2981 - val_accuracy: 0.8714\n",
      "Epoch 12/20\n",
      "407/407 [==============================] - 668s 2s/step - loss: 0.3069 - accuracy: 0.8703 - val_loss: 0.2972 - val_accuracy: 0.8706\n",
      "Epoch 13/20\n",
      "407/407 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8704"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 701s 2s/step - loss: 0.3011 - accuracy: 0.8704 - val_loss: 0.2951 - val_accuracy: 0.8739\n",
      "Epoch 14/20\n",
      "407/407 [==============================] - 669s 2s/step - loss: 0.3005 - accuracy: 0.8714 - val_loss: 0.2996 - val_accuracy: 0.8725\n",
      "Epoch 15/20\n",
      "407/407 [==============================] - 669s 2s/step - loss: 0.3053 - accuracy: 0.8686 - val_loss: 0.3089 - val_accuracy: 0.8680\n",
      "Epoch 16/20\n",
      "407/407 [==============================] - 669s 2s/step - loss: 0.3009 - accuracy: 0.8705 - val_loss: 0.3275 - val_accuracy: 0.8582\n",
      "Epoch 17/20\n",
      "407/407 [==============================] - 669s 2s/step - loss: 0.2941 - accuracy: 0.8741 - val_loss: 0.3040 - val_accuracy: 0.8694\n",
      "Epoch 18/20\n",
      "407/407 [==============================] - 669s 2s/step - loss: 0.2959 - accuracy: 0.8734 - val_loss: 0.3064 - val_accuracy: 0.8651\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "history = model.fit(x=X_train_bert, y=y_train, batch_size=128, \n",
    "                     epochs=20, shuffle=True, verbose=1,\n",
    "                     validation_data=(X_valid_bert, y_valid),\n",
    "                     callbacks=[callback_es, callback_mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 09:07:18.704842: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert: FAILED_PRECONDITION: /home/shius/projects/plant_sci_hist/2_text_classify/model_cln_bert_scibert; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f91c898d240>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_loaded = tf.keras.models.Model([idx, masks], y_out)\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "loss      ='sparse_categorical_crossentropy'\n",
    "metrics   =['accuracy']\n",
    "model_loaded.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "model_loaded.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Get validation f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred_prob = model_loaded.predict(X_valid_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_valid))}\n",
    "y_valid_pred  = [dic_y_mapping[np.argmax(pred)] for pred in y_valid_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8736775163323119\n"
     ]
    }
   ],
   "source": [
    "valid_score = f1_score(y_valid, y_valid_pred)\n",
    "print(valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ec201",
   "metadata": {},
   "source": [
    "#### _Evaluate model with test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8715057825303656\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_prob = model.predict(X_test_bert)\n",
    "y_test_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred_prob]\n",
    "test_score  = f1_score(y_test, y_test_pred)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd3af54f5fe992bccbd23931b262c263c643af7052ca64c3b616d552ec510a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
