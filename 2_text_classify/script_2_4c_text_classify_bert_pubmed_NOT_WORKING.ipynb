{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "# __Test another way to finetune BERT__\n",
    "\n",
    "Based on [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) from TensorflowHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:29:38.728372Z",
     "iopub.status.busy": "2022-03-29T12:29:38.727971Z",
     "iopub.status.idle": "2022-03-29T12:29:40.581474Z",
     "shell.execute_reply": "2022-03-29T12:29:40.580473Z"
    },
    "id": "q-YbjCkzw0yU"
   },
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Install_\n",
    "\n",
    "For tensorflow\n",
    "```bash\n",
    "  pip install -U \"tensorflow-text==2.8.*\"\n",
    "  #pip install tf-models-official==2.7.0\n",
    "  # Run into tensorflow_model module not found error. \n",
    "  # Try without specify version\n",
    "  pip install tf-models-official\n",
    "````\n",
    "\n",
    "For running in vscode:\n",
    "```bash\n",
    "  conda install -n bert_finetune ipykernel --update-deps --force-reinstall\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:29:49.396206Z",
     "iopub.status.busy": "2022-03-29T12:29:49.395613Z",
     "iopub.status.idle": "2022-03-29T12:29:52.068483Z",
     "shell.execute_reply": "2022-03-29T12:29:52.067720Z"
    },
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_models as tfm\n",
    "from official.nlp import optimization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Configuration info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting paths\n",
    "work_dir          = Path.home() / \"projects/plant_sci_hist/2_text_classify\"\n",
    "corpus_combo_file = work_dir / \"corpus_combo\"\n",
    "\n",
    "# Dataset\n",
    "batch_size     = 16\n",
    "# Was 32, cannot run\n",
    "# Changed to 16, run on RTX 3090\n",
    "# Changed to 2, still cannot run on 3070.\n",
    "\n",
    "\n",
    "shuffle_buffer = 2\n",
    "\n",
    "# https://stackoverflow.com/questions/56613155/tensorflow-tf-data-autotune\n",
    "# tf.data builds a performance model of the input pipeline and runs an \n",
    "# optimization algorithm to find a good allocation of its CPU budget across all\n",
    "# parameters specified as AUTOTUNE\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# maximum number of tokens in a document\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Get text ready__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Read json to dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, test, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus = corpus_combo[['label','txt']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_train_validate_test(corpus_combo_file, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Save text entries into files_\n",
    "\n",
    "Follow the same structure as the IMDB dataset in the `aclImdb` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = work_dir / \"corpus_dir\"\n",
    "\n",
    "# Create train, valid, test dir\n",
    "train_dir  = corpus_dir / 'train'\n",
    "valid_dir  = corpus_dir / 'valid'\n",
    "test_dir   = corpus_dir / 'test'\n",
    "\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "valid_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pos, neg directory for each\n",
    "(train_dir / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(train_dir / \"neg\").mkdir(parents=True, exist_ok=True)\n",
    "(valid_dir / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(valid_dir / \"neg\").mkdir(parents=True, exist_ok=True)\n",
    "(test_dir  / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(test_dir  / \"neg\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entry_to_file(df, target_dir):\n",
    "  '''Save each text entry in the dataframe as a file\n",
    "  '''\n",
    "\n",
    "  labels = df['label'].values\n",
    "  txts   = df['txt'].values\n",
    "  c_dict = {0:0, 1:0}\n",
    "  for count, label in tqdm(enumerate(labels), total=len(labels)):\n",
    "    if label == 0:\n",
    "      c_dict[0] += 1\n",
    "      with open(target_dir / f\"neg/{count}.txt\", \"w\") as f:\n",
    "        f.write(txts[count])\n",
    "    else:\n",
    "      c_dict[1] += 1\n",
    "      with open(target_dir / f\"pos/{count}.txt\", \"w\") as f:\n",
    "        f.write(txts[count])\n",
    "\n",
    "  print(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make into comments because they have been done\n",
    "#save_entry_to_file(train, train_dir)\n",
    "#save_entry_to_file(valid, valid_dir)\n",
    "#save_entry_to_file(test , test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Create datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51987 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 12:05:53.246457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.255299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.255708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.256389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-21 12:05:53.258001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.258464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.258852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.937253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.937669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.937682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-21 12:05:53.938071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-21 12:05:53.938113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train dataset\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(str(train_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "type(raw_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['neg', 'pos'],\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = raw_train_ds.class_names\n",
    "train_ds    = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "class_names, train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17329 files belonging to 2 classes.\n",
      "Found 17330 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Get valition set\n",
    "raw_valid_ds = tf.keras.utils.text_dataset_from_directory(str(valid_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "valid_ds     = raw_valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Get test set\n",
    "raw_test_ds  = tf.keras.utils.text_dataset_from_directory(str(test_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "test_ds      = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b'Genome-Wide Identification of VQ Motif-Containing '\n",
      "Label : 1 (pos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 12:06:40.574739: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check out one record\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(f'Text: {text_batch.numpy()[i][:50]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Hub models and initial testing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "### _Hub models to use_\n",
    "\n",
    "Use BERT trained on MEDLINE/Pubmed:\n",
    "- https://tfhub.dev/google/experts/bert/pubmed/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_encoder = 'https://tfhub.dev/google/experts/bert/pubmed/2'\n",
    "tfhub_preproc = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "### _Load and test preprocessing model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.795438Z",
     "iopub.status.busy": "2022-03-29T12:30:15.795041Z",
     "iopub.status.idle": "2022-03-29T12:30:18.992854Z",
     "shell.execute_reply": "2022-03-29T12:30:18.992262Z"
    },
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_preproc)\n",
    "type(bert_preprocess_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note this is not a layer but a saved model\n",
    "bert_preprocess = hub.load(tfhub_preproc)\n",
    "type(bert_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:18.996679Z",
     "iopub.status.busy": "2022-03-29T12:30:18.996239Z",
     "iopub.status.idle": "2022-03-29T12:30:19.160173Z",
     "shell.execute_reply": "2022-03-29T12:30:19.159548Z"
    },
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
      "Shape      : (1, 512)\n",
      "Word Ids   : [  101  2023  3259  2003  2055  3269  1010  2066 21154  1010  5785  1010\n",
      "  1998 20856   999   102  2023  3259  2003  2055  3269  1010  2066 21154\n",
      "  1010  5785  1010  1998 20856   999]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['This paper is about Plant, like maize, rice, and tomato!']\n",
    "\n",
    "#######################\n",
    "# CRITICAL STEP!!! NEED TO CHANGE DIMENSION From 128 to 512\n",
    "#######################\n",
    "\n",
    "tok = bert_preprocess.tokenize(tf.constant(text_test))\n",
    "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], \n",
    "                                                     tf.constant(max_length))\n",
    "#text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "\n",
    "# The size is 128.\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :30]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :30]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :30]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "### _Load and test BERT model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:19.163550Z",
     "iopub.status.busy": "2022-03-29T12:30:19.163058Z",
     "iopub.status.idle": "2022-03-29T12:30:26.095648Z",
     "shell.execute_reply": "2022-03-29T12:30:26.094996Z"
    },
    "id": "tXxYpK8ixL34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_encoder)\n",
    "type(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.099587Z",
     "iopub.status.busy": "2022-03-29T12:30:26.098983Z",
     "iopub.status.idle": "2022-03-29T12:30:26.708358Z",
     "shell.execute_reply": "2022-03-29T12:30:26.707624Z"
    },
    "id": "_OoF9mebuSZc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/google/experts/bert/pubmed/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 12:06:54.636726: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "print(f'Loaded BERT: {tfhub_encoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Outputs Shape:(1, 768)\n",
      "Pooled Outputs Values:[ 0.19967358 -0.6023183   0.01133679 -0.94678575 -0.32777378  0.3923298\n",
      " -0.90228444]\n"
     ]
    }
   ],
   "source": [
    "# pooled_output: embedding of the document\n",
    "# 768: size of the embedding vector\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Outputs Shape:(1, 512, 768)\n",
      "Sequence Outputs Values:[[ 0.20244306 -0.6965506   0.01134092 ...  0.07541777  0.804416\n",
      "   0.43197766]\n",
      " [-0.8271446  -1.1451745   0.23592134 ... -0.15745929 -0.5484226\n",
      "  -2.219939  ]]\n"
     ]
    }
   ],
   "source": [
    "# sequence_output: embeddings of each token\n",
    "# 512: number of tokens of text_preprocessed\n",
    "# 768: size of the embedding vector\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Outputs length:12\n",
      "Sequence Outputs shape:(1, 512, 768)\n",
      "Sequence Outputs shape:[[ 0.04164825 -0.05509862  0.05931972 ... -0.07429437  0.01309458\n",
      "   0.04005507]\n",
      " [-0.7039484  -0.5594542   0.16843846 ... -0.15212026  0.34699732\n",
      "  -0.403786  ]]\n"
     ]
    }
   ],
   "source": [
    "# encoder_outputs: intermediate activation of a transformer block\n",
    "# Q: Assuming activation is the output value of the activation function.\n",
    "# 12: number of transformer blocks\n",
    "print(f'Encoder Outputs length:{len(bert_results[\"encoder_outputs\"])}')\n",
    "\n",
    "# Saem as sequence output values\n",
    "print(f'Sequence Outputs shape:{bert_results[\"encoder_outputs\"][0].shape}')\n",
    "print(f'Sequence Outputs shape:{bert_results[\"encoder_outputs\"][0][0, :2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## __Build classification model__\n",
    "\n",
    "6/18/19\n",
    "- The challenge is how to use bert_pack_inputs as a prprocessing layer.\n",
    "- [This hub page](https://www.tensorflow.org/hub/common_saved_model_apis/text) has some info: does not help much, as the info is for individual instance and the model is very different from what I want.\n",
    "- [Fine-tuning a BERT model](https://www.tensorflow.org/text/tutorials/fine_tune_bert) tutorial: Here the dataset is tokenized, packed, and them used as input to model. Try it and see.\n",
    "- Also check the [preprocess model page](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) for syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get tokenizer to work_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original example\n",
    "#tok = bert_preprocess.tokenize(tf.constant(text_test))\n",
    "#text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], \n",
    "#                                                     tf.constant(max_length))\n",
    "tokenizer  = hub.KerasLayer(bert_preprocess.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[[7592], [23435, 12314]]]>,\n",
       " <tf.RaggedTensor [[[9119], [23435, 12314]]]>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1 = tf.constant([\"hello tensorflow\"])\n",
    "sentences2 = tf.constant([\"goodbye tensorflow\"])\n",
    "tokens1    = tokenizer(sentences1)\n",
    "tokens2    = tokenizer(sentences2)\n",
    "tokens1, tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_id': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'end_of_segment_id': <tf.Tensor: shape=(), dtype=int32, numpy=102>,\n",
       " 'mask_id': <tf.Tensor: shape=(), dtype=int32, numpy=103>,\n",
       " 'start_of_sequence_id': <tf.Tensor: shape=(), dtype=int32, numpy=101>,\n",
       " 'vocab_size': <tf.Tensor: shape=(), dtype=int32, numpy=30522>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following throw an error:\n",
    "# AttributeError: 'KerasLayer' object has no attribute 'get_special_tokens_dict'\n",
    "#special = tokenizer.get_special_tokens_dict\n",
    "\n",
    "# Ok, in the \"Custom input packing and MLM support\" section of this page:\n",
    "# https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
    "# There is this line:\n",
    "special = bert_preprocess.tokenize.get_special_tokens_dict()\n",
    "special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get Packer to work_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_length,\n",
    "    special_tokens_dict = special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_word_ids : (1, 512), [[  101  7592 23435 12314   102  9119 23435 12314   102     0]]\n",
      "input_mask     : (1, 512), [[1 1 1 1 1 1 1 1 1 0]]\n",
      "input_type_ids : (1, 512), [[0 0 0 0 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "packed = packer([tokens1, tokens2])\n",
    "for key, tensor in packed.items():\n",
    "  print(f\"{key:15s}: {tensor.shape}, {tensor[:, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Combine tokenizer and packer into a layer_\n",
    "\n",
    "The following class definition is from [this colab file](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/fine_tune_bert.ipynb?hl=sv-SEIt&skip_cache=false).\n",
    "- In the `train_ds.map(bert_inputs_processor).prefetch(1)` line below, there is this error:\n",
    "  - TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() takes 2 positional arguments but 3 were given\n",
    "- Looking into how the class is implemented, it is asking for `inputs['txt']` and `inputs['label']`. But train_ds does not have that kind of structure. This is likely the reason why this fails.\n",
    "- So instead, put the tokenizer and packer into separate layers in the `define model architecture` section below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not work for me. \n",
    "class BertInputProcessor(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, packer):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.packer = packer\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Original code is expecting two features, but I only have one.\n",
    "    #tok1 = self.tokenizer(inputs['sentence1'])\n",
    "    #tok2 = self.tokenizer(inputs['sentence2'])\n",
    "    tok = self.tokenizer(inputs['txt'])\n",
    "\n",
    "    packed = self.packer([tok, tok])\n",
    "\n",
    "    if 'label' in inputs:\n",
    "      return packed, inputs['label']\n",
    "    else:\n",
    "      return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_inputs_processor = BertInputProcessor(tokenizer, packer)\n",
    "#train_packed          = train_ds.map(bert_inputs_processor).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Define model architecture_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.711934Z",
     "iopub.status.busy": "2022-03-29T12:30:26.711712Z",
     "iopub.status.idle": "2022-03-29T12:30:26.716508Z",
     "shell.execute_reply": "2022-03-29T12:30:26.715939Z"
    },
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  # Input layer\n",
    "  text_input        = tf.keras.layers.Input(shape=(), dtype=tf.string, \n",
    "                                            name='txt')\n",
    "\n",
    "  # Will this work??  \n",
    "  tokenizer         = hub.KerasLayer(bert_preprocess.tokenize)\n",
    "  special           = bert_preprocess.tokenize.get_special_tokens_dict()\n",
    "  tokenizer_outputs = tokenizer(text_input)\n",
    "\n",
    "  # Processing layer: This has the key change to allow longer texts.\n",
    "  packer            = tfm.nlp.layers.BertPackInputs(\n",
    "                      seq_length=max_length,\n",
    "                      special_tokens_dict = special)\n",
    "\n",
    "  #preproc_layer     = hub.KerasLayer(bert_preprocess.bert_pack_inputs, \n",
    "  #                                   arguments=dict(seq_lenght=max_length),\n",
    "  #                                   name='preprocessing')\n",
    "                                     \n",
    "  packer_outputs   = packer(tokenizer_outputs)\n",
    "\n",
    "  # Initialize encoder\n",
    "  encoder           = hub.KerasLayer(tfhub_encoder, trainable=True, \n",
    "                                   name='BERT_encoder')\n",
    "  encoder_outputs   = encoder(packer_outputs)\n",
    "  \n",
    "  # Q: Wonder if this is the dense layer mentioned above.\n",
    "  print(type(encoder_outputs))\n",
    "\n",
    "  # Get just the embeddings for each doc (ignore token level info)\n",
    "  net            = encoder_outputs['pooled_output']\n",
    "\n",
    "  # Dropout layer\n",
    "  net            = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "  # output layer: single node, Q: Why??\n",
    "  net            = tf.keras.layers.Dense(2, activation='softmax', \n",
    "                                                        name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.719652Z",
     "iopub.status.busy": "2022-03-29T12:30:26.719155Z",
     "iopub.status.idle": "2022-03-29T12:30:33.483247Z",
     "shell.execute_reply": "2022-03-29T12:30:33.482586Z"
    },
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "Raw result   : tf.Tensor([[0.54030627 0.4596937 ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test        = ['Plant science focuses on studies of photosynthetic species.']\n",
    "\n",
    "# tf.constant: create a Tensor from tensor like objects\n",
    "tensor_test      = tf.constant(text_test)\n",
    "bert_raw_result  = classifier_model(tensor_test)\n",
    "\n",
    "print(\"Raw result   :\", bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.486539Z",
     "iopub.status.busy": "2022-03-29T12:30:33.486005Z",
     "iopub.status.idle": "2022-03-29T12:30:33.607583Z",
     "shell.execute_reply": "2022-03-29T12:30:33.606965Z"
    },
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " txt (InputLayer)               [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_3 (KerasLayer)     (None, None, None)   0           ['txt[0][0]']                    \n",
      "                                                                                                  \n",
      " bert_pack_inputs_1 (BertPackIn  {'input_word_ids':   0          ['keras_layer_3[0][0]']          \n",
      " puts)                          (None, 512),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 512),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 512)}                                                      \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'encoder_outputs':  109482241   ['bert_pack_inputs_1[0][0]',     \n",
      "                                 [(None, 512, 768),               'bert_pack_inputs_1[0][1]',     \n",
      "                                 (None, 512, 768),                'bert_pack_inputs_1[0][2]']     \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768)],                                               \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'default': (None,                                                \n",
      "                                768)}                                                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['BERT_encoder[0][13]']          \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 2)            1538        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,779\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## __Model training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### _Compile model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.624696Z",
     "iopub.status.busy": "2022-03-29T12:30:33.624232Z",
     "iopub.status.idle": "2022-03-29T12:30:33.628769Z",
     "shell.execute_reply": "2022-03-29T12:30:33.628248Z"
    },
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs           = 20\n",
    "cardinality      = tf.data.experimental.cardinality(train_ds)\n",
    "steps_per_epoch  = cardinality.numpy()\n",
    "num_train_steps  = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "# loss function: \n",
    "loss    = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# evaluation metrics\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "# Initial learning rate\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.631424Z",
     "iopub.status.busy": "2022-03-29T12:30:33.631013Z",
     "iopub.status.idle": "2022-03-29T12:30:33.640619Z",
     "shell.execute_reply": "2022-03-29T12:30:33.640151Z"
    },
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### _Train model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify callbacks\n",
    "\n",
    "# early stopping\n",
    "callback_es  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# model checkpoint\n",
    "cp_filepath  = work_dir / \"model_ori_bert_tf_pubmed\"\n",
    "callback_mcp = tf.keras.callbacks.ModelCheckpoint(filepath=str(cp_filepath), \n",
    "              mode='max', save_weights_only=False, monitor='val_accuracy', \n",
    "              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.643759Z",
     "iopub.status.busy": "2022-03-29T12:30:33.643317Z",
     "iopub.status.idle": "2022-03-29T12:37:37.231432Z",
     "shell.execute_reply": "2022-03-29T12:37:37.230870Z"
    },
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/google/experts/bert/pubmed/2\n",
      "Epoch 1/20\n",
      "3250/3250 [==============================] - ETA: 0s - loss: 0.1631 - binary_accuracy: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb Cell 57'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining model with \u001b[39m\u001b[39m{\u001b[39;00mtfhub_encoder\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_ds, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=2'>3</a>\u001b[0m                                batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=3'>4</a>\u001b[0m                                epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=4'>5</a>\u001b[0m                                validation_data\u001b[39m=\u001b[39;49mvalid_ds, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=5'>6</a>\u001b[0m                                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000055vscode-remote?line=6'>7</a>\u001b[0m                                callbacks\u001b[39m=\u001b[39;49m[callback_es, callback_mcp])\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/keras/engine/training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1432\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1433\u001b[0m       x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1434\u001b[0m       y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1443\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1444\u001b[0m       steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1445\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1446\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1447\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1448\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1449\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1450\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1451\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1452\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1453\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1454\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1455\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1456\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1457\u001b[0m val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   1458\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/keras/engine/training.py:1756\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1755\u001b[0m   callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1756\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[1;32m   1757\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1758\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_encoder}')\n",
    "history = classifier_model.fit(x=train_ds, \n",
    "                               batch_size=batch_size, \n",
    "                               epochs=epochs,\n",
    "                               validation_data=valid_ds, \n",
    "                               verbose=1,\n",
    "                               callbacks=[callback_es, callback_mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:37:37.235151Z",
     "iopub.status.busy": "2022-03-29T12:37:37.234581Z",
     "iopub.status.idle": "2022-03-29T12:38:36.128910Z",
     "shell.execute_reply": "2022-03-29T12:38:36.128342Z"
    },
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:36.132081Z",
     "iopub.status.busy": "2022-03-29T12:38:36.131630Z",
     "iopub.status.idle": "2022-03-29T12:38:36.424503Z",
     "shell.execute_reply": "2022-03-29T12:38:36.424018Z"
    },
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## Export for inference\n",
    "\n",
    "Now you just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:36.428146Z",
     "iopub.status.busy": "2022-03-29T12:38:36.427717Z",
     "iopub.status.idle": "2022-03-29T12:38:42.015407Z",
     "shell.execute_reply": "2022-03-29T12:38:42.014764Z"
    },
    "id": "ShcvqJAgVera"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbI25bS1vD7s"
   },
   "source": [
    "Let's reload the model, so you can try it side by side with the model that is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:42.019271Z",
     "iopub.status.busy": "2022-03-29T12:38:42.018826Z",
     "iopub.status.idle": "2022-03-29T12:38:48.305286Z",
     "shell.execute_reply": "2022-03-29T12:38:48.304688Z"
    },
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyTappHTvNCz"
   },
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:48.309376Z",
     "iopub.status.busy": "2022-03-29T12:38:48.308953Z",
     "iopub.status.idle": "2022-03-29T12:38:48.694457Z",
     "shell.execute_reply": "2022-03-29T12:38:48.693822Z"
    },
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cOmih754Y_M"
   },
   "source": [
    "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:48.697594Z",
     "iopub.status.busy": "2022-03-29T12:38:48.697394Z",
     "iopub.status.idle": "2022-03-29T12:38:48.996870Z",
     "shell.execute_reply": "2022-03-29T12:38:48.996220Z"
    },
    "id": "0FdVD3973S-O"
   },
   "outputs": [],
   "source": [
    "serving_results = reloaded_model \\\n",
    "            .signatures['serving_default'](tf.constant(examples))\n",
    "\n",
    "serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "print_my_examples(examples, serving_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4gN1KwReLPN"
   },
   "source": [
    "# __FAILED STEPS__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _[Gave up on this] Convert training dataframe to dataset_\n",
    "\n",
    "Try many different ways, was trying not to slit text records into different files then load as a dataset like the tutorial. But converting from dataframe always missed something that I cannot quite put my finger on. Moving on. Revisit this later...\n",
    "\n",
    "- See the [pd_dataframe_to_tf_dataset](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/pd_dataframe_to_tf_dataset) function, but this needs tf 2.9, conflict with tensorflow_text.\n",
    "- See [this](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe): See the shuffle and batch functions. Does not work...\n",
    "- See [this post](https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168):  Was able to create SicedDataset, then BatchDatabase after applying the batch function, then PrefetchDataset. But trying to retreive a test example from trainin dataset lead to:\n",
    "  - InvalidArgumentError: Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/\n",
    "  - Ok, as I was implmenting the next solution, realize that I did not call the right obj for prefetch. Can be the reason why.\n",
    "- Ah, see [this post](https://stackoverflow.com/questions/58461609/how-to-convert-pandas-dataframe-to-tensorflow-dataset): key is to turn train_data to dictionary before calling from_tensor_slices.\n",
    "  - A little comment below say need to do .to_dict() instead which make sense. Because if just do dict(train), the thing finish in 0.1 sec which does not make sense. But this fails and throw:\n",
    "    - ValueError: Unbatching a tensor is only supported for rank >= 1\n",
    "  - Found [this post](https://stackoverflow.com/questions/55560620/valueerror-unbatching-a-tensor-is-only-supported-for-rank-1): Now try to uses this syntax. Still does not work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following DOES NOT work\n",
    "\n",
    "```Python\n",
    "raw_train_ds = (tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(train['txt'].values, tf.string),\n",
    "         tf.cast(train['label'].values, tf.int32),)))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(dict(train))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(train.to_dict())\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((y_train,X_train))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((i_train, \n",
    "                                                   y_train.values,\n",
    "                                                   X_train.values))\n",
    "\n",
    "The last one is almost working, but at later stage when I try to pack the input, it breaks. Upon closer examination, the train_ds looks like:\n",
    "\n",
    "```\n",
    "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
    "```\n",
    "\n",
    "But what I need is more like:\n",
    "\n",
    "```\n",
    "<PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'sentence1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'sentence2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = train.iloc[:3,:]\n",
    "raw_trial_ds = (tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(trial['txt'].values, tf.string),\n",
    "         tf.cast(trial['label'].values, tf.int32),)))\n",
    "raw_trial_ds\n",
    "# Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \"trial\" so it does not mix with the testing set\n",
    "trial_dict   = {\"idx\":[123,322], \"label\":[0,1], \n",
    "               \"txt\":[\"The 1st sentence\", \"The second\"]}\n",
    "\n",
    "# This creates a TensorSliceDataset\n",
    "raw_trial_ds =  tf.data.Dataset.from_tensor_slices(trial_dict)\n",
    "\n",
    "# Now this has the right structure!!!\n",
    "raw_trial_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_batch, label_batch, text_batch in raw_trial_ds.take(1):\n",
    "  for i in range(1):\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_trial_ds_batch = raw_trial_ds.batch(batch_size)\n",
    "type(raw_trial_ds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ds = raw_trial_ds_batch.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "len(trial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still have the right structure.\n",
    "trial_ds.element_spec['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "take1 = trial_ds.take(1)\n",
    "help(take1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train, valid, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(df):\n",
    "  '''Convert DataFrame to PrefetchDataset\n",
    "  Args:\n",
    "    df (dataframe): with two columns ('txt', and 'label') and indices\n",
    "  Return:\n",
    "    dataset (PrefetchDataset): with:\n",
    "      <PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.int32, name=None), \n",
    "                                     'label': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.int32, name=None), \n",
    "                                     'txt': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.string, name=None)}>\n",
    "\n",
    "  '''\n",
    "  idx = df.index   # aded this, as the tutorial has an index input\n",
    "  X   = df['txt']\n",
    "  y   = df['label']\n",
    "\n",
    "  df_dict = {\"idx\":idx, \"label\":y.values, \"txt\":X.values}\n",
    "\n",
    "  raw_ds       = tf.data.Dataset.from_tensor_slices(df_dict)  \n",
    "  raw_ds_batch = raw_ds.batch(batch_size)\n",
    "  dataset      = raw_ds_batch.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data\n",
    "train_ds = dataframe_to_dataset(train)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation dataset\n",
    "valid_ds = dataframe_to_dataset(valid)\n",
    "\n",
    "# Get testing dataset\n",
    "test_ds = dataframe_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Testing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  print(text_batch[0])\n",
    "  print(len(text_batch))\n",
    "  print(label_batch)\n",
    "  print(len(label_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('bert_finetune')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "323c618d0395b34183a36199d7c8eddbd4e55d51aee9dabbfbc9809db817fb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
