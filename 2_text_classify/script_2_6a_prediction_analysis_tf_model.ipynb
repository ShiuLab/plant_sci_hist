{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir      = Path.home() / \"projects/plant_sci_hist/2_text_classify\"\n",
    "tf_model_file = work_dir / \"model_tf/model_ori_1000-1to2-0.0001.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=5, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.02, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=10,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=600,\n",
       "              n_jobs=1, nthread=1, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "tf_model = joblib.load(tf_model_file)\n",
    "tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_scores = tf_model.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5</th>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9</th>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f25</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "f2    3.0\n",
       "f5   17.0\n",
       "f6    1.0\n",
       "f9    8.0\n",
       "f25   2.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_scores_df = pd.DataFrame.from_dict(feat_scores, orient='index')\n",
    "feat_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Booster in module xgboost.core object:\n",
      "\n",
      "class Booster(builtins.object)\n",
      " |  Booster(params: Optional[Dict] = None, cache: Optional[Sequence[xgboost.core.DMatrix]] = None, model_file: Union[ForwardRef('Booster'), bytearray, os.PathLike, str, NoneType] = None) -> None\n",
      " |  \n",
      " |  A Booster of XGBoost.\n",
      " |  \n",
      " |  Booster is the model of xgboost, that contains low level routines for\n",
      " |  training, prediction and evaluation.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __copy__(self) -> 'Booster'\n",
      " |  \n",
      " |  __deepcopy__(self, _: Any) -> 'Booster'\n",
      " |      Return a copy of booster.\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  __getitem__(self, val: Union[int, tuple, slice]) -> 'Booster'\n",
      " |  \n",
      " |  __getstate__(self) -> Dict\n",
      " |  \n",
      " |  __init__(self, params: Optional[Dict] = None, cache: Optional[Sequence[xgboost.core.DMatrix]] = None, model_file: Union[ForwardRef('Booster'), bytearray, os.PathLike, str, NoneType] = None) -> None\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for boosters.\n",
      " |      cache : list\n",
      " |          List of cache items.\n",
      " |      model_file : string/os.PathLike/Booster/bytearray\n",
      " |          Path to the model file if it's string or PathLike.\n",
      " |  \n",
      " |  __setstate__(self, state: Dict) -> None\n",
      " |  \n",
      " |  attr(self, key: str) -> Optional[str]\n",
      " |      Get attribute string from the Booster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key to get attribute from.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      value : str\n",
      " |          The attribute value of the key, returns None if attribute do not exist.\n",
      " |  \n",
      " |  attributes(self) -> Dict[str, str]\n",
      " |      Get attributes stored in the Booster as a dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      " |          Returns an empty dict if there's no attributes.\n",
      " |  \n",
      " |  boost(self, dtrain: xgboost.core.DMatrix, grad: numpy.ndarray, hess: numpy.ndarray) -> None\n",
      " |      Boost the booster for one iteration, with customized gradient\n",
      " |      statistics.  Like :py:func:`xgboost.Booster.update`, this\n",
      " |      function should not be called directly by users.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtrain :\n",
      " |          The training DMatrix.\n",
      " |      grad :\n",
      " |          The first order of gradient.\n",
      " |      hess :\n",
      " |          The second order of gradient.\n",
      " |  \n",
      " |  copy(self) -> 'Booster'\n",
      " |      Copy the booster object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster: `Booster`\n",
      " |          a copied booster model\n",
      " |  \n",
      " |  dump_model(self, fout: Union[str, os.PathLike], fmap: Union[str, os.PathLike] = '', with_stats: bool = False, dump_format: str = 'text') -> None\n",
      " |      Dump model into a text or JSON file.  Unlike :py:meth:`save_model`, the\n",
      " |      output format is primarily used for visualization or interpretation,\n",
      " |      hence it's more human readable but cannot be loaded back to XGBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fout : string or os.PathLike\n",
      " |          Output file name.\n",
      " |      fmap : string or os.PathLike, optional\n",
      " |          Name of the file containing feature map names.\n",
      " |      with_stats : bool, optional\n",
      " |          Controls whether the split statistics are output.\n",
      " |      dump_format : string, optional\n",
      " |          Format of model dump file. Can be 'text' or 'json'.\n",
      " |  \n",
      " |  eval(self, data: xgboost.core.DMatrix, name: str = 'eval', iteration: int = 0) -> str\n",
      " |      Evaluate the model on mat.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          The dmatrix storing the input.\n",
      " |      \n",
      " |      name :\n",
      " |          The name of the dataset.\n",
      " |      \n",
      " |      iteration :\n",
      " |          The current iteration number.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result: str\n",
      " |          Evaluation result string.\n",
      " |  \n",
      " |  eval_set(self, evals: Sequence[Tuple[xgboost.core.DMatrix, str]], iteration: int = 0, feval: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]] = None, output_margin: bool = True) -> str\n",
      " |      Evaluate a set of data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      evals :\n",
      " |          List of items to be evaluated.\n",
      " |      iteration :\n",
      " |          Current iteration.\n",
      " |      feval :\n",
      " |          Custom evaluation function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result: str\n",
      " |          Evaluation result string.\n",
      " |  \n",
      " |  get_dump(self, fmap: Union[str, os.PathLike] = '', with_stats: bool = False, dump_format: str = 'text') -> List[str]\n",
      " |      Returns the model dump as a list of strings.  Unlike :py:meth:`save_model`, the output\n",
      " |      format is primarily used for visualization or interpretation, hence it's more\n",
      " |      human readable but cannot be loaded back to XGBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap :\n",
      " |          Name of the file containing feature map names.\n",
      " |      with_stats :\n",
      " |          Controls whether the split statistics are output.\n",
      " |      dump_format :\n",
      " |          Format of model dump. Can be 'text', 'json' or 'dot'.\n",
      " |  \n",
      " |  get_fscore(self, fmap: Union[str, os.PathLike] = '') -> Dict[str, Union[float, List[float]]]\n",
      " |      Get feature importance of each feature.\n",
      " |      \n",
      " |      .. note:: Zero-importance features will not be included\n",
      " |      \n",
      " |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      " |         those features that have not been used in any split conditions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap :\n",
      " |         The name of feature map file\n",
      " |  \n",
      " |  get_score(self, fmap: Union[str, os.PathLike] = '', importance_type: str = 'weight') -> Dict[str, Union[float, List[float]]]\n",
      " |      Get feature importance of each feature.\n",
      " |      For tree model Importance type can be defined as:\n",
      " |      \n",
      " |      * 'weight': the number of times a feature is used to split the data across all trees.\n",
      " |      * 'gain': the average gain across all splits the feature is used in.\n",
      " |      * 'cover': the average coverage across all splits the feature is used in.\n",
      " |      * 'total_gain': the total gain across all splits the feature is used in.\n",
      " |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      " |         without bias.\n",
      " |      \n",
      " |      .. note:: Zero-importance features will not be included\n",
      " |      \n",
      " |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      " |         those features that have not been used in any split conditions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap:\n",
      " |         The name of feature map file.\n",
      " |      importance_type:\n",
      " |          One of the importance types defined above.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A map between feature names and their scores.  When `gblinear` is used for\n",
      " |      multi-class classification the scores for each feature is a list with length\n",
      " |      `n_classes`, otherwise they're scalars.\n",
      " |  \n",
      " |  get_split_value_histogram(self, feature: str, fmap: Union[os.PathLike, str] = '', bins: Optional[int] = None, as_pandas: bool = True) -> Union[numpy.ndarray, pandas.core.frame.DataFrame]\n",
      " |      Get split value histogram of a feature\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature: str\n",
      " |          The name of the feature.\n",
      " |      fmap: str or os.PathLike (optional)\n",
      " |          The name of feature map file.\n",
      " |      bin: int, default None\n",
      " |          The maximum number of bins.\n",
      " |          Number of bins equals number of unique split values n_unique,\n",
      " |          if bins == None or bins > n_unique.\n",
      " |      as_pandas: bool, default True\n",
      " |          Return pd.DataFrame when pandas is installed.\n",
      " |          If False or pandas is not installed, return numpy ndarray.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a histogram of used splitting values for the specified feature\n",
      " |      either as numpy array or pandas DataFrame.\n",
      " |  \n",
      " |  inplace_predict(self, data: Any, iteration_range: Tuple[int, int] = (0, 0), predict_type: str = 'value', missing: float = nan, validate_features: bool = True, base_margin: Any = None, strict_shape: bool = False) -> Any\n",
      " |      Run prediction in-place, Unlike :py:meth:`predict` method, inplace prediction does not\n",
      " |      cache the prediction result.\n",
      " |      \n",
      " |      Calling only ``inplace_predict`` in multiple threads is safe and lock\n",
      " |      free.  But the safety does not hold when used in conjunction with other\n",
      " |      methods. E.g. you can't train the booster in one thread and perform\n",
      " |      prediction in the other.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          booster.set_param({'predictor': 'gpu_predictor'})\n",
      " |          booster.inplace_predict(cupy_array)\n",
      " |      \n",
      " |          booster.set_param({'predictor': 'cpu_predictor})\n",
      " |          booster.inplace_predict(numpy_array)\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : numpy.ndarray/scipy.sparse.csr_matrix/cupy.ndarray/\n",
      " |             cudf.DataFrame/pd.DataFrame\n",
      " |          The input data, must not be a view for numpy array.  Set\n",
      " |          ``predictor`` to ``gpu_predictor`` for running prediction on CuPy\n",
      " |          array or CuDF DataFrame.\n",
      " |      iteration_range :\n",
      " |          See :py:meth:`predict` for details.\n",
      " |      predict_type :\n",
      " |          * `value` Output model prediction values.\n",
      " |          * `margin` Output the raw untransformed margin value.\n",
      " |      missing :\n",
      " |          See :py:obj:`xgboost.DMatrix` for details.\n",
      " |      validate_features:\n",
      " |          See :py:meth:`xgboost.Booster.predict` for details.\n",
      " |      base_margin:\n",
      " |          See :py:obj:`xgboost.DMatrix` for details.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      strict_shape:\n",
      " |          See :py:meth:`xgboost.Booster.predict` for details.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy.ndarray/cupy.ndarray\n",
      " |          The prediction result.  When input data is on GPU, prediction\n",
      " |          result is stored in a cupy array.\n",
      " |  \n",
      " |  load_config(self, config: str) -> None\n",
      " |      Load configuration returned by `save_config`.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the various\n",
      " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      " |      feature_names) will not be loaded when using binary format.  To save those\n",
      " |      attributes, use JSON/UBJ instead.  See :doc:`Model IO </tutorials/saving_model>`\n",
      " |      for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.load_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.load_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  num_boosted_rounds(self) -> int\n",
      " |      Get number of boosted rounds.  For gblinear this is reset to 0 after\n",
      " |      serializing the model.\n",
      " |  \n",
      " |  num_features(self) -> int\n",
      " |      Number of features in booster.\n",
      " |  \n",
      " |  predict(self, data: xgboost.core.DMatrix, output_margin: bool = False, ntree_limit: int = 0, pred_leaf: bool = False, pred_contribs: bool = False, approx_contribs: bool = False, pred_interactions: bool = False, validate_features: bool = True, training: bool = False, iteration_range: Tuple[int, int] = (0, 0), strict_shape: bool = False) -> numpy.ndarray\n",
      " |      Predict with data.  The full model will be used unless `iteration_range` is specified,\n",
      " |      meaning user have to either slice the model or use the ``best_iteration``\n",
      " |      attribute to get prediction from best model returned from early stopping.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          See :doc:`Prediction </prediction>` for issues like thread safety and a\n",
      " |          summary of outputs from this function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          The dmatrix storing the input.\n",
      " |      \n",
      " |      output_margin :\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      \n",
      " |      ntree_limit :\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      \n",
      " |      pred_leaf :\n",
      " |          When this option is on, the output will be a matrix of (nsample,\n",
      " |          ntrees) with each record indicating the predicted leaf index of\n",
      " |          each sample in each tree.  Note that the leaf index of a tree is\n",
      " |          unique per tree, so you may find leaf 1 in both tree 1 and tree 0.\n",
      " |      \n",
      " |      pred_contribs :\n",
      " |          When this is True the output will be a matrix of size (nsample,\n",
      " |          nfeats + 1) with each record indicating the feature contributions\n",
      " |          (SHAP values) for that prediction. The sum of all feature\n",
      " |          contributions is equal to the raw untransformed margin value of the\n",
      " |          prediction. Note the final column is the bias term.\n",
      " |      \n",
      " |      approx_contribs :\n",
      " |          Approximate the contributions of each feature.  Used when ``pred_contribs`` or\n",
      " |          ``pred_interactions`` is set to True.  Changing the default of this parameter\n",
      " |          (False) is not recommended.\n",
      " |      \n",
      " |      pred_interactions :\n",
      " |          When this is True the output will be a matrix of size (nsample,\n",
      " |          nfeats + 1, nfeats + 1) indicating the SHAP interaction values for\n",
      " |          each pair of features. The sum of each row (or column) of the\n",
      " |          interaction values equals the corresponding SHAP value (from\n",
      " |          pred_contribs), and the sum of the entire matrix equals the raw\n",
      " |          untransformed margin value of the prediction. Note the last row and\n",
      " |          column correspond to the bias term.\n",
      " |      \n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's\n",
      " |          feature_names are identical.  Otherwise, it is assumed that the\n",
      " |          feature_names are the same.\n",
      " |      \n",
      " |      training :\n",
      " |          Whether the prediction value is used for training.  This can effect `dart`\n",
      " |          booster, which performs dropouts during training iterations but use all trees\n",
      " |          for inference. If you want to obtain result with dropouts, set this parameter\n",
      " |          to `True`.  Also, the parameter is set to true when obtaining prediction for\n",
      " |          custom objective function.\n",
      " |      \n",
      " |          .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      strict_shape :\n",
      " |          When set to True, output shape is invariant to whether classification is used.\n",
      " |          For both value and margin prediction, the output shape is (n_samples,\n",
      " |          n_groups), n_groups == 1 when multi-class is not used.  Default to False, in\n",
      " |          which case the output shape can be (n_samples, ) if multi-class is not used.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  save_config(self) -> str\n",
      " |      Output internal parameter configuration of Booster as a JSON\n",
      " |      string.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      " |      (such as feature_names) will not be saved when using binary format.  To save\n",
      " |      those attributes, use JSON/UBJ instead. See :doc:`Model IO\n",
      " |      </tutorials/saving_model>` for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.save_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.save_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Output file name\n",
      " |  \n",
      " |  save_raw(self, raw_format: str = 'deprecated') -> bytearray\n",
      " |      Save the model to a in memory buffer representation instead of file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_format :\n",
      " |          Format of output buffer. Can be `json`, `ubj` or `deprecated`.  Right now\n",
      " |          the default is `deprecated` but it will be changed to `ubj` (univeral binary\n",
      " |          json) in the future.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      An in memory buffer representation of the model\n",
      " |  \n",
      " |  set_attr(self, **kwargs: Optional[str]) -> None\n",
      " |      Set the attribute of the Booster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs\n",
      " |          The attributes to set. Setting a value to None deletes an attribute.\n",
      " |  \n",
      " |  set_param(self, params: Union[Dict, Iterable[Tuple[str, Any]], str], value: Optional[str] = None) -> None\n",
      " |      Set parameters into the Booster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params: dict/list/str\n",
      " |         list of key,value pairs, dict of key to value or simply str key\n",
      " |      value: optional\n",
      " |         value of the specified parameter, when params is str key\n",
      " |  \n",
      " |  trees_to_dataframe(self, fmap: Union[str, os.PathLike] = '') -> pandas.core.frame.DataFrame\n",
      " |      Parse a boosted tree model text dump into a pandas DataFrame structure.\n",
      " |      \n",
      " |      This feature is only defined when the decision tree model is chosen as base\n",
      " |      learner (`booster in {gbtree, dart}`). It is not defined for other base learner\n",
      " |      types, such as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fmap: str or os.PathLike (optional)\n",
      " |         The name of feature map file.\n",
      " |  \n",
      " |  update(self, dtrain: xgboost.core.DMatrix, iteration: int, fobj: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[numpy.ndarray, numpy.ndarray]]] = None) -> None\n",
      " |      Update for one iteration, with objective function calculated\n",
      " |      internally.  This function should not be called directly by users.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtrain : DMatrix\n",
      " |          Training data.\n",
      " |      iteration : int\n",
      " |          Current iteration number.\n",
      " |      fobj : function\n",
      " |          Customized objective function.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  feature_names\n",
      " |      Feature names for this booster.  Can be directly set by input data or by\n",
      " |      assignment.\n",
      " |  \n",
      " |  feature_types\n",
      " |      Feature types for this booster.  Can be directly set by input data or by\n",
      " |      assignment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf_model.get_booster())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on XGBClassifier in module xgboost.sklearn object:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = False, **kwargs: Any) -> None\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : int\n",
      " |          Number of boosting rounds.\n",
      " |  \n",
      " |      max_depth :  Optional[int]\n",
      " |          Maximum tree depth for base learners.\n",
      " |      max_leaves :\n",
      " |          Maximum number of leaves; 0 indicates no limit.\n",
      " |      max_bin :\n",
      " |          If using histogram-based algorithm, maximum number of bins per feature\n",
      " |      grow_policy :\n",
      " |          Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
      " |          depth-wise. 1: favor splitting at nodes with highest loss change.\n",
      " |      learning_rate : Optional[float]\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : Optional[int]\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: Optional[str]\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: Optional[str]\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      " |          default, XGBoost will choose the most conservative option available.  It's\n",
      " |          recommended to study this option from the parameters document :doc:`tree method\n",
      " |          </treemethod>`\n",
      " |      n_jobs : Optional[int]\n",
      " |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      " |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      " |          balance the threads.  Creating thread contention will significantly slow down both\n",
      " |          algorithms.\n",
      " |      gamma : Optional[float]\n",
      " |          (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
      " |          leaf node of the tree.\n",
      " |      min_child_weight : Optional[float]\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : Optional[float]\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : Optional[float]\n",
      " |          Subsample ratio of the training instance.\n",
      " |      sampling_method :\n",
      " |          Sampling method. Used only by `gpu_hist` tree method.\n",
      " |            - `uniform`: select random training instances uniformly.\n",
      " |            - `gradient_based` select random training instances with higher probability when\n",
      " |              the gradient and hessian are larger. (cf. CatBoost)\n",
      " |      colsample_bytree : Optional[float]\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : Optional[float]\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : Optional[float]\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : Optional[float]\n",
      " |          L1 regularization term on weights (xgb's alpha).\n",
      " |      reg_lambda : Optional[float]\n",
      " |          L2 regularization term on weights (xgb's lambda).\n",
      " |      scale_pos_weight : Optional[float]\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score : Optional[float]\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: Optional[int]\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      " |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      " |          for more information.\n",
      " |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      " |          3, 4]]``, where each inner list is a group of indices of features that are\n",
      " |          allowed to interact with each other.  See :doc:`tutorial\n",
      " |          </tutorials/feature_interaction_constraint>` for more information\n",
      " |      importance_type: Optional[str]\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |  \n",
      " |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      " |            \"total_cover\".\n",
      " |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      " |            without bias.\n",
      " |  \n",
      " |      gpu_id : Optional[int]\n",
      " |          Device ordinal.\n",
      " |      validate_parameters : Optional[bool]\n",
      " |          Give warnings for unknown parameter.\n",
      " |      predictor : Optional[str]\n",
      " |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      " |          gpu_predictor].\n",
      " |      enable_categorical : bool\n",
      " |  \n",
      " |          .. versionadded:: 1.5.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
      " |          should be used to specify categorical data type.  Also, JSON/UBJSON\n",
      " |          serialization format is required.\n",
      " |  \n",
      " |      max_cat_to_onehot : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      " |          for categorical data.  When number of categories is lesser than the threshold\n",
      " |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      " |          into children nodes.  Only relevant for regression and binary classification.\n",
      " |          See :doc:`Categorical Data </tutorials/categorical>` for details.\n",
      " |  \n",
      " |      eval_metric : Optional[Union[str, List[str], Callable]]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Metric used for monitoring the training result and early stopping.  It can be a\n",
      " |          string or list of strings as names of predefined metric in XGBoost (See\n",
      " |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
      " |          user defined metric that looks like `sklearn.metrics`.\n",
      " |  \n",
      " |          If custom objective is also provided, then custom metric should implement the\n",
      " |          corresponding reverse link function.\n",
      " |  \n",
      " |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      " |          object is provided, it's assumed to be a cost function and by default XGBoost will\n",
      " |          minimize the result during early stopping.\n",
      " |  \n",
      " |          For advanced usage on Early stopping like directly choosing to maximize instead of\n",
      " |          minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      " |  \n",
      " |          See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
      " |          for more.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |               This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old one\n",
      " |               receives un-transformed prediction regardless of whether custom objective is\n",
      " |               being used.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              from sklearn.datasets import load_diabetes\n",
      " |              from sklearn.metrics import mean_absolute_error\n",
      " |              X, y = load_diabetes(return_X_y=True)\n",
      " |              reg = xgb.XGBRegressor(\n",
      " |                  tree_method=\"hist\",\n",
      " |                  eval_metric=mean_absolute_error,\n",
      " |              )\n",
      " |              reg.fit(X, y, eval_set=[(X, y)])\n",
      " |  \n",
      " |      early_stopping_rounds : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Activates early stopping. Validation metric needs to improve at least once in\n",
      " |          every **early_stopping_rounds** round(s) to continue training.  Requires at least\n",
      " |          one item in **eval_set** in :py:meth:`fit`.\n",
      " |  \n",
      " |          The method returns the model from the last iteration (not the best one).  If\n",
      " |          there's more than one item in **eval_set**, the last entry will be used for early\n",
      " |          stopping.  If there's more than one metric in **eval_metric**, the last metric\n",
      " |          will be used for early stopping.\n",
      " |  \n",
      " |          If early stopping occurs, the model will have three additional fields:\n",
      " |          :py:attr:`best_score`, :py:attr:`best_iteration` and\n",
      " |          :py:attr:`best_ntree_limit`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |              This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
      " |  \n",
      " |      callbacks : Optional[List[TrainingCallback]]\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using\n",
      " |          :ref:`Callback API <callback_api>`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             States in callback are not preserved during training, which means callback\n",
      " |             objects can not be reused for multiple training sessions without\n",
      " |             reinitialization or deepcopy.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              for params in parameters_grid:\n",
      " |                  # be sure to (re)initialize the callbacks before each run\n",
      " |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      " |                  xgboost.train(params, Xy, callbacks=callbacks)\n",
      " |  \n",
      " |      kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      " |          can be found :doc:`here </parameter>`.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = False, **kwargs: Any) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, eval_metric: Union[str, Sequence[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None) -> 'XGBClassifier'\n",
      " |      Fit gradient boosting classifier.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Feature matrix\n",
      " |      y :\n",
      " |          Labels\n",
      " |      sample_weight :\n",
      " |          instance weights\n",
      " |      base_margin :\n",
      " |          global bias for each instance.\n",
      " |      eval_set :\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      \n",
      " |      eval_metric : str, list of str, or callable, optional\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `eval_metric` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `early_stopping_rounds` in :py:meth:`__init__` or\n",
      " |              :py:meth:`set_params` instead.\n",
      " |      verbose :\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      " |          measured on the validation set to stderr.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set :\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set :\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights :\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.\n",
      " |      \n",
      " |      callbacks :\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `callbacks` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |  \n",
      " |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      " |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      " |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      " |      automatically, otherwise it will run on CPU.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Data to predict with.\n",
      " |      output_margin :\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit :\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin :\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      " |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction\n",
      " |  \n",
      " |  predict_proba(self, X: Any, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Predict the probability of each `X` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      " |          probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      " |      early stopping, then `best_iteration` is used automatically.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      iteration_range :\n",
      " |          See :py:meth:`predict`.\n",
      " |      \n",
      " |      ntree_limit :\n",
      " |          Deprecated, use ``iteration_range`` instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
      " |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
      " |      function.\n",
      " |      \n",
      " |      The returned evaluation result is a dictionary:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result\n",
      " |  \n",
      " |  get_booster(self) -> xgboost.core.Booster\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self) -> int\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self) -> Dict[str, Any]\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the various\n",
      " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      " |      feature_names) will not be loaded when using binary format.  To save those\n",
      " |      attributes, use JSON/UBJ instead.  See :doc:`Model IO </tutorials/saving_model>`\n",
      " |      for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.load_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.load_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      " |      (such as feature_names) will not be saved when using binary format.  To save\n",
      " |      those attributes, use JSON/UBJ instead. See :doc:`Model IO\n",
      " |      </tutorials/saving_model>` for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.save_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.save_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params: Any) -> 'XGBModel'\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
      " |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
      " |  \n",
      " |  best_ntree_limit\n",
      " |  \n",
      " |  best_score\n",
      " |      The best score obtained by early stopping.\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property, return depends on `importance_type` parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      " |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      " |  \n",
      " |  feature_names_in_\n",
      " |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has feature\n",
      " |      names that are all strings.\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types,\n",
      " |          such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :py:meth:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('sklearn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a58c4b570f0c81787ad3bf7051ef2a89a6b2aa4690349b9bb2049c4fd7871c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
