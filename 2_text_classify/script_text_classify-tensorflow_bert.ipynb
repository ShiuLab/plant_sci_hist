{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "# __Test another way to finetune BERT__\n",
    "\n",
    "Based on [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) from TensorflowHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:29:38.728372Z",
     "iopub.status.busy": "2022-03-29T12:29:38.727971Z",
     "iopub.status.idle": "2022-03-29T12:29:40.581474Z",
     "shell.execute_reply": "2022-03-29T12:29:40.580473Z"
    },
    "id": "q-YbjCkzw0yU"
   },
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Install_\n",
    "\n",
    "For tensorflow\n",
    "```bash\n",
    "  pip install -U \"tensorflow-text==2.8.*\"\n",
    "  #pip install tf-models-official==2.7.0\n",
    "  # Run into tensorflow_model module not found error. \n",
    "  # Try without specify version\n",
    "  pip install tf-models-official\n",
    "````\n",
    "\n",
    "For running in vscode:\n",
    "```bash\n",
    "  conda install -n bert_finetune ipykernel --update-deps --force-reinstall\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:29:49.396206Z",
     "iopub.status.busy": "2022-03-29T12:29:49.395613Z",
     "iopub.status.idle": "2022-03-29T12:29:52.068483Z",
     "shell.execute_reply": "2022-03-29T12:29:52.067720Z"
    },
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_models as tfm\n",
    "from official.nlp import optimization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Configuration info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting paths\n",
    "work_dir          = Path.home() / \"projects/plant_sci_hist/2_text_classify\"\n",
    "corpus_combo_file = work_dir / \"corpus_combo\"\n",
    "\n",
    "# Dataset\n",
    "batch_size     = 32\n",
    "shuffle_buffer = 2\n",
    "\n",
    "# https://stackoverflow.com/questions/56613155/tensorflow-tf-data-autotune\n",
    "# tf.data builds a performance model of the input pipeline and runs an \n",
    "# optimization algorithm to find a good allocation of its CPU budget across all\n",
    "# parameters specified as AUTOTUNE\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# maximum number of tokens in a document\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Get text ready__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Read json to dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, test, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus = corpus_combo[['label','txt']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_train_validate_test(corpus_combo_file, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Save text entires into files_\n",
    "\n",
    "Follow the same structure as the IMDB dataset in the `aclImdb` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = work_dir / \"corpus_dir\"\n",
    "\n",
    "# Create train, valid, test dir\n",
    "train_dir  = corpus_dir / 'train'\n",
    "valid_dir  = corpus_dir / 'valid'\n",
    "test_dir   = corpus_dir / 'test'\n",
    "\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "valid_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pos, neg directory for each\n",
    "(train_dir / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(train_dir / \"neg\").mkdir(parents=True, exist_ok=True)\n",
    "(valid_dir / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(valid_dir / \"neg\").mkdir(parents=True, exist_ok=True)\n",
    "(test_dir  / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(test_dir  / \"neg\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entry_to_file(df, target_dir):\n",
    "  '''Save each text entry in the dataframe as a file\n",
    "  '''\n",
    "\n",
    "  labels = df['label'].values\n",
    "  txts   = df['txt'].values\n",
    "  c_dict = {0:0, 1:0}\n",
    "  for count, label in tqdm(enumerate(labels), total=len(labels)):\n",
    "    if label == 0:\n",
    "      c_dict[0] += 1\n",
    "      with open(target_dir / f\"neg/{count}.txt\", \"w\") as f:\n",
    "        f.write(txts[count])\n",
    "    else:\n",
    "      c_dict[1] += 1\n",
    "      with open(target_dir / f\"pos/{count}.txt\", \"w\") as f:\n",
    "        f.write(txts[count])\n",
    "\n",
    "  print(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_entry_to_file(train, train_dir)\n",
    "save_entry_to_file(valid, valid_dir)\n",
    "save_entry_to_file(test , test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Create datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(str(train_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "type(raw_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = raw_train_ds.class_names\n",
    "train_ds    = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "class_names, type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get valition set\n",
    "raw_valid_ds = tf.keras.utils.text_dataset_from_directory(str(valid_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "valid_ds     = raw_valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Get test set\n",
    "raw_test_ds  = tf.keras.utils.text_dataset_from_directory(str(test_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "test_ds      = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "# Check out one record\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(f'Text: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Hub models and initial testing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "### _Hub models to use_\n",
    "\n",
    "Use BERT trained on MEDLINE/Pubmed:\n",
    "- https://tfhub.dev/google/experts/bert/pubmed/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_encoder = 'https://tfhub.dev/google/experts/bert/pubmed/2'\n",
    "tfhub_preproc = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "### _Load and test preprocessing model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.795438Z",
     "iopub.status.busy": "2022-03-29T12:30:15.795041Z",
     "iopub.status.idle": "2022-03-29T12:30:18.992854Z",
     "shell.execute_reply": "2022-03-29T12:30:18.992262Z"
    },
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_preproc)\n",
    "type(bert_preprocess_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is not a layer but a saved model\n",
    "bert_preprocess = hub.load(tfhub_preproc)\n",
    "type(bert_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:18.996679Z",
     "iopub.status.busy": "2022-03-29T12:30:18.996239Z",
     "iopub.status.idle": "2022-03-29T12:30:19.160173Z",
     "shell.execute_reply": "2022-03-29T12:30:19.159548Z"
    },
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [],
   "source": [
    "text_test = ['This paper is about Plant, like maize, rice, and tomato!']\n",
    "\n",
    "#######################\n",
    "# CRITICAL STEP!!! NEED TO CHANGE DIMENSION From 128 to 512\n",
    "#######################\n",
    "\n",
    "tok = bert_preprocess.tokenize(tf.constant(text_test))\n",
    "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], \n",
    "                                                     tf.constant(max_length))\n",
    "#text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "\n",
    "# The size is 128.\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :30]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :30]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :30]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "### _Load and test BERT model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:19.163550Z",
     "iopub.status.busy": "2022-03-29T12:30:19.163058Z",
     "iopub.status.idle": "2022-03-29T12:30:26.095648Z",
     "shell.execute_reply": "2022-03-29T12:30:26.094996Z"
    },
    "id": "tXxYpK8ixL34"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_encoder)\n",
    "type(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.099587Z",
     "iopub.status.busy": "2022-03-29T12:30:26.098983Z",
     "iopub.status.idle": "2022-03-29T12:30:26.708358Z",
     "shell.execute_reply": "2022-03-29T12:30:26.707624Z"
    },
    "id": "_OoF9mebuSZc"
   },
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "print(f'Loaded BERT: {tfhub_encoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_output: embedding of the document\n",
    "# 768: size of the embedding vector\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_output: embeddings of each token\n",
    "# 512: number of tokens of text_preprocessed\n",
    "# 768: size of the embedding vector\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_outputs: intermediate activation of a transformer block\n",
    "# Q: Assuming activation is the output value of the activation function.\n",
    "# 12: number of transformer blocks\n",
    "print(f'Encoder Outputs length:{len(bert_results[\"encoder_outputs\"])}')\n",
    "\n",
    "# Saem as sequence output values\n",
    "print(f'Sequence Outputs shape:{bert_results[\"encoder_outputs\"][0].shape}')\n",
    "print(f'Sequence Outputs shape:{bert_results[\"encoder_outputs\"][0][0, :2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## __Build classification model__\n",
    "\n",
    "6/18/19\n",
    "- The challenge is how to use bert_pack_inputs as a prprocessing layer.\n",
    "- [This hub page](https://www.tensorflow.org/hub/common_saved_model_apis/text) has some info: does not help much, as the info is for individual instance and the model is very different from what I want.\n",
    "- [Fine-tuning a BERT model](https://www.tensorflow.org/text/tutorials/fine_tune_bert) tutorial: Here the dataset is tokenized, packed, and them used as input to model. Try it and see.\n",
    "- Also check the [preprocess model page](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) for syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get tokenizer to work_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original example\n",
    "#tok = bert_preprocess.tokenize(tf.constant(text_test))\n",
    "#text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], \n",
    "#                                                     tf.constant(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = hub.KerasLayer(bert_preprocess.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = tf.constant([\"hello tensorflow\"])\n",
    "sentences2 = tf.constant([\"goodbye tensorflow\"])\n",
    "tokens1    = tokenizer(sentences1)\n",
    "tokens2    = tokenizer(sentences2)\n",
    "tokens1, tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following throw an error:\n",
    "# AttributeError: 'KerasLayer' object has no attribute 'get_special_tokens_dict'\n",
    "# Not sure how important this is...\n",
    "#special = tokenizer.get_special_tokens_dict\n",
    "\n",
    "# Ok, in the \"Custom input packing and MLM support\" section of this page:\n",
    "# https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
    "# There is this line:\n",
    "special = bert_preprocess.tokenize.get_special_tokens_dict()\n",
    "special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get Packer to work_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_length,\n",
    "    special_tokens_dict = special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = packer([tokens1, tokens2])\n",
    "for key, tensor in packed.items():\n",
    "  print(f\"{key:15s}: {tensor.shape}, {tensor[:, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Combine tokenizer and packer into a layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertInputProcessor(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, packer):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.packer = packer\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Original code is expecting two features, but I only have one.\n",
    "    #tok1 = self.tokenizer(inputs['sentence1'])\n",
    "    #tok2 = self.tokenizer(inputs['sentence2'])\n",
    "    tok = self.tokenizer(inputs['txt'])\n",
    "\n",
    "    packed = self.packer([tok, tok])\n",
    "\n",
    "    if 'label' in inputs:\n",
    "      return packed, inputs['label']\n",
    "    else:\n",
    "      return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_processor = BertInputProcessor(tokenizer, packer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_packed, _ = train_ds.map(bert_inputs_processor).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.711934Z",
     "iopub.status.busy": "2022-03-29T12:30:26.711712Z",
     "iopub.status.idle": "2022-03-29T12:30:26.716508Z",
     "shell.execute_reply": "2022-03-29T12:30:26.715939Z"
    },
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  # Input layer\n",
    "  text_input        = tf.keras.layers.Input(shape=(), dtype=tf.string, \n",
    "                                            name='txt')\n",
    "\n",
    "  # Will this work??  \n",
    "  tokenizer         = hub.KerasLayer(bert_preprocess.tokenize)\n",
    "  special           = bert_preprocess.tokenize.get_special_tokens_dict()\n",
    "  tokenizer_outputs = tokenizer(text_input)\n",
    "\n",
    "  # Processing layer: This has the key change to allow longer texts.\n",
    "  packer            = tfm.nlp.layers.BertPackInputs(\n",
    "                      seq_length=max_length,\n",
    "                      special_tokens_dict = special)\n",
    "\n",
    "  #preproc_layer     = hub.KerasLayer(bert_preprocess.bert_pack_inputs, \n",
    "  #                                   arguments=dict(seq_lenght=max_length),\n",
    "  #                                   name='preprocessing')\n",
    "                                     \n",
    "  packer_outputs   = packer(tokenizer_outputs)\n",
    "\n",
    "  # Initialize encoder\n",
    "  encoder           = hub.KerasLayer(tfhub_encoder, trainable=True, \n",
    "                                   name='BERT_encoder')\n",
    "  encoder_outputs   = encoder(packer_outputs)\n",
    "  \n",
    "  # Q: Wonder if this is the dense layer mentioned above.\n",
    "  print(type(encoder_outputs))\n",
    "\n",
    "  # Get just the embeddings for each doc (ignore token level info)\n",
    "  net            = encoder_outputs['pooled_output']\n",
    "\n",
    "  # Dropout layer\n",
    "  net            = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "  # output layer: single node, Q: Why??\n",
    "  net            = tf.keras.layers.Dense(2, activation='softmax', \n",
    "                                                        name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.719652Z",
     "iopub.status.busy": "2022-03-29T12:30:26.719155Z",
     "iopub.status.idle": "2022-03-29T12:30:33.483247Z",
     "shell.execute_reply": "2022-03-29T12:30:33.482586Z"
    },
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "\n",
    "text_test        = ['Plant science focuses on studies of photosynthetic species.']\n",
    "\n",
    "# tf.constant: create a Tensor from tensor like objects\n",
    "tensor_test      = tf.constant(text_test)\n",
    "bert_raw_result  = classifier_model(tensor_test)\n",
    "\n",
    "print(\"Raw result   :\", bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.486539Z",
     "iopub.status.busy": "2022-03-29T12:30:33.486005Z",
     "iopub.status.idle": "2022-03-29T12:30:33.607583Z",
     "shell.execute_reply": "2022-03-29T12:30:33.606965Z"
    },
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## __Model training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### _Compile model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.624696Z",
     "iopub.status.busy": "2022-03-29T12:30:33.624232Z",
     "iopub.status.idle": "2022-03-29T12:30:33.628769Z",
     "shell.execute_reply": "2022-03-29T12:30:33.628248Z"
    },
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs           = 20\n",
    "cardinality      = tf.data.experimental.cardinality(train_ds)\n",
    "steps_per_epoch  = cardinality.numpy()\n",
    "num_train_steps  = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "# loss function: \n",
    "loss    = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# evaluation metrics\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "# Initial learning rate\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.631424Z",
     "iopub.status.busy": "2022-03-29T12:30:33.631013Z",
     "iopub.status.idle": "2022-03-29T12:30:33.640619Z",
     "shell.execute_reply": "2022-03-29T12:30:33.640151Z"
    },
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### _Train model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify callbacks\n",
    "\n",
    "# early stopping\n",
    "callback_es  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# model checkpoint\n",
    "cp_filepath  = work_dir / \"model_ori_bert_tf_pubmed\"\n",
    "callback_mcp = tf.keras.callbacks.ModelCheckpoint(filepath=str(cp_filepath), \n",
    "              mode='max', save_weights_only=False, monitor='val_accuracy', \n",
    "              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.643759Z",
     "iopub.status.busy": "2022-03-29T12:30:33.643317Z",
     "iopub.status.idle": "2022-03-29T12:37:37.231432Z",
     "shell.execute_reply": "2022-03-29T12:37:37.230870Z"
    },
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_encoder}')\n",
    "history = classifier_model.fit(x=train_ds, \n",
    "                               batch_size=2, \n",
    "                               epochs=epochs,\n",
    "                               validation_data=valid_ds, \n",
    "                               verbose=1,\n",
    "                               callbacks=[callback_es, callback_mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:37:37.235151Z",
     "iopub.status.busy": "2022-03-29T12:37:37.234581Z",
     "iopub.status.idle": "2022-03-29T12:38:36.128910Z",
     "shell.execute_reply": "2022-03-29T12:38:36.128342Z"
    },
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:36.132081Z",
     "iopub.status.busy": "2022-03-29T12:38:36.131630Z",
     "iopub.status.idle": "2022-03-29T12:38:36.424503Z",
     "shell.execute_reply": "2022-03-29T12:38:36.424018Z"
    },
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## Export for inference\n",
    "\n",
    "Now you just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:36.428146Z",
     "iopub.status.busy": "2022-03-29T12:38:36.427717Z",
     "iopub.status.idle": "2022-03-29T12:38:42.015407Z",
     "shell.execute_reply": "2022-03-29T12:38:42.014764Z"
    },
    "id": "ShcvqJAgVera"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbI25bS1vD7s"
   },
   "source": [
    "Let's reload the model, so you can try it side by side with the model that is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:42.019271Z",
     "iopub.status.busy": "2022-03-29T12:38:42.018826Z",
     "iopub.status.idle": "2022-03-29T12:38:48.305286Z",
     "shell.execute_reply": "2022-03-29T12:38:48.304688Z"
    },
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyTappHTvNCz"
   },
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:48.309376Z",
     "iopub.status.busy": "2022-03-29T12:38:48.308953Z",
     "iopub.status.idle": "2022-03-29T12:38:48.694457Z",
     "shell.execute_reply": "2022-03-29T12:38:48.693822Z"
    },
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cOmih754Y_M"
   },
   "source": [
    "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:48.697594Z",
     "iopub.status.busy": "2022-03-29T12:38:48.697394Z",
     "iopub.status.idle": "2022-03-29T12:38:48.996870Z",
     "shell.execute_reply": "2022-03-29T12:38:48.996220Z"
    },
    "id": "0FdVD3973S-O"
   },
   "outputs": [],
   "source": [
    "serving_results = reloaded_model \\\n",
    "            .signatures['serving_default'](tf.constant(examples))\n",
    "\n",
    "serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "print_my_examples(examples, serving_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4gN1KwReLPN"
   },
   "source": [
    "# __FAILED STEPS__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _[Gave up on this] Convert training dataframe to dataset_\n",
    "\n",
    "Try many different ways, was trying not to slit text records into different files then load as a dataset like the tutorial. But converting from dataframe always missed something that I cannot quite put my finger on. Moving on. Revisit this later...\n",
    "\n",
    "- See the [pd_dataframe_to_tf_dataset](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/pd_dataframe_to_tf_dataset) function, but this needs tf 2.9, conflict with tensorflow_text.\n",
    "- See [this](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe): See the shuffle and batch functions. Does not work...\n",
    "- See [this post](https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168):  Was able to create SicedDataset, then BatchDatabase after applying the batch function, then PrefetchDataset. But trying to retreive a test example from trainin dataset lead to:\n",
    "  - InvalidArgumentError: Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/\n",
    "  - Ok, as I was implmenting the next solution, realize that I did not call the right obj for prefetch. Can be the reason why.\n",
    "- Ah, see [this post](https://stackoverflow.com/questions/58461609/how-to-convert-pandas-dataframe-to-tensorflow-dataset): key is to turn train_data to dictionary before calling from_tensor_slices.\n",
    "  - A little comment below say need to do .to_dict() instead which make sense. Because if just do dict(train), the thing finish in 0.1 sec which does not make sense. But this fails and throw:\n",
    "    - ValueError: Unbatching a tensor is only supported for rank >= 1\n",
    "  - Found [this post](https://stackoverflow.com/questions/55560620/valueerror-unbatching-a-tensor-is-only-supported-for-rank-1): Now try to uses this syntax. Still does not work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following DOES NOT work\n",
    "\n",
    "```Python\n",
    "raw_train_ds = (tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(train['txt'].values, tf.string),\n",
    "         tf.cast(train['label'].values, tf.int32),)))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(dict(train))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(train.to_dict())\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((y_train,X_train))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((i_train, \n",
    "                                                   y_train.values,\n",
    "                                                   X_train.values))\n",
    "\n",
    "The last one is almost working, but at later stage when I try to pack the input, it breaks. Upon closer examination, the train_ds looks like:\n",
    "\n",
    "```\n",
    "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
    "```\n",
    "\n",
    "But what I need is more like:\n",
    "\n",
    "```\n",
    "<PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'sentence1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'sentence2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = train.iloc[:3,:]\n",
    "raw_trial_ds = (tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(trial['txt'].values, tf.string),\n",
    "         tf.cast(trial['label'].values, tf.int32),)))\n",
    "raw_trial_ds\n",
    "# Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \"trial\" so it does not mix with the testing set\n",
    "trial_dict   = {\"idx\":[123,322], \"label\":[0,1], \n",
    "               \"txt\":[\"The 1st sentence\", \"The second\"]}\n",
    "\n",
    "# This creates a TensorSliceDataset\n",
    "raw_trial_ds =  tf.data.Dataset.from_tensor_slices(trial_dict)\n",
    "\n",
    "# Now this has the right structure!!!\n",
    "raw_trial_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_batch, label_batch, text_batch in raw_trial_ds.take(1):\n",
    "  for i in range(1):\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_trial_ds_batch = raw_trial_ds.batch(batch_size)\n",
    "type(raw_trial_ds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ds = raw_trial_ds_batch.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "len(trial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still have the right structure.\n",
    "trial_ds.element_spec['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "take1 = trial_ds.take(1)\n",
    "help(take1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train, valid, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(df):\n",
    "  '''Convert DataFrame to PrefetchDataset\n",
    "  Args:\n",
    "    df (dataframe): with two columns ('txt', and 'label') and indices\n",
    "  Return:\n",
    "    dataset (PrefetchDataset): with:\n",
    "      <PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.int32, name=None), \n",
    "                                     'label': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.int32, name=None), \n",
    "                                     'txt': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.string, name=None)}>\n",
    "\n",
    "  '''\n",
    "  idx = df.index   # aded this, as the tutorial has an index input\n",
    "  X   = df['txt']\n",
    "  y   = df['label']\n",
    "\n",
    "  df_dict = {\"idx\":idx, \"label\":y.values, \"txt\":X.values}\n",
    "\n",
    "  raw_ds       = tf.data.Dataset.from_tensor_slices(df_dict)  \n",
    "  raw_ds_batch = raw_ds.batch(batch_size)\n",
    "  dataset      = raw_ds_batch.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data\n",
    "train_ds = dataframe_to_dataset(train)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation dataset\n",
    "valid_ds = dataframe_to_dataset(valid)\n",
    "\n",
    "# Get testing dataset\n",
    "test_ds = dataframe_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Testing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  print(text_batch[0])\n",
    "  print(len(text_batch))\n",
    "  print(label_batch)\n",
    "  print(len(label_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('bert_finetune')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "323c618d0395b34183a36199d7c8eddbd4e55d51aee9dabbfbc9809db817fb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
