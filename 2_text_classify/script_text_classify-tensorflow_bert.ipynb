{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "# __Test another way to finetune BERT__\n",
    "\n",
    "Based on [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) from TensorflowHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:29:38.728372Z",
     "iopub.status.busy": "2022-03-29T12:29:38.727971Z",
     "iopub.status.idle": "2022-03-29T12:29:40.581474Z",
     "shell.execute_reply": "2022-03-29T12:29:40.580473Z"
    },
    "id": "q-YbjCkzw0yU"
   },
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Install_\n",
    "\n",
    "For tensorflow\n",
    "```bash\n",
    "  pip install -U \"tensorflow-text==2.8.*\"\n",
    "  #pip install tf-models-official==2.7.0\n",
    "  # Run into tensorflow_model module not found error. \n",
    "  # Try without specify version\n",
    "  pip install tf-models-official\n",
    "````\n",
    "\n",
    "For running in vscode:\n",
    "```bash\n",
    "  conda install -n bert_finetune ipykernel --update-deps --force-reinstall\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:29:49.396206Z",
     "iopub.status.busy": "2022-03-29T12:29:49.395613Z",
     "iopub.status.idle": "2022-03-29T12:29:52.068483Z",
     "shell.execute_reply": "2022-03-29T12:29:52.067720Z"
    },
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_models as tfm\n",
    "from official.nlp import optimization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Configuration info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting paths\n",
    "work_dir          = Path.home() / \"projects/plant_sci_hist/2_text_classify\"\n",
    "corpus_combo_file = work_dir / \"corpus_combo\"\n",
    "\n",
    "# Dataset\n",
    "batch_size     = 32\n",
    "shuffle_buffer = 2\n",
    "\n",
    "# https://stackoverflow.com/questions/56613155/tensorflow-tf-data-autotune\n",
    "# tf.data builds a performance model of the input pipeline and runs an \n",
    "# optimization algorithm to find a good allocation of its CPU budget across all\n",
    "# parameters specified as AUTOTUNE\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# maximum number of tokens in a document\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Get text ready__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Read json to dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, test, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus = corpus_combo[['label','txt']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_train_validate_test(corpus_combo_file, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Save text entires into files_\n",
    "\n",
    "Follow the same structure as the IMDB dataset in the `aclImdb` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = work_dir / \"corpus_dir\"\n",
    "\n",
    "# Create train, valid, test dir\n",
    "train_dir  = corpus_dir / 'train'\n",
    "valid_dir  = corpus_dir / 'valid'\n",
    "test_dir   = corpus_dir / 'test'\n",
    "\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "valid_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pos, neg directory for each\n",
    "(train_dir / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(train_dir / \"neg\").mkdir(parents=True, exist_ok=True)\n",
    "(valid_dir / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(valid_dir / \"neg\").mkdir(parents=True, exist_ok=True)\n",
    "(test_dir  / \"pos\").mkdir(parents=True, exist_ok=True)\n",
    "(test_dir  / \"neg\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entry_to_file(df, target_dir):\n",
    "  '''Save each text entry in the dataframe as a file\n",
    "  '''\n",
    "\n",
    "  labels = df['label'].values\n",
    "  txts   = df['txt'].values\n",
    "  c_dict = {0:0, 1:0}\n",
    "  for count, label in tqdm(enumerate(labels), total=len(labels)):\n",
    "    if label == 0:\n",
    "      c_dict[0] += 1\n",
    "      with open(target_dir / f\"neg/{count}.txt\", \"w\") as f:\n",
    "        f.write(txts[count])\n",
    "    else:\n",
    "      c_dict[1] += 1\n",
    "      with open(target_dir / f\"pos/{count}.txt\", \"w\") as f:\n",
    "        f.write(txts[count])\n",
    "\n",
    "  print(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51987/51987 [03:06<00:00, 278.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 25994, 1: 25993}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17329/17329 [01:01<00:00, 281.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8664, 1: 8665}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17330/17330 [00:59<00:00, 288.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8665, 1: 8665}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_entry_to_file(train, train_dir)\n",
    "save_entry_to_file(valid, valid_dir)\n",
    "save_entry_to_file(test , test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Create datasets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51987 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 09:04:14.127789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.137509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.137941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.139051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-20 09:04:14.140447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.141029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.141466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.806749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.807475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.807490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-20 09:04:14.807948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-20 09:04:14.807999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train dataset\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(str(train_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "type(raw_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['neg', 'pos'], tensorflow.python.data.ops.dataset_ops.PrefetchDataset)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = raw_train_ds.class_names\n",
    "train_ds    = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "class_names, type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17329 files belonging to 2 classes.\n",
      "Found 17330 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Get valition set\n",
    "raw_valid_ds = tf.keras.utils.text_dataset_from_directory(str(valid_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "valid_ds     = raw_valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Get test set\n",
    "raw_test_ds  = tf.keras.utils.text_dataset_from_directory(str(test_dir), \n",
    "                                                          batch_size=batch_size)\n",
    "test_ds      = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b'Ten Broad Spectrum Resistances to Downy Mildew Physically Mapped on the Sunflower Genome.. Resistance to downy mildew (Plasmopara halstedii) in sunflower (Helianthus annuus L.) is conferred by major resistance genes, denoted Pl. Twenty-two Pl genes have been identified and genetically mapped so far. However, over the past 50 years, wide-scale presence of only a few of them in sunflower crops led to the appearance of new, more virulent pathotypes (races) so it is important for sunflower varieties to carry as wide a range of resistance genes as possible. We analyzed phenotypically 12 novel resistant sources discovered in breeding pools derived from two wild Helianthus species and in eight wild H. annuus ecotypes. All were effective against at least 16 downy mildew pathotypes. We mapped their resistance genes on the sunflower reference genome of 3,600 Mb, in intervals that varied from 75 Kb to 32 Mb using an AXIOM\\xc2\\xae genotyping array of 49,449 SNP. Ten probably new genes were identified according to resistance spectrum, map position, hypersensitive response to the transient expression of a P. halstedii RXLR effector, or the ecotype/species from which they originated. The resistance source HAS6 was found to carry the first downy mildew resistance gene mapped on chromosome 11, whereas the other resistances were positioned on chromosomes 1, 2, 4, and 13 carrying already published Pl genes that we also mapped physically on the same reference genome. The new genes were designated Pl23-Pl32 according to the current nomenclature. However, since sunflower downy mildew resistance genes have not yet been sequenced, rules for designation are discussed. This is the first large scale physical mapping of both 10 new and 10 already reported downy mildew resistance genes in sunflower.'\n",
      "Label : 1 (pos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 09:05:01.078317: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check out one record\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(f'Text: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Define Hub models and initial testing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "### _Hub models to use_\n",
    "\n",
    "Use BERT trained on MEDLINE/Pubmed:\n",
    "- https://tfhub.dev/google/experts/bert/pubmed/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_encoder = 'https://tfhub.dev/google/experts/bert/pubmed/2'\n",
    "tfhub_preproc = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "### _Load and test preprocessing model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.795438Z",
     "iopub.status.busy": "2022-03-29T12:30:15.795041Z",
     "iopub.status.idle": "2022-03-29T12:30:18.992854Z",
     "shell.execute_reply": "2022-03-29T12:30:18.992262Z"
    },
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_preproc)\n",
    "type(bert_preprocess_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note this is not a layer but a saved model\n",
    "bert_preprocess = hub.load(tfhub_preproc)\n",
    "type(bert_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:18.996679Z",
     "iopub.status.busy": "2022-03-29T12:30:18.996239Z",
     "iopub.status.idle": "2022-03-29T12:30:19.160173Z",
     "shell.execute_reply": "2022-03-29T12:30:19.159548Z"
    },
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
      "Shape      : (1, 512)\n",
      "Word Ids   : [  101  2023  3259  2003  2055  3269  1010  2066 21154  1010  5785  1010\n",
      "  1998 20856   999   102  2023  3259  2003  2055  3269  1010  2066 21154\n",
      "  1010  5785  1010  1998 20856   999]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['This paper is about Plant, like maize, rice, and tomato!']\n",
    "\n",
    "#######################\n",
    "# CRITICAL STEP!!! NEED TO CHANGE DIMENSION From 128 to 512\n",
    "#######################\n",
    "\n",
    "tok = bert_preprocess.tokenize(tf.constant(text_test))\n",
    "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], \n",
    "                                                     tf.constant(max_length))\n",
    "#text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "\n",
    "# The size is 128.\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :30]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :30]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :30]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "### _Load and test BERT model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:19.163550Z",
     "iopub.status.busy": "2022-03-29T12:30:19.163058Z",
     "iopub.status.idle": "2022-03-29T12:30:26.095648Z",
     "shell.execute_reply": "2022-03-29T12:30:26.094996Z"
    },
    "id": "tXxYpK8ixL34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_hub.keras_layer.KerasLayer"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_encoder)\n",
    "type(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.099587Z",
     "iopub.status.busy": "2022-03-29T12:30:26.098983Z",
     "iopub.status.idle": "2022-03-29T12:30:26.708358Z",
     "shell.execute_reply": "2022-03-29T12:30:26.707624Z"
    },
    "id": "_OoF9mebuSZc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 09:05:21.106603: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/google/experts/bert/pubmed/2\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "print(f'Loaded BERT: {tfhub_encoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled Outputs Shape:(1, 768)\n",
      "Pooled Outputs Values:[ 0.19967358 -0.6023183   0.01133679 -0.94678575 -0.32777378  0.3923298\n",
      " -0.90228444]\n"
     ]
    }
   ],
   "source": [
    "# pooled_output: embedding of the document\n",
    "# 768: size of the embedding vector\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :7]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Outputs Shape:(1, 512, 768)\n",
      "Sequence Outputs Values:[[ 0.20244306 -0.6965506   0.01134092 ...  0.07541777  0.804416\n",
      "   0.43197766]\n",
      " [-0.8271446  -1.1451745   0.23592134 ... -0.15745929 -0.5484226\n",
      "  -2.219939  ]]\n"
     ]
    }
   ],
   "source": [
    "# sequence_output: embeddings of each token\n",
    "# 512: number of tokens of text_preprocessed\n",
    "# 768: size of the embedding vector\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Outputs length:12\n",
      "Sequence Outputs shape:(1, 512, 768)\n",
      "Sequence Outputs shape:[[ 0.04164825 -0.05509862  0.05931972 ... -0.07429437  0.01309458\n",
      "   0.04005507]\n",
      " [-0.7039484  -0.5594542   0.16843846 ... -0.15212026  0.34699732\n",
      "  -0.403786  ]]\n"
     ]
    }
   ],
   "source": [
    "# encoder_outputs: intermediate activation of a transformer block\n",
    "# Q: Assuming activation is the output value of the activation function.\n",
    "# 12: number of transformer blocks\n",
    "print(f'Encoder Outputs length:{len(bert_results[\"encoder_outputs\"])}')\n",
    "\n",
    "# Saem as sequence output values\n",
    "print(f'Sequence Outputs shape:{bert_results[\"encoder_outputs\"][0].shape}')\n",
    "print(f'Sequence Outputs shape:{bert_results[\"encoder_outputs\"][0][0, :2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## __Build classification model__\n",
    "\n",
    "6/18/19\n",
    "- The challenge is how to use bert_pack_inputs as a prprocessing layer.\n",
    "- [This hub page](https://www.tensorflow.org/hub/common_saved_model_apis/text) has some info: does not help much, as the info is for individual instance and the model is very different from what I want.\n",
    "- [Fine-tuning a BERT model](https://www.tensorflow.org/text/tutorials/fine_tune_bert) tutorial: Here the dataset is tokenized, packed, and them used as input to model. Try it and see.\n",
    "- Also check the [preprocess model page](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) for syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get tokenizer to work_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original example\n",
    "#tok = bert_preprocess.tokenize(tf.constant(text_test))\n",
    "#text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], \n",
    "#                                                     tf.constant(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = hub.KerasLayer(bert_preprocess.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[[7592], [23435, 12314]]]>,\n",
       " <tf.RaggedTensor [[[9119], [23435, 12314]]]>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1 = tf.constant([\"hello tensorflow\"])\n",
    "sentences2 = tf.constant([\"goodbye tensorflow\"])\n",
    "tokens1    = tokenizer(sentences1)\n",
    "tokens2    = tokenizer(sentences2)\n",
    "tokens1, tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mask_id': <tf.Tensor: shape=(), dtype=int32, numpy=103>,\n",
       " 'padding_id': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'end_of_segment_id': <tf.Tensor: shape=(), dtype=int32, numpy=102>,\n",
       " 'vocab_size': <tf.Tensor: shape=(), dtype=int32, numpy=30522>,\n",
       " 'start_of_sequence_id': <tf.Tensor: shape=(), dtype=int32, numpy=101>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following throw an error:\n",
    "# AttributeError: 'KerasLayer' object has no attribute 'get_special_tokens_dict'\n",
    "# Not sure how important this is...\n",
    "#special = tokenizer.get_special_tokens_dict\n",
    "\n",
    "# Ok, in the \"Custom input packing and MLM support\" section of this page:\n",
    "# https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
    "# There is this line:\n",
    "special = bert_preprocess.tokenize.get_special_tokens_dict()\n",
    "special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get Packer to work_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_length,\n",
    "    special_tokens_dict = special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_word_ids : (1, 512), [[  101  7592 23435 12314   102  9119 23435 12314   102     0]]\n",
      "input_mask     : (1, 512), [[1 1 1 1 1 1 1 1 1 0]]\n",
      "input_type_ids : (1, 512), [[0 0 0 0 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "packed = packer([tokens1, tokens2])\n",
    "for key, tensor in packed.items():\n",
    "  print(f\"{key:15s}: {tensor.shape}, {tensor[:, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Combine tokenizer and packer into a layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertInputProcessor(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, packer):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.packer = packer\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Original code is expecting two features, but I only have one.\n",
    "    #tok1 = self.tokenizer(inputs['sentence1'])\n",
    "    #tok2 = self.tokenizer(inputs['sentence2'])\n",
    "    tok = self.tokenizer(inputs['txt'])\n",
    "\n",
    "    packed = self.packer([tok, tok])\n",
    "\n",
    "    if 'label' in inputs:\n",
    "      return packed, inputs['label']\n",
    "    else:\n",
    "      return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_processor = BertInputProcessor(tokenizer, packer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() takes 2 positional arguments but 3 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb Cell 49'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shius/github/plant_sci_hist/2_text_classify/script_text_classify-tensorflow_bert.ipynb#ch0000048vscode-remote?line=0'>1</a>\u001b[0m train_packed, _ \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39;49mmap(bert_inputs_processor)\u001b[39m.\u001b[39mprefetch(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2048\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2045\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m   2046\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39;49m, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   2049\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   2051\u001b[0m       \u001b[39mself\u001b[39m,\n\u001b[1;32m   2052\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2055\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2056\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5243\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m   5242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality \u001b[39m=\u001b[39m preserve_cardinality\n\u001b[0;32m-> 5243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m   5244\u001b[0m     map_func,\n\u001b[1;32m   5245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m   5246\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m   5247\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m   5248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m   5249\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mmap_dataset(\n\u001b[1;32m   5250\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   5251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5254\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m   5255\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2559\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[1;32m   2561\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2567\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m   2568\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2569\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2533\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2534\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m   2535\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2536\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m   cache_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mgeneralize(cache_key)\n\u001b[1;32m   2709\u001b[0m   (args, kwargs) \u001b[39m=\u001b[39m cache_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   2712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                          graph_function)\n\u001b[1;32m   2715\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   2623\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2624\u001b[0m ]\n\u001b[1;32m   2625\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   2626\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   2628\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2629\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   2630\u001b[0m         args,\n\u001b[1;32m   2631\u001b[0m         kwargs,\n\u001b[1;32m   2632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   2633\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   2634\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   2635\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   2636\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   2637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   2638\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   2639\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2644\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1141\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1143\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    243\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    244\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[1;32m    245\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    249\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    176\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 177\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n\u001b[1;32m    179\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ret)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/bert_finetune/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() takes 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "train_packed, _ = train_ds.map(bert_inputs_processor).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.711934Z",
     "iopub.status.busy": "2022-03-29T12:30:26.711712Z",
     "iopub.status.idle": "2022-03-29T12:30:26.716508Z",
     "shell.execute_reply": "2022-03-29T12:30:26.715939Z"
    },
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  # Input layer\n",
    "  text_input        = tf.keras.layers.Input(shape=(), dtype=tf.string, \n",
    "                                            name='txt')\n",
    "\n",
    "  # Will this work??  \n",
    "  tokenizer         = hub.KerasLayer(bert_preprocess.tokenize)\n",
    "  special           = bert_preprocess.tokenize.get_special_tokens_dict()\n",
    "  tokenizer_outputs = tokenizer(text_input)\n",
    "\n",
    "  # Processing layer: This has the key change to allow longer texts.\n",
    "  packer            = tfm.nlp.layers.BertPackInputs(\n",
    "                      seq_length=max_length,\n",
    "                      special_tokens_dict = special)\n",
    "\n",
    "  #preproc_layer     = hub.KerasLayer(bert_preprocess.bert_pack_inputs, \n",
    "  #                                   arguments=dict(seq_lenght=max_length),\n",
    "  #                                   name='preprocessing')\n",
    "                                     \n",
    "  packer_outputs   = packer(tokenizer_outputs)\n",
    "\n",
    "  # Initialize encoder\n",
    "  encoder           = hub.KerasLayer(tfhub_encoder, trainable=True, \n",
    "                                   name='BERT_encoder')\n",
    "  encoder_outputs   = encoder(packer_outputs)\n",
    "  \n",
    "  # Q: Wonder if this is the dense layer mentioned above.\n",
    "  print(type(encoder_outputs))\n",
    "\n",
    "  # Get just the embeddings for each doc (ignore token level info)\n",
    "  net            = encoder_outputs['pooled_output']\n",
    "\n",
    "  # Dropout layer\n",
    "  net            = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "  # output layer: single node, Q: Why??\n",
    "  net            = tf.keras.layers.Dense(2, activation='softmax', \n",
    "                                                        name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:26.719652Z",
     "iopub.status.busy": "2022-03-29T12:30:26.719155Z",
     "iopub.status.idle": "2022-03-29T12:30:33.483247Z",
     "shell.execute_reply": "2022-03-29T12:30:33.482586Z"
    },
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "\n",
    "text_test        = ['Plant science focuses on studies of photosynthetic species.']\n",
    "\n",
    "# tf.constant: create a Tensor from tensor like objects\n",
    "tensor_test      = tf.constant(text_test)\n",
    "bert_raw_result  = classifier_model(tensor_test)\n",
    "\n",
    "print(\"Raw result   :\", bert_raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.486539Z",
     "iopub.status.busy": "2022-03-29T12:30:33.486005Z",
     "iopub.status.idle": "2022-03-29T12:30:33.607583Z",
     "shell.execute_reply": "2022-03-29T12:30:33.606965Z"
    },
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## __Model training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### _Compile model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.624696Z",
     "iopub.status.busy": "2022-03-29T12:30:33.624232Z",
     "iopub.status.idle": "2022-03-29T12:30:33.628769Z",
     "shell.execute_reply": "2022-03-29T12:30:33.628248Z"
    },
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs           = 20\n",
    "cardinality      = tf.data.experimental.cardinality(train_ds)\n",
    "steps_per_epoch  = cardinality.numpy()\n",
    "num_train_steps  = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "# loss function: \n",
    "loss    = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# evaluation metrics\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "# Initial learning rate\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.631424Z",
     "iopub.status.busy": "2022-03-29T12:30:33.631013Z",
     "iopub.status.idle": "2022-03-29T12:30:33.640619Z",
     "shell.execute_reply": "2022-03-29T12:30:33.640151Z"
    },
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### _Train model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify callbacks\n",
    "\n",
    "# early stopping\n",
    "callback_es  = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# model checkpoint\n",
    "cp_filepath  = work_dir / \"model_ori_bert_tf_pubmed\"\n",
    "callback_mcp = tf.keras.callbacks.ModelCheckpoint(filepath=str(cp_filepath), \n",
    "              mode='max', save_weights_only=False, monitor='val_accuracy', \n",
    "              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:33.643759Z",
     "iopub.status.busy": "2022-03-29T12:30:33.643317Z",
     "iopub.status.idle": "2022-03-29T12:37:37.231432Z",
     "shell.execute_reply": "2022-03-29T12:37:37.230870Z"
    },
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_encoder}')\n",
    "history = classifier_model.fit(x=train_ds, \n",
    "                               batch_size=2, \n",
    "                               epochs=epochs,\n",
    "                               validation_data=valid_ds, \n",
    "                               verbose=1,\n",
    "                               callbacks=[callback_es, callback_mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:37:37.235151Z",
     "iopub.status.busy": "2022-03-29T12:37:37.234581Z",
     "iopub.status.idle": "2022-03-29T12:38:36.128910Z",
     "shell.execute_reply": "2022-03-29T12:38:36.128342Z"
    },
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:36.132081Z",
     "iopub.status.busy": "2022-03-29T12:38:36.131630Z",
     "iopub.status.idle": "2022-03-29T12:38:36.424503Z",
     "shell.execute_reply": "2022-03-29T12:38:36.424018Z"
    },
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## Export for inference\n",
    "\n",
    "Now you just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:36.428146Z",
     "iopub.status.busy": "2022-03-29T12:38:36.427717Z",
     "iopub.status.idle": "2022-03-29T12:38:42.015407Z",
     "shell.execute_reply": "2022-03-29T12:38:42.014764Z"
    },
    "id": "ShcvqJAgVera"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbI25bS1vD7s"
   },
   "source": [
    "Let's reload the model, so you can try it side by side with the model that is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:42.019271Z",
     "iopub.status.busy": "2022-03-29T12:38:42.018826Z",
     "iopub.status.idle": "2022-03-29T12:38:48.305286Z",
     "shell.execute_reply": "2022-03-29T12:38:48.304688Z"
    },
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyTappHTvNCz"
   },
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:48.309376Z",
     "iopub.status.busy": "2022-03-29T12:38:48.308953Z",
     "iopub.status.idle": "2022-03-29T12:38:48.694457Z",
     "shell.execute_reply": "2022-03-29T12:38:48.693822Z"
    },
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cOmih754Y_M"
   },
   "source": [
    "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:38:48.697594Z",
     "iopub.status.busy": "2022-03-29T12:38:48.697394Z",
     "iopub.status.idle": "2022-03-29T12:38:48.996870Z",
     "shell.execute_reply": "2022-03-29T12:38:48.996220Z"
    },
    "id": "0FdVD3973S-O"
   },
   "outputs": [],
   "source": [
    "serving_results = reloaded_model \\\n",
    "            .signatures['serving_default'](tf.constant(examples))\n",
    "\n",
    "serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "print_my_examples(examples, serving_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4gN1KwReLPN"
   },
   "source": [
    "# __FAILED STEPS__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _[Gave up on this] Convert training dataframe to dataset_\n",
    "\n",
    "Try many different ways, was trying not to slit text records into different files then load as a dataset like the tutorial. But converting from dataframe always missed something that I cannot quite put my finger on. Moving on. Revisit this later...\n",
    "\n",
    "- See the [pd_dataframe_to_tf_dataset](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/pd_dataframe_to_tf_dataset) function, but this needs tf 2.9, conflict with tensorflow_text.\n",
    "- See [this](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe): See the shuffle and batch functions. Does not work...\n",
    "- See [this post](https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168):  Was able to create SicedDataset, then BatchDatabase after applying the batch function, then PrefetchDataset. But trying to retreive a test example from trainin dataset lead to:\n",
    "  - InvalidArgumentError: Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/\n",
    "  - Ok, as I was implmenting the next solution, realize that I did not call the right obj for prefetch. Can be the reason why.\n",
    "- Ah, see [this post](https://stackoverflow.com/questions/58461609/how-to-convert-pandas-dataframe-to-tensorflow-dataset): key is to turn train_data to dictionary before calling from_tensor_slices.\n",
    "  - A little comment below say need to do .to_dict() instead which make sense. Because if just do dict(train), the thing finish in 0.1 sec which does not make sense. But this fails and throw:\n",
    "    - ValueError: Unbatching a tensor is only supported for rank >= 1\n",
    "  - Found [this post](https://stackoverflow.com/questions/55560620/valueerror-unbatching-a-tensor-is-only-supported-for-rank-1): Now try to uses this syntax. Still does not work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following DOES NOT work\n",
    "\n",
    "```Python\n",
    "raw_train_ds = (tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(train['txt'].values, tf.string),\n",
    "         tf.cast(train['label'].values, tf.int32),)))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(dict(train))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(train.to_dict())\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((y_train,X_train))\n",
    "raw_train_ds = tf.data.Dataset.from_tensor_slices((i_train, \n",
    "                                                   y_train.values,\n",
    "                                                   X_train.values))\n",
    "\n",
    "The last one is almost working, but at later stage when I try to pack the input, it breaks. Upon closer examination, the train_ds looks like:\n",
    "\n",
    "```\n",
    "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
    "```\n",
    "\n",
    "But what I need is more like:\n",
    "\n",
    "```\n",
    "<PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'sentence1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'sentence2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = train.iloc[:3,:]\n",
    "raw_trial_ds = (tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(trial['txt'].values, tf.string),\n",
    "         tf.cast(trial['label'].values, tf.int32),)))\n",
    "raw_trial_ds\n",
    "# Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call \"trial\" so it does not mix with the testing set\n",
    "trial_dict   = {\"idx\":[123,322], \"label\":[0,1], \n",
    "               \"txt\":[\"The 1st sentence\", \"The second\"]}\n",
    "\n",
    "# This creates a TensorSliceDataset\n",
    "raw_trial_ds =  tf.data.Dataset.from_tensor_slices(trial_dict)\n",
    "\n",
    "# Now this has the right structure!!!\n",
    "raw_trial_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_batch, label_batch, text_batch in raw_trial_ds.take(1):\n",
    "  for i in range(1):\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_trial_ds_batch = raw_trial_ds.batch(batch_size)\n",
    "type(raw_trial_ds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ds = raw_trial_ds_batch.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "len(trial_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still have the right structure.\n",
    "trial_ds.element_spec['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "take1 = trial_ds.take(1)\n",
    "help(take1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train, valid, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(df):\n",
    "  '''Convert DataFrame to PrefetchDataset\n",
    "  Args:\n",
    "    df (dataframe): with two columns ('txt', and 'label') and indices\n",
    "  Return:\n",
    "    dataset (PrefetchDataset): with:\n",
    "      <PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.int32, name=None), \n",
    "                                     'label': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.int32, name=None), \n",
    "                                     'txt': TensorSpec(shape=(None,), \n",
    "                                            dtype=tf.string, name=None)}>\n",
    "\n",
    "  '''\n",
    "  idx = df.index   # aded this, as the tutorial has an index input\n",
    "  X   = df['txt']\n",
    "  y   = df['label']\n",
    "\n",
    "  df_dict = {\"idx\":idx, \"label\":y.values, \"txt\":X.values}\n",
    "\n",
    "  raw_ds       = tf.data.Dataset.from_tensor_slices(df_dict)  \n",
    "  raw_ds_batch = raw_ds.batch(batch_size)\n",
    "  dataset      = raw_ds_batch.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data\n",
    "train_ds = dataframe_to_dataset(train)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation dataset\n",
    "valid_ds = dataframe_to_dataset(valid)\n",
    "\n",
    "# Get testing dataset\n",
    "test_ds = dataframe_to_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Testing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  print(text_batch[0])\n",
    "  print(len(text_batch))\n",
    "  print(label_batch)\n",
    "  print(len(label_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.751221Z",
     "iopub.status.busy": "2022-03-29T12:30:15.750998Z",
     "iopub.status.idle": "2022-03-29T12:30:15.778963Z",
     "shell.execute_reply": "2022-03-29T12:30:15.778411Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('bert_finetune')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "323c618d0395b34183a36199d7c8eddbd4e55d51aee9dabbfbc9809db817fb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
