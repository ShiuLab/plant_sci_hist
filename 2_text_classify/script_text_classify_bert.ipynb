{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 2c: BERT models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Imports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For building text classification model based on embedding of Word2Vec and BERT\n",
    "'''\n",
    "\n",
    "## for data\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "## for word embedding with w2v\n",
    "import gensim\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, callbacks, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'bert_param':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def split_train_validate_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train, validation, test subsets for the cleaned texts\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train, valid, test (pandas dataframes): training, validation, testing sets\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus = corpus_combo[['label','txt_clean']]\n",
    "\n",
    "  # Split train test\n",
    "  train, test = model_selection.train_test_split(corpus, \n",
    "      test_size=0.2, stratify=corpus['label'], random_state=rand_state)\n",
    "\n",
    "  # Split train validate\n",
    "  train, valid = model_selection.train_test_split(train, \n",
    "      test_size=0.25, stratify=train['label'], random_state=rand_state)\n",
    "\n",
    "  X_train = train['txt_clean']\n",
    "  X_valid = valid['txt_clean']\n",
    "  X_test  = test['txt_clean']\n",
    "  y_train = train['label']\n",
    "  y_valid = valid['label']\n",
    "  y_test  = test['label']\n",
    "\n",
    "  print(f\"    size: train={X_train.shape}, valid={X_valid.shape},\" +\\\n",
    "        f\" test={X_test.shape}\")\n",
    "\n",
    "  return [X_train, X_valid, X_test, y_train, y_valid, y_test]\n",
    "  \n",
    "def get_hyperparameters(w2v_param):\n",
    "  ''' Return a list with hyperparameters based on the passed dictionary\n",
    "  Adopted from:\n",
    "    https://stackoverflow.com/questions/38721847/how-to-generate-all-combination-from-values-in-dict-of-lists-in-python\n",
    "  Args:\n",
    "    param (dict): a dictionary specified in the config.txt file.\n",
    "  Return:\n",
    "    param_list (list): a nested list of hyperparameters in the order of\n",
    "      max_feature, ngram_range, and p_threshold\n",
    "  '''\n",
    "  print(w2v_param)\n",
    "  keys, values = zip(*w2v_param.items())\n",
    "  param_list = [v for v in itertools.product(*values)]\n",
    "  \n",
    "  return keys, param_list\n",
    "\n",
    "def get_unigram(corpus):\n",
    "  unigram = []\n",
    "  for txt in corpus:\n",
    "    lst_words = txt.split()\n",
    "    unigram.append(lst_words)\n",
    "\n",
    "  return unigram\n",
    "\n",
    "def get_ngram(X_train, X_valid, X_test, ngram):\n",
    "\n",
    "  uni_train = get_unigram(X_train)\n",
    "  uni_valid = get_unigram(X_valid)\n",
    "  uni_test  = get_unigram(X_test)\n",
    "\n",
    "  if ngram == 1:\n",
    "    return uni_train, uni_valid, uni_test\n",
    "  # ngram >1\n",
    "  else:\n",
    "    # Get bigrams\n",
    "    bigrams_detector  = gensim.models.phrases.Phrases(\n",
    "                          uni_train, delimiter=\" \", min_count=5, threshold=10)\n",
    "    bigrams_detector  = gensim.models.phrases.Phraser(bigrams_detector)\n",
    "    bi_train = list(bigrams_detector[uni_train])\n",
    "    bi_valid = list(bigrams_detector[uni_valid])\n",
    "    bi_test  = list(bigrams_detector[uni_test])\n",
    "\n",
    "    # Return bigrams\n",
    "    if ngram == 2:\n",
    "      return bi_train, bi_valid, bi_test\n",
    "\n",
    "    # Get trigrams and return them\n",
    "    elif ngram == 3:\n",
    "      trigrams_detector = gensim.models.phrases.Phrases(\n",
    "                          bigrams_detector[uni_train], delimiter=\" \", \n",
    "                          min_count=5, threshold=10)\n",
    "      trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
    "      tri_train = list(trigrams_detector[bi_train])\n",
    "      tri_valid = list(trigrams_detector[bi_valid])\n",
    "      tri_test  = list(trigrams_detector[bi_test])\n",
    "      return tri_train, tri_valid, tri_test\n",
    "    \n",
    "    else:\n",
    "      print('ERR: ngram cannot be larger than 3. QUIT!')\n",
    "      sys.exit(0)\n",
    "\n",
    "\n",
    "def get_w2v_model(X_train, X_valid, X_test, param, rand_state):\n",
    "  '''Get ngram lists and w2v model\n",
    "  Args:\n",
    "  Return:\n",
    "  '''\n",
    "  [min_count, window, ngram] = param\n",
    "\n",
    "  ngram_train, ngram_valid, ngram_test = get_ngram(X_train, X_valid, X_test, \n",
    "                                                  ngram)\n",
    "\n",
    "  # Check if w2v model is already generated\n",
    "  model_w2v_name = work_dir / f\"model_cln_w2v_{min_count}-{window}-{ngram}\"\n",
    "\n",
    "  if model_w2v_name.is_file():\n",
    "    print(\"  load the w2v model\")\n",
    "    with open(work_dir / model_w2v_name, \"rb\") as f:\n",
    "        model_w2v = pickle.load(f)\n",
    "  else:\n",
    "    print(\"  geneate and save w2v model\")\n",
    "    model_w2v = gensim.models.Word2Vec(ngram_train, vector_size=300, \n",
    "                                      window=window, min_count=min_count, \n",
    "                                      sg=1, epochs=30, seed=rand_state)\n",
    "    \n",
    "    with open(model_w2v_name, \"wb\") as f:\n",
    "      pickle.dump(model_w2v, f)\n",
    "\n",
    "  return model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test\n",
    "\n",
    "\n",
    "def train_tokenizer(corpus):\n",
    "  '''Train a tokenizer\n",
    "  Args:\n",
    "    corpus (list): a nested list of word lists\n",
    "  Return:\n",
    "    tokenizer (keras.preprocessing.text.Tokenizer): trained tokenizer\n",
    "    dic_vocab_token (dict): token as key, index as value\n",
    "  '''\n",
    "\n",
    "  # intialize tokenizer\n",
    "  # See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "  # This is replaced by tf.keras.layers.TextVectorization\n",
    "  tokenizer = preprocessing.text.Tokenizer(lower=True, split=' ', \n",
    "                oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "  # tokenize corpus \n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "  # get token dictionary, with token as key, index number as value\n",
    "  dic_vocab_token = tokenizer.word_index\n",
    "\n",
    "  return tokenizer, dic_vocab_token\n",
    "\n",
    "\n",
    "def get_embeddings(corpus, model_w2v, tokenizer, dic_vocab_token):\n",
    "\n",
    "  # Transforms each text in texts to a sequence of integers.\n",
    "  lst_text2seq = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "  # pad or trucate sequence\n",
    "  X_w2v = preprocessing.sequence.pad_sequences(\n",
    "                    lst_text2seq,      # List of sequences, each a list of ints \n",
    "                    maxlen=500,        # maximum length of all sequences\n",
    "                    padding=\"post\",    # 'pre' or 'post' \n",
    "                    truncating=\"post\") # remove values from sequences > maxlen\n",
    "\n",
    "  ## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "\n",
    "  embeddings = np.zeros((len(dic_vocab_token)+1, 300))\n",
    "  not_in_emb = {}\n",
    "  for word, idx in dic_vocab_token.items():\n",
    "      ## update the row with vector\n",
    "      try:\n",
    "          embeddings[idx] =  model_w2v.wv[word]\n",
    "      ## if word not in model then skip and the row stays all 0s\n",
    "      except KeyError:\n",
    "          not_in_emb[word] = 1\n",
    "\n",
    "  return embeddings, X_w2v\n",
    "\n",
    "\n",
    "def get_w2v_emb_model(embeddings):\n",
    "  '''Build a deep learning model with Word2Vec embeddings\n",
    "  Args:\n",
    "    embeddings\n",
    "  '''\n",
    "\n",
    "  ## code attention layer\n",
    "  def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "  ## input\n",
    "  x_in = layers.Input(shape=(500,)) ## embedding\n",
    "  x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                      output_dim=embeddings.shape[1], \n",
    "                      weights=[embeddings],\n",
    "                      input_length=500, trainable=False)(x_in)\n",
    "\n",
    "  ## apply attention\n",
    "  x = attention_layer(x, neurons=500)\n",
    "\n",
    "  ## 2 layers of bidirectional lstm\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                          return_sequences=True))(x)\n",
    "  x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "  ## final dense layers\n",
    "  x = layers.Dense(64, activation='relu')(x)\n",
    "  y_out = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "  ## Initialize and compile model\n",
    "  model = models.Model(x_in, y_out)\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def run_main_function():\n",
    "\n",
    "  pass\n",
    "\n",
    "\n",
    "def run_pipeline(param, subsets):\n",
    "  '''Carry out the major steps'''\n",
    "\n",
    "  rand_state = config_dict['rand_state']\n",
    "\n",
    "  [X_train, X_valid, X_test, y_train, y_valid, y_test] = subsets\n",
    "\n",
    "  # Get list of ngrams and w2v model\n",
    "  print(\"  get list of ngrams and w2v model\")\n",
    "  model_w2v, model_w2v_name, ngram_train, ngram_valid, ngram_test = \\\n",
    "                      get_w2v_model(X_train, X_valid, X_test, param, rand_state)\n",
    "  \n",
    "  # Train tokenizer\n",
    "  print(\"  train tokenizer\")\n",
    "  tokenizer, dic_vocab_token = train_tokenizer(ngram_train)\n",
    "\n",
    "  # Get embeddings\n",
    "  print(\"  get embeddings\")\n",
    "  embeddings, X_train_w2v = get_embeddings(ngram_train, model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "  _, X_valid_w2v = get_embeddings(ngram_valid, model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "  _ , X_test_w2v  = get_embeddings(ngram_test , model_w2v, \n",
    "                                                    tokenizer, dic_vocab_token)\n",
    "\n",
    "  # Model checkpoint path and output model file name\n",
    "  cp_filepath  = Path(str(model_w2v_name) + \"_dnn\")\n",
    "\n",
    "  # Load model if exists\n",
    "  if cp_filepath.is_dir():\n",
    "    print(\"  load model in:\", cp_filepath)\n",
    "    model_emb = models.load_model(cp_filepath)\n",
    "\n",
    "  # Train and save model if not\n",
    "  else:\n",
    "    print(\"  train model\")\n",
    "    model_emb    = get_w2v_emb_model(embeddings)\n",
    "\n",
    "    # setup check points\n",
    "    callback_es  = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    callback_mcp = callbacks.ModelCheckpoint(filepath=cp_filepath, mode='max', \n",
    "            save_weights_only=False, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "    # Train model\n",
    "    history = model_emb.fit(x=X_train_w2v, y=y_train, batch_size=256, \n",
    "                            epochs=20, shuffle=True, verbose=1, \n",
    "                            validation_data=(X_valid_w2v, y_valid), \n",
    "                            callbacks=[callback_es, callback_mcp])\n",
    "\n",
    "  print(\"  get validation f1 score\")\n",
    "  y_valid_pred_prob = model_emb.predict(X_valid_w2v)\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_valid))}\n",
    "  y_valid_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_valid_pred_prob]\n",
    "  best_score = metrics.f1_score(y_valid, y_valid_pred)\n",
    "  print(\"    \", best_score)\n",
    "\n",
    "  print(\"  get testing f1 score\")\n",
    "  y_test_pred_prob = model_emb.predict(X_test_w2v)\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_test))}\n",
    "  y_test_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred_prob]\n",
    "  test_score = metrics.f1_score(y_test, y_test_pred)\n",
    "  print(\"    \", test_score)\n",
    "\n",
    "  # provide some space between runs\n",
    "  print('\\n')\n",
    "\n",
    "  return best_score, cp_filepath, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get training/testing split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read configuration file...\n",
      "   lang_model = bert\n",
      "   proj_dir = /home/shius/projects/plant_sci_hist\n",
      "   work_dir = 2_text_classify\n",
      "   corpus_combo_file = corpus_combo\n",
      "   rand_state = 20220609\n",
      "   bert_param = {}\n",
      "  all config available\n"
     ]
    }
   ],
   "source": [
    "config_file = Path(\"config_bert.txt\")\n",
    "\n",
    "print(\"\\nRead configuration file...\")\n",
    "config_dict = read_configs(config_file)\n",
    "\n",
    "# Set up working directory and corpus file location\n",
    "proj_dir          = Path(config_dict['proj_dir'])\n",
    "work_dir          = proj_dir / config_dict['work_dir']\n",
    "corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "\n",
    "# For reproducibility\n",
    "rand_state = config_dict['rand_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read file and split train/validate/test...\n",
      "    size: train=(51987,), valid=(17329,), test=(17330,)\n"
     ]
    }
   ],
   "source": [
    "# Split train/validate/test for cleaned text\n",
    "#   Will not focus on original due to issues with non-alphanumeric characters\n",
    "#   and stop words.\n",
    "print(\"\\nRead file and split train/validate/test...\")\n",
    "subsets = split_train_validate_test(corpus_combo_file, rand_state)\n",
    "[X_train, X_valid, X_test, y_train, y_valid, y_test] = subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # get w2c parameter list\n",
    "  #   [min_count, window, ngram]\n",
    "  param_keys, param_list  = get_hyperparameters(w2v_param)\n",
    "\n",
    "  # iterate through different parameters\n",
    "  with open(work_dir / f\"scores_cln_w2v\", \"w\") as f:\n",
    "    f.write(\"run\\ttxt_flag\\tlang_model\\tparameters\\tvalidate_f1\\t\" +\\\n",
    "            \"test_f1\\tmodel_dir\\n\")\n",
    "    run_num = 0\n",
    "    for param in param_list:\n",
    "      print(f\"\\n## param: {param}\")\n",
    "      best_score, model_dir, test_score = run_pipeline(param, subsets)\n",
    "\n",
    "      f.write(f\"{run_num}\\tcln\\t{lang_model}\\t{str(param)}\\t\"+\\\n",
    "              f\"{best_score}\\t{test_score}\\t{model_dir}\\n\")\n",
    "\n",
    "      run_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test for original and cleaned text\n",
    "print(\"\\nRead file and split train/test...\")\n",
    "train_ori, test_ori, train_cln, test_cln = split_train_test(\n",
    "                                              corpus_combo_file, rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Set up the corpus__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Create a list of unigram lists_\n",
    "\n",
    "A nested list with the first dimension the number of training instances:\n",
    "- 69316\n",
    "\n",
    "Q: Why not use tokenizer now instead of later?\n",
    "- Need to get the unigrams so a tokenizer can be trained.\n",
    "\n",
    "Q: Why bi- and tri-gram detector not used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cleaned text as corpus\n",
    "corpus = train_cln['txt_clean'] # pandas Series\n",
    "type(corpus), corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "lst_corpus_test = []\n",
    "for string in corpus:\n",
    "\n",
    "   # Q: lst_words and lst_grams are the same, what's the point?\n",
    "   lst_words = string.split()\n",
    "   #lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "   #            for i in range(0, len(lst_words), 1)]\n",
    "   #lst_corpus.append(lst_grams)\n",
    "   #lst_corpus_test.append(lst_words)\n",
    "   lst_corpus.append(lst_words)\n",
    "\n",
    "len(lst_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if lst_words and lst_grams are the same. They are EXACTLY the same. So\n",
    "# did not use the lst_gram part in the cell above.\n",
    "'''\n",
    "count_not_the_same = 0\n",
    "for i in range(len(lst_corpus)):\n",
    "    gram = lst_corpus[i]\n",
    "    word = lst_corpus_test[i]\n",
    "    if gram != word:\n",
    "        count_not_the_same += 1\n",
    "print(count_not_the_same)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect bigrams and trigrams\n",
    "bigrams_detector  = gensim.models.phrases.Phrases(lst_corpus, \n",
    "                                                  delimiter=\" \", \n",
    "                                                  min_count=5, \n",
    "                                                  threshold=10)\n",
    "bigrams_detector  = gensim.models.phrases.Phraser(bigrams_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the input the trigrams_detector is output of the bigrams_detector\n",
    "trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
    "                                                  delimiter=\" \", \n",
    "                                                  min_count=5, \n",
    "                                                  threshold=10)\n",
    "trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d8b33",
   "metadata": {},
   "source": [
    "### _Initialize Word2Vec model_\n",
    "\n",
    "- `sequences`: lst_corpus\n",
    "- `vector_size`: dimension of word embeddings\n",
    "- `window`: max distance between the current and the predicted words in a sentence\n",
    "- `min_count`: ignore all words with total frquency lower than this.\n",
    "  - [Discussion on seeting min_count](https://stackoverflow.com/questions/50723303/how-is-word2vec-min-count-applied)\n",
    "- `sg`: history algorithm, 1: skip-gram, otherwise CBOW.\n",
    "\n",
    "Following [this](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook).\n",
    "- Q: Why don't we train the w2v model using bi and tri-grams?\n",
    "  - See [this article](https://www.kaggle.com/code/hamishdickson/training-and-plotting-word2vec-with-bigrams/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.models.Word2Vec(vector_size=300, window=8, \n",
    "                                   min_count=20, sg=1, epochs=30, workers=16,\n",
    "                                   seed=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Vocabulary Table\n",
    "model_w2v.build_vocab(lst_corpus, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train w2v model\n",
    "model_w2v.train(lst_corpus, total_examples=model_w2v.corpus_count, epochs=30,\n",
    "                report_delay=1)\n",
    "\n",
    "# Save the w2v model\n",
    "with open(work_dir / \"model_cln_w2v\", \"wb\") as f:\n",
    "    pickle.dump(model_w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the w2v model\n",
    "with open(work_dir / \"model_cln_w2v\", \"rb\") as f:\n",
    "    model_w2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the w2v model\n",
    "# Here there is problem with stop words. Like 'jasmonate..', '(MeJA)', and other\n",
    "# variants. So should use the cleaned text.\n",
    "example = \"jasmonate\"\n",
    "print(len(model_w2v.wv[example]))\n",
    "print(model_w2v.wv.most_similar(example, topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Feature engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79996785",
   "metadata": {},
   "source": [
    "#### _Train tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "                        lower=True, \n",
    "                        split=' ', \n",
    "                        oov_token=\"NaN\", \n",
    "                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "# tokenize corpus \n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "\n",
    "# get token dictionary, with token as key, index number as value\n",
    "dic_vocab_token = tokenizer.word_index\n",
    "len(dic_vocab_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f3631",
   "metadata": {},
   "source": [
    "#### _Turn texts into index numbers_\n",
    "\n",
    "Transforms each text in texts to a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b631dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
    "print(lst_corpus[0][:5])\n",
    "print(lst_text2seq[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index numbers are the values from the token dictionary\n",
    "# Note that these are lowercased\n",
    "dic_vocab_token['update'], dic_vocab_token['exertional']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Pad or trucate sequences_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499096a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = keras.preprocessing.sequence.pad_sequences(\n",
    "                    lst_text2seq,      # List of sequences, each a list of ints \n",
    "                    maxlen=500,         # maximum length of all sequences\n",
    "                    padding=\"post\",    # 'pre' or 'post' \n",
    "                    truncating=\"post\") # remove values from sequences > maxlen\n",
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Create embedding matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "\n",
    "embeddings = np.zeros((len(dic_vocab_token)+1, 300))\n",
    "not_in_emb = {}\n",
    "for word, idx in dic_vocab_token.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  model_w2v.wv[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except KeyError:\n",
    "        not_in_emb[word] = 1\n",
    "\n",
    "len(not_in_emb) # Q: How did this got into the corpus??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Set up ANN_\n",
    "\n",
    "The model contains:\n",
    "- An embedding layer:\n",
    "  - Sequences as input (15 tokens, including padding)\n",
    "  - Word (embedding?) vectors as weights (what??)\n",
    "  - Embedding as output (15x300).\n",
    "- An attention layer\n",
    "  - Capture the eughts of each instance for building an explaniner.\n",
    "  - Not needed for the predictions.\n",
    "- Two layers of bidirectional LSTM.\n",
    "- Two final dense layer to predict probabilities of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13763269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_emb_model(embeddings):\n",
    "\n",
    "    ## code attention layer\n",
    "    def attention_layer(inputs, neurons):\n",
    "        x = layers.Permute((2,1))(inputs)\n",
    "        x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "        x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "        x = layers.multiply([inputs, x])\n",
    "        return x\n",
    "\n",
    "    ## input\n",
    "    x_in = layers.Input(shape=(500,)) ## embedding\n",
    "    x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                        output_dim=embeddings.shape[1], \n",
    "                        weights=[embeddings],\n",
    "                        input_length=500, trainable=False)(x_in)\n",
    "\n",
    "    ## apply attention\n",
    "    x = attention_layer(x, neurons=500)\n",
    "\n",
    "    ## 2 layers of bidirectional lstm\n",
    "    x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                            return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "    ## final dense layers\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    y_out = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    ## Initialize and compile model\n",
    "    model = models.Model(x_in, y_out)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = get_w2v_emb_model(embeddings)\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5796c1",
   "metadata": {},
   "source": [
    "#### _Convert text labels to numeric ones_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7818d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y\n",
    "#  This is the class label, not sure why inversse is done\n",
    "y_train       = train_cln['label']\n",
    "dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "inverse_dic   = {v:k for k,v in dic_y_mapping.items()}\n",
    "inverse_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397318ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text labels to numeric ones.\n",
    "#y_train_label = np.array([inverse_dic[y] for y in y_train])\n",
    "y_train_label = y_train\n",
    "X_train_w2v.shape, len(y_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7c049",
   "metadata": {},
   "source": [
    "#### _Train model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c21389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "callback = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model_emb.fit(x=X_train_w2v, y=y_train_label, batch_size=256, \n",
    "                        epochs=20, shuffle=True, verbose=1, \n",
    "                        validation_split=0.3, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_emb.save('model_cln_w2v_dnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a4dc6",
   "metadata": {},
   "source": [
    "#### _Plot loss and accuracy_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_keys = history.history.keys()\n",
    "his_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "df_history_loss = pd.DataFrame(history.history)[['loss','val_loss']]\n",
    "df_history_loss.plot(ax=ax1)\n",
    "df_history_accu = pd.DataFrame(history.history)[['accuracy','val_accuracy']]\n",
    "df_history_accu.plot(ax=ax2)\n",
    "ax1.grid(True); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax2.grid(True); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Get prediction f1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_prob = model_emb.predict(X_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = [dic_y_mapping[np.argmax(pred)] for pred in y_train_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_train_label, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ec201",
   "metadata": {},
   "source": [
    "#### _Evaluate model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of n-grams for test set\n",
    "X_test = test_cln['txt_clean']\n",
    "y_test = test_cln['label']\n",
    "\n",
    "lst_corpus_test = []\n",
    "for text in X_test:\n",
    "    lst_words = text.split()\n",
    "    lst_corpus_test.append(lst_words)\n",
    "\n",
    "## Detect common bigram and trigram with fitted detectors\n",
    "lst_corpus_test_bi = list(bigrams_detector[lst_corpus_test])\n",
    "lst_corpus_test_tr = list(trigrams_detector[lst_corpus_test_bi])\n",
    "\n",
    "len(lst_corpus_test_bi), len(lst_corpus_test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq_test = tokenizer.texts_to_sequences(lst_corpus_test_tr)\n",
    "\n",
    "## padding sequence\n",
    "X_test_w2v = keras.preprocessing.sequence.pad_sequences(lst_text2seq_test, \n",
    "                                maxlen=500, padding=\"post\", truncating=\"post\")\n",
    "X_test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a31ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_w2v = model_emb.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_w2v[1:4,] # Why are there 3 columns??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q: Why use dic_y_mapping instead of inverse_dic???\n",
    "y_pred_w2v      = [dic_y_mapping[np.argmax(pred)] for pred in y_pred_prob_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = metrics.f1_score(y_test, y_pred_w2v)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __NOT USED__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166e656",
   "metadata": {},
   "source": [
    "#### _Check min, max, avg len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT Run ###\n",
    "'''\n",
    "minlen = 100; maxlen = 0; totlen = 0\n",
    "lst_0  = []   # index of sequences with zero lengths\n",
    "\n",
    "for idx in tqdm(range(len(lst_text2seq))):\n",
    "    slen   = len(lst_text2seq[idx])\n",
    "    totlen +=slen\n",
    "    if slen > maxlen: maxlen = slen\n",
    "    if slen < minlen: \n",
    "        if slen == 0: lst_0.append(idx)\n",
    "        else: minlen = slen\n",
    "print(f'Min:{minlen}, Max:{maxlen}, Avg:{totlen/len(lst_text2seq)}')\n",
    "print('Zero length:', lst_0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e975340",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation\n",
    "def eval_model(y_test, y_pred, y_pred_prob, plot_auc=1):\n",
    "    \n",
    "    classes = np.unique(y_test)\n",
    "\n",
    "    # pd.get_dummies: Convert categorical variable into dummy/indicator \n",
    "    # variables.\n",
    "    y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "    ## Accuracy, Precision, Recall\n",
    "    #accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_prob[:, 1])\n",
    "    #print(\"Accuracy:\",  round(accuracy,2))\n",
    "    print(\"Auc:\", round(auc,2))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "        \n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    _, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "        yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Setup subplots\n",
    "    if plot_auc:\n",
    "        _, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "        ## Plot roc\n",
    "        for i in range(len(classes)):\n",
    "                fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                                y_pred_prob[:,i])\n",
    "                ax[0].plot(fpr, tpr, lw=3, \n",
    "                        label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                        metrics.auc(fpr, tpr))\n",
    "                        )\n",
    "        ax[0].plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "        ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "                xlabel='False Positive Rate', \n",
    "                ylabel=\"True Positive Rate (Recall)\", \n",
    "                title=\"Receiver operating characteristic\")\n",
    "        ax[0].legend(loc=\"lower right\")\n",
    "        ax[0].grid(True)\n",
    "                \n",
    "        ## Plot precision-recall curve\n",
    "        for i in range(len(classes)):\n",
    "                precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                        y_test_array[:,i], y_pred_prob[:,i])\n",
    "                ax[1].plot(recall, precision, lw=3, \n",
    "                        label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                        metrics.auc(recall, precision))\n",
    "                        )\n",
    "        ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "                ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "        ax[1].plot([0,1], [1/3,1/3], color='navy', lw=2, linestyle='--')\n",
    "        ax[1].legend(loc=\"best\")\n",
    "        ax[1].grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(y_test, y_pred_w2v, y_pred_prob_w2v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd3af54f5fe992bccbd23931b262c263c643af7052ca64c3b616d552ec510a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
