{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 2b: Word2Vec and BERT models__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Setup__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Imports_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For building text classification model based on embedding of Word2Vec and BERT\n",
    "'''\n",
    "\n",
    "## For reproducibility\n",
    "rand_state = 20220609\n",
    "\n",
    "## for data\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import sys\n",
    "from os import chdir\n",
    "from pathlib import Path\n",
    "\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, feature_selection, metrics\n",
    "from sklearn import model_selection\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4016c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shius/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## for data\n",
    "from tqdm import tqdm\n",
    "from numpy.random import randint\n",
    "\n",
    "## for word embedding with w2v\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, preprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "## for bert language model\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_configs(config_file):\n",
    "  \"\"\"Read configuration file and return a config_dict\"\"\"\n",
    "  # required\n",
    "  config_dict = {'lang_model':0,\n",
    "                 'proj_dir':0,\n",
    "                 'work_dir':0,\n",
    "                 'corpus_combo_file':0,\n",
    "                 'rand_state':0,\n",
    "                 'p_threshold':0,\n",
    "                 'xg_param':0,\n",
    "                 'n_splits':0,\n",
    "                 'xg_param_comb':0,\n",
    "                 'n_jobs':0,}\n",
    "\n",
    "  # Read config file and fill in the dictionary\n",
    "  with open(config_file, 'r') as f:\n",
    "    configs     = f.readlines()\n",
    "    for config in configs:\n",
    "      if config.strip() == \"\" or config[0] == \"#\":\n",
    "        pass\n",
    "      else:\n",
    "        config = config.strip().split(\"=\")\n",
    "        if config[0] in config_dict:\n",
    "          config_dict[config[0]] = eval(config[1])\n",
    "\n",
    "  # Check if any config missing\n",
    "  missing = 0\n",
    "  for config in config_dict:\n",
    "    if config_dict[config] == 0:\n",
    "      print(\"  missing:\", config)\n",
    "      missing += 1\n",
    "    else:\n",
    "      print(\"  \", config, \"=\", config_dict[config])\n",
    "\n",
    "  if missing == 0:\n",
    "    print(\"  all config available\")\n",
    "  else:\n",
    "    print(\"  missing config, QUIT!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "  return config_dict\n",
    "\n",
    "\n",
    "def split_train_test(corpus_combo_file, rand_state):\n",
    "  '''Load data and split train test\n",
    "  Args:\n",
    "    corpus_combo_file (str): path to the json data file\n",
    "    rand_state (int): for reproducibility\n",
    "  Return:\n",
    "    train_ori, test_ori, train_cln, test_cln (pandas dataframes): for the\n",
    "      original and clean texts, training and testing splits.\n",
    "  '''\n",
    "  # Load json file\n",
    "  with corpus_combo_file.open(\"r+\") as f:\n",
    "      corpus_combo_json = json.load(f)\n",
    "\n",
    "  # Convert json back to dataframe\n",
    "  corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "  corpus_ori = corpus_combo[['label','txt']]\n",
    "  train_ori, test_ori = model_selection.train_test_split(corpus_ori, \n",
    "      test_size=0.2, stratify=corpus_ori['label'], random_state=rand_state)\n",
    "\n",
    "  # Cleaned corpus\n",
    "  corpus_cln = corpus_combo[['label','txt_clean']]\n",
    "  corpus_cln.rename(columns={'txt_clean': 'txt'}) # make col names consistent\n",
    "  train_cln, test_cln = model_selection.train_test_split(corpus_cln, \n",
    "      test_size=0.2, stratify=corpus_cln['label'], random_state=rand_state)\n",
    "\n",
    "  return train_ori, test_ori, train_cln, test_cln\n",
    "\n",
    "\n",
    "def select_feat(X_train, y_train, vectorizer, p_threshold):\n",
    "  '''Select features based on chi-square test results\n",
    "  Args:\n",
    "    X_train (pandas series): the txt column in the training data frame\n",
    "    y_train (pandas series): the label column in the training data frame\n",
    "    vecorizer: fitted with original X_train and returned from get_vectorizer()\n",
    "    p_threshold (float): p is derived from chi-square test. Features with p <= \n",
    "      p_threshold_s are selected.\n",
    "  Return:\n",
    "    X_names (list): names of selected features\n",
    "  '''\n",
    "  y            = y_train\n",
    "  X_names      = vectorizer.get_feature_names_out()\n",
    "  dtf_features = pd.DataFrame()\n",
    "  for cat in np.unique(y):\n",
    "    _, p = feature_selection.chi2(X_train, y==cat)\n",
    "    dtf_features = pd.concat([dtf_features, \n",
    "                pd.DataFrame({\"feature\":X_names, \"p\":p, \"y\":cat})])\n",
    "    dtf_features = dtf_features.sort_values(\n",
    "                [\"y\",\"p\"], ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"p\"] <= p_threshold]\n",
    "  \n",
    "  X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "\n",
    "  return X_names\n",
    "\n",
    "\n",
    "def run_xgboost(X_train, y_train, config_dict):\n",
    "  '''Do hyperparameter tuning and cross-validation of XgBoost models\n",
    "  Args:\n",
    "    X_train (pandas dataframe): features\n",
    "    y_train (pandas series): labels\n",
    "    config_dict (dict): from read_config()\n",
    "  Return:\n",
    "    rand_search (RandomizedSearchCV): fitted obj\n",
    "  '''\n",
    "\n",
    "  rand_state = config_dict[\"rand_state\"]\n",
    "  param_grid = config_dict[\"xg_param\"] \n",
    "  n_splits   = config_dict[\"n_splits\"]\n",
    "  param_comb = config_dict[\"xg_param_comb\"]\n",
    "  n_jobs     = config_dict[\"n_jobs\"]\n",
    "\n",
    "  # Initialize classifier\n",
    "  # 06/11/2022: the silent parameter is deprecated, use verbosity=0\n",
    "  xgb = XGBClassifier(learning_rate=0.02, \n",
    "                      n_estimators=600, \n",
    "                      objective='binary:logistic',\n",
    "                      verbosity=1, \n",
    "                      nthread=1)\n",
    "  # Initilize stratified k fold obj\n",
    "  skf = model_selection.StratifiedKFold(n_splits=n_splits, \n",
    "                      shuffle = True, random_state = rand_state)\n",
    "  # initiate randomized search CV obj\n",
    "  rand_search = model_selection.RandomizedSearchCV(\n",
    "                      xgb                , param_distributions = param_grid, \n",
    "                      n_iter = param_comb, scoring      = 'f1', \n",
    "                      n_jobs = n_jobs    , cv = skf.split(X_train,y_train), \n",
    "                      verbose = 3        , random_state =rand_state)\n",
    "  # Train\n",
    "  rand_search.fit(X_train, y_train)\n",
    "\n",
    "  return rand_search\n",
    "\n",
    "\n",
    "def run_main_function(work_dir, train, test, txt_flag, config_dict):\n",
    "\n",
    "  # Get the training/testing corpus and labels\n",
    "  if txt_flag == \"ori\":\n",
    "    X_train = train['txt']\n",
    "    X_test  = test['txt']\n",
    "  else:\n",
    "    X_train = train['txt_clean']\n",
    "    X_test  = test['txt_clean']\n",
    "\n",
    "  y_train = train['label']\n",
    "  y_test  = test['label']\n",
    "\n",
    "  # get vectorizer parameter list\n",
    "  p_threshold = config_dict['p_threshold']\n",
    "  param_list  = get_hyperparameters(config_dict['vec_param'], p_threshold)\n",
    "  lang_model  = config_dict['lang_model']\n",
    "\n",
    "  # iterate through different parameters\n",
    "  with open(work_dir / f\"scores_{txt_flag}\", \"w\") as f:\n",
    "    f.write(\"run\\ttxt_flag\\tlang_model\\tparameters\\tnum_feat\\tcv_f1\\t\" +\\\n",
    "            \"test_f1\\tmodel_name\\n\")\n",
    "    run_num = 0\n",
    "    for param in param_list:\n",
    "      print(f\"\\n## param: {param}\")\n",
    "      best_score, num_select, model_name, test_score = run_pipeline(\n",
    "        work_dir, X_train, y_train, X_test, y_test, param, txt_flag, config_dict)\n",
    "\n",
    "      f.write(f\"{run_num}\\t{txt_flag}\\t{lang_model}\\t{str(param)}\\t\"+\\\n",
    "              f\"{num_select}\\t{best_score}\\t{test_score}\\t{model_name}\\n\")\n",
    "\n",
    "      run_num += 1\n",
    "\n",
    "\n",
    "def run_pipeline(work_dir, X_train, y_train, X_test, y_test, param, txt_flag,\n",
    "                 config_dict):\n",
    "  '''Carry out the major steps'''\n",
    "\n",
    "  # Get vectorizer and fitted X_train\n",
    "  print(\"  extract features by fitting a vectorizer\")\n",
    "  lang_model = config_dict['lang_model']\n",
    "  vectorizer, X_train_vec = extract_feat(X_train, param, lang_model)\n",
    "  print(\"    train dim:\", X_train_vec.shape)\n",
    "\n",
    "  # Get selected feature names\n",
    "  print(\"  select features\")\n",
    "  p_threshold = config_dict['p_threshold']\n",
    "  X_names     = select_feat(X_train_vec, y_train, vectorizer, p_threshold)\n",
    "  num_select  = len(X_names)\n",
    "  print('    total selected:', num_select)\n",
    "\n",
    "  # Refit vectorizer with selected features and re-transform X_train\n",
    "  print(\"  refit vectorizer with training data and transform\")\n",
    "  vectorizer_sel, X_train_vec_sel = extract_feat(X_train, vocab=X_names)\n",
    "  print(\"    train dim:\", X_train_vec_sel.shape)\n",
    "\n",
    "  # Also apply the refitted vecorizer to testing data\n",
    "  print(\"  transform testing data\")\n",
    "  X_test_vec_sel = vectorizer_sel.transform(X_test)\n",
    "  print(\"    test dim:\", X_test_vec_sel.shape)\n",
    "\n",
    "  # Get xgboost model and cv results\n",
    "  print(\"  cross-validation and tuning with xgboost\")\n",
    "  rand_search = run_xgboost(X_train_vec_sel, y_train, config_dict)\n",
    "\n",
    "  best_est   = rand_search.best_estimator_\n",
    "  best_param = rand_search.best_params_\n",
    "  best_score = rand_search.best_score_\n",
    "  print(\"    best F1:\", best_score)\n",
    "  print(\"    best param:\", best_param)\n",
    "\n",
    "  # Save the best model\n",
    "  print (\"  save model\")\n",
    "  param_str  = \\\n",
    "      f\"{int(param[0])}-{'to'.join(map(str,param[1]))}-{param[2]}\"\n",
    "\n",
    "  model_name = work_dir / f'model_{txt_flag}_{param_str}.sav'\n",
    "  joblib.dump(best_est, model_name)\n",
    "\n",
    "  # Get testing results: This is not for tuning/selection purpose but because\n",
    "  # X_test is transformed by vectorizer for each parameter combination. If\n",
    "  # the testing set is not evaluated now, things just get too complicated.\n",
    "  # Keep in mind that the testing F1s will not be compared against each other.\n",
    "  print(\"  get testing f1\")\n",
    "  y_pred = best_est.predict(X_test_vec_sel)\n",
    "  test_score = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "  # provide some space between runs\n",
    "  print('\\n')\n",
    "\n",
    "  return best_score, num_select, model_name, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Get training/testing split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read configuration file...\n",
      "   lang_model = w2v\n",
      "   proj_dir = /home/shius/projects/plant_sci_hist\n",
      "   work_dir = 2_text_classify\n",
      "   corpus_combo_file = corpus_combo\n",
      "   rand_state = 20220609\n",
      "   p_threshold = 0.0001\n",
      "   xg_param = {'min_child_weight': [1, 5, 10], 'gamma': [0.5, 1, 1.5, 2, 5], 'subsample': [0.6, 0.8, 1.0], 'colsample_bytree': [0.6, 0.8, 1.0], 'max_depth': [3, 4, 5]}\n",
      "   n_splits = 5\n",
      "   xg_param_comb = 5\n",
      "   n_jobs = 12\n",
      "  all config available\n",
      "\n",
      "Read file and split train/test...\n"
     ]
    }
   ],
   "source": [
    "config_file = Path(\"config_w2v_bert.txt\")\n",
    "\n",
    "print(\"\\nRead configuration file...\")\n",
    "config_dict = read_configs(config_file)\n",
    "\n",
    "# Set up working directory and corpus file location\n",
    "proj_dir          = Path(config_dict['proj_dir'])\n",
    "work_dir          = proj_dir / config_dict['work_dir']\n",
    "corpus_combo_file = work_dir / config_dict['corpus_combo_file']\n",
    "\n",
    "# For reproducibility\n",
    "rand_state = config_dict['rand_state']\n",
    "\n",
    "# Split train/test for original and cleaned text\n",
    "print(\"\\nRead file and split train/test...\")\n",
    "train_ori, test_ori, train_cln, test_cln = split_train_test(\n",
    "                                              corpus_combo_file, rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Word2Vec-based model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Testing with cleaned text_\n",
    "\n",
    "Starting out, I was testing with the original text. When looking into most similar words to \"jasmonate\" there are substantial issue. Top 20 most similar:\n",
    "\n",
    "````Python\n",
    "[('(MeJA)', 0.7294234037399292), ('jasmonic', 0.6400792002677917), ('methyl', 0.638791024684906), ('JA', 0.6380835175514221), ('(JA)', 0.6271078586578369), ('(MeJA),', 0.6264932155609131), ('jasmonate,', 0.6005000472068787), ('(MeJA).', 0.5945847034454346), ('ZIM-domain', 0.5596743226051331), ('jasmonate-', 0.5553220510482788), ('salicylic', 0.5499346256256104), ('JA-isoleucine', 0.5474823713302612), ('12-oxophytodienoic', 0.5435537099838257), ('aminocyclopropane', 0.542698860168457), ('methyl-jasmonate', 0.5406786799430847), ('methyljasmonate', 0.5398091077804565), ('wounding', 0.5363242030143738), ('jasmonate..', 0.5355076789855957), ('acid-isoleucine', 0.531741738319397), ('(MeSA)', 0.5283530950546265)]\n",
    "```\n",
    "\n",
    "So for the next test run repeat and the real run, only cleaned text is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get uni-, bi-, and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series,\n",
       " 853778     update exertional hyponatremia active componen...\n",
       " 1165206    stable megadalton toctic supercomplexes major ...\n",
       " Name: txt_clean, dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit my own Word2Vec model with my corpus\n",
    "corpus = train_cln['txt_clean'] # pandas Series\n",
    "type(corpus), corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['update', 'exertional', 'hyponatremia', 'active', 'component', 'u', 'armed', 'force', '19992012', '1999', '2012', '1333', 'incident', 'diagnosis', 'exertional', 'hyponatremia', 'among', 'active', 'component', 'member']\n"
     ]
    }
   ],
   "source": [
    "## create list of lists of unigrams\n",
    "lst_corpus = []\n",
    "for string in corpus:\n",
    "\n",
    "   # Q: lst_words and lst_grams are the same, what's the point?\n",
    "   lst_words = string.split()\n",
    "   \n",
    "   #lst_grams = [\" \".join(lst_words[i:i+1]) \n",
    "   #            for i in range(0, len(lst_words), 1)]\n",
    "   #lst_corpus.append(lst_grams)\n",
    "\n",
    "   lst_corpus.append(lst_words)\n",
    "print(lst_corpus[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect bigrams and trigrams\n",
    "\n",
    "bigrams_detector  = gensim.models.phrases.Phrases(lst_corpus, \n",
    "                                                  delimiter=\" \", \n",
    "                                                  min_count=5, \n",
    "                                                  threshold=10)\n",
    "bigrams_detector  = gensim.models.phrases.Phraser(bigrams_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the input the trigrams_detector is output of the bigrams_detector\n",
    "trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
    "                                                  delimiter=\" \", \n",
    "                                                  min_count=5, \n",
    "                                                  threshold=10)\n",
    "trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d8b33",
   "metadata": {},
   "source": [
    "#### _Initialize Word2Vec model_\n",
    "\n",
    "- `sequences`: lst_corpus\n",
    "- `vector_size`: dimension of word embeddings\n",
    "- `window`: max distance between the current and the predicted words in a sentence\n",
    "- `min_count`: ignore all words with total frquency lower than this.\n",
    "  - [Discussion on seeting min_count](https://stackoverflow.com/questions/50723303/how-is-word2vec-min-count-applied)\n",
    "- `sg`: history algorithm, 1: skip-gram, otherwise CBOW.\n",
    "\n",
    "Following [this](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.models.Word2Vec(vector_size=300, window=8, \n",
    "                                   min_count=20, sg=1, epochs=30, workers=16,\n",
    "                                   seed=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Vocabulary Table\n",
    "model_w2v.build_vocab(lst_corpus, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train w2v model\n",
    "model_w2v.train(lst_corpus, total_examples=model_w2v.corpus_count, epochs=30,\n",
    "                report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the w2v model\n",
    "#with open(work_dir / \"model_ori_w2v\", \"wb\") as f:\n",
    "#    pickle.dump(model_w2v, f)\n",
    "\n",
    "# Genism build in functionfor pickling\n",
    "model_w2v.save(work_dir / \"model_ori_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the w2v model\n",
    "# Here there is problem with stop words. Like 'jasmonate..', '(MeJA)', and other\n",
    "# variants. So should use the cleaned text.\n",
    "example = \"jasmonate\"\n",
    "print(len(model_w2v.wv[example]))\n",
    "print(model_w2v.wv.most_similar(example, topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79996785",
   "metadata": {},
   "source": [
    "#### _Train tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "                        lower=True, \n",
    "                        split=' ', \n",
    "                        oov_token=\"NaN\", \n",
    "                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "# tokenize corpus \n",
    "tokenizer.fit_on_texts(lst_corpus)\n",
    "\n",
    "# get token dictionary, with token as key, index number as value\n",
    "dic_vocab_token = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f3631",
   "metadata": {},
   "source": [
    "#### _Turn texts into index numbers_\n",
    "\n",
    "Transforms each text in texts to a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b631dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)\n",
    "print(lst_corpus[0][:10])\n",
    "print(lst_text2seq[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index numbers are the values from the token dictionary\n",
    "# Note that these are lowercased\n",
    "dic_vocab_token['update:'], dic_vocab_token['exceptional']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166e656",
   "metadata": {},
   "source": [
    "#### _Check min, max, avg len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT Run ###\n",
    "\n",
    "minlen = 100; maxlen = 0; totlen = 0\n",
    "lst_0  = []   # index of sequences with zero lengths\n",
    "\n",
    "for idx in tqdm(range(len(lst_text2seq))):\n",
    "    slen   = len(lst_text2seq[idx])\n",
    "    totlen +=slen\n",
    "    if slen > maxlen: maxlen = slen\n",
    "    if slen < minlen: \n",
    "        if slen == 0: lst_0.append(idx)\n",
    "        else: minlen = slen\n",
    "print(f'Min:{minlen}, Max:{maxlen}, Avg:{totlen/len(lst_text2seq)}')\n",
    "print('Zero length:', lst_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Pad or trucate sequences_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499096a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = keras.preprocessing.sequence.pad_sequences(\n",
    "                    lst_text2seq,      # List of sequences, each a list of ints \n",
    "                    maxlen=500,         # maximum length of all sequences\n",
    "                    padding=\"post\",    # 'pre' or 'post' \n",
    "                    truncating=\"post\") # remove values from sequences > maxlen\n",
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Create embedding matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "\n",
    "embeddings = np.zeros((len(dic_vocab_token)+1, 300))\n",
    "not_in_emb = {}\n",
    "for word, idx in dic_vocab_token.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  model_w2v.wv[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except KeyError:\n",
    "        not_in_emb[word] = 1\n",
    "\n",
    "not_in_emb # Q: How did this got into the corpus??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Set up ANN_\n",
    "\n",
    "The model contains:\n",
    "- An embedding layer:\n",
    "  - Sequences as input (15 tokens, including padding)\n",
    "  - Word (embedding?) vectors as weights (what??)\n",
    "  - Embedding as output (15x300).\n",
    "- An attention layer\n",
    "  - Capture the eughts of each instance for building an explaniner.\n",
    "  - Not needed for the predictions.\n",
    "- Two layers of bidirectional LSTM.\n",
    "- Two final dense layer to predict probabilities of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13763269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_emb_model(embeddings):\n",
    "\n",
    "    ## code attention layer\n",
    "    def attention_layer(inputs, neurons):\n",
    "        x = layers.Permute((2,1))(inputs)\n",
    "        x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "        x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "        x = layers.multiply([inputs, x])\n",
    "        return x\n",
    "\n",
    "    ## input\n",
    "    x_in = layers.Input(shape=(15,)) ## embedding\n",
    "    x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                        output_dim=embeddings.shape[1], \n",
    "                        weights=[embeddings],\n",
    "                        input_length=15, trainable=False)(x_in)\n",
    "\n",
    "    ## apply attention\n",
    "    x = attention_layer(x, neurons=15)\n",
    "\n",
    "    ## 2 layers of bidirectional lstm\n",
    "    x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n",
    "                            return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)\n",
    "\n",
    "    ## final dense layers\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    y_out = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "    ## Initialize and compile model\n",
    "    model = models.Model(x_in, y_out)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = get_w2v_emb_model(embeddings)\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5796c1",
   "metadata": {},
   "source": [
    "#### _Convert text labels to numeric ones_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7818d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y\n",
    "#  This is the class label, not sure why inverse is done.\n",
    "dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "inverse_dic   = {v:k for k,v in dic_y_mapping.items()}\n",
    "inverse_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397318ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text labels to numeric ones.\n",
    "y_train_label = np.array([inverse_dic[y] for y in y_train])\n",
    "X_train_w2v.shape, len(y_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7c049",
   "metadata": {},
   "source": [
    "#### _Train model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c21389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "history = model_emb.fit(x=X_train_w2v, y=y_train_label, batch_size=256, \n",
    "                        epochs=10, shuffle=True, verbose=1, \n",
    "                        validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a4dc6",
   "metadata": {},
   "source": [
    "#### _Plot loss and accuracy_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_keys = history.history.keys()\n",
    "his_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "df_history_loss = pd.DataFrame(history.history)[['loss','val_loss']]\n",
    "df_history_loss.plot(ax=ax1)\n",
    "df_history_accu = pd.DataFrame(history.history)[['accuracy','val_accuracy']]\n",
    "df_history_accu.plot(ax=ax2)\n",
    "ax1.grid(True); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax2.grid(True); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ec201",
   "metadata": {},
   "source": [
    "#### _Evaluate model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a31ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_w2v = model_emb.predict(X_test_w2v)\n",
    "\n",
    "# q: Why use dic_y_mapping instead of inverse_dic???\n",
    "y_pred_w2v      = [dic_y_mapping[np.argmax(pred)] for pred in y_pred_prob_w2v]\n",
    "\n",
    "eval(y_test, y_pred_w2v, y_pred_prob_w2v, plot_auc=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd3af54f5fe992bccbd23931b262c263c643af7052ca64c3b616d552ec510a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
