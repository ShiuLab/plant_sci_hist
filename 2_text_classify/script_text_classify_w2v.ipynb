{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 2b: Word2Vec and BERT models__\n",
    "\n",
    "Contruct bag of word and Tf-idf models. Hyperparameters include:\n",
    "- Feature extraction\n",
    "  - Cleaned or not in step 1\n",
    "  - `CountVectorizer`\n",
    "    - `max_features`: try 1e4, 2e4, 5e4, 1e5\n",
    "    - `ngram_range`: default [1,1], try also [1,2], [1,3]\n",
    "    - `max_df`: default 1.0, try also 0.9, 0.7, 0.5\n",
    "    - `min_df`: default 1, try also 2, 4, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For reproducibility\n",
    "rand_state = 20220609\n",
    "\n",
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from os import chdir\n",
    "from pathlib import Path\n",
    "\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, feature_selection, metrics\n",
    "from sklearn import model_selection\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def split_train_test(corpus_combo_file, rand_state):\n",
    "    '''Load data and split train test\n",
    "    Args:\n",
    "      corpus_combo_file (str): path to the json data file\n",
    "      rand_state (int): for reproducibility\n",
    "    Return:\n",
    "      train_ori, test_ori, train_cln, test_cln (pandas dataframes): for the\n",
    "        original and clean texts, training and testing splits.\n",
    "    '''\n",
    "    # Load json file\n",
    "    with corpus_combo_file.open(\"r+\") as f:\n",
    "        corpus_combo_json = json.load(f)\n",
    "\n",
    "    # Convert json back to dataframe\n",
    "    corpus_combo = pd.read_json(corpus_combo_json)\n",
    "\n",
    "    corpus_ori = corpus_combo[['label','txt']]\n",
    "    train_ori, test_ori = model_selection.train_test_split(corpus_ori, \n",
    "        test_size=0.2, stratify=corpus_ori['label'], random_state=rand_state)\n",
    "\n",
    "    # Cleaned corpus\n",
    "    corpus_cln = corpus_combo[['label','txt_clean']]\n",
    "    corpus_cln.rename(columns={'txt_clean': 'txt'}) # make col names consistent\n",
    "    train_cln, test_cln = model_selection.train_test_split(corpus_cln, \n",
    "        test_size=0.2, stratify=corpus_cln['label'], random_state=rand_state)\n",
    "\n",
    "    return train_ori, test_ori, train_cln, test_cln\n",
    "\n",
    "def get_hyperparameters():\n",
    "    ''' Return a dictionary with hyperparameters\n",
    "    Return:\n",
    "      param_list (list): a nested list of hyperparameters in the order of\n",
    "        max_feature, ngram_range, and p_threshold\n",
    "    '''\n",
    "   \n",
    "    param_grid = {\"max_features\": [1e4, 5e4, 1e5],\n",
    "                  \"ngram_range\": [(1,1), (1,2), (1,3)]}\n",
    "\n",
    "    param_list = []\n",
    "    p_threshold = 1e-2 \n",
    "    for i in param_grid['max_features']:\n",
    "        for j in param_grid['ngram_range']:\n",
    "            param_list.append([i, j, p_threshold])\n",
    "    \n",
    "    return param_list\n",
    "\n",
    "def extract_feat(X_train, param=[], vocab=\"\"):\n",
    "    '''Extracting features as term frequencies\n",
    "    Args:\n",
    "      X_train (pandas series): the txt column in the training data frame\n",
    "      param (list): contains max_features, ngram_range, stop_words, p_threshold\n",
    "      vocab (list): a list of features to fit.\n",
    "    Returns:\n",
    "      vectorizer (sklearn.feature_extraction.text.CountVectorizer) \n",
    "      X_train (pandas series): the transformed X_train\n",
    "    '''\n",
    "    # vectorizerd term frequencies\n",
    "    if vocab == \"\":\n",
    "      [max_features, ngram_range, _] = param\n",
    "      max_features = int(max_features)\n",
    "      vectorizer = feature_extraction.text.CountVectorizer(\n",
    "                              max_features = max_features, \n",
    "                              ngram_range  = ngram_range)\n",
    "    else:\n",
    "      vectorizer = feature_extraction.text.CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "    # fit the vectorizer with training corpus\n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    # transform the training corpus\n",
    "    X_train_vec = vectorizer.transform(X_train)\n",
    "\n",
    "    return vectorizer, X_train_vec\n",
    "\n",
    "def select_feat(X_train, y_train, vectorizer, p_threshold):\n",
    "    '''Select features based on chi-square test results\n",
    "    Args:\n",
    "      X_train (pandas series): the txt column in the training data frame\n",
    "      y_train (pandas series): the label column in the training data frame\n",
    "      vecorizer: fitted with original X_train and returned from get_vectorizer()\n",
    "      p_threshold (float): p is derived from chi-square test. Features with p <= \n",
    "        p_threshold_s are selected.\n",
    "    Return:\n",
    "      X_names (list): names of selected features\n",
    "    '''\n",
    "    y            = y_train\n",
    "    X_names      = vectorizer.get_feature_names_out()\n",
    "    dtf_features = pd.DataFrame()\n",
    "    for cat in np.unique(y):\n",
    "        _, p = feature_selection.chi2(X_train, y==cat)\n",
    "        dtf_features = pd.concat([dtf_features, \n",
    "                    pd.DataFrame({\"feature\":X_names, \"p\":p, \"y\":cat})])\n",
    "        dtf_features = dtf_features.sort_values(\n",
    "                    [\"y\",\"p\"], ascending=[True,False])\n",
    "        dtf_features = dtf_features[dtf_features[\"p\"] <= p_threshold]\n",
    "    \n",
    "    X_names = dtf_features[\"feature\"].unique().tolist()\n",
    "\n",
    "    return X_names\n",
    "\n",
    "def run_xgboost(X_train, y_train, rand_state):\n",
    "    '''Do hyperparameter tuning and cross-validation of XgBoost models\n",
    "    Args:\n",
    "      X_train (pandas dataframe): features\n",
    "      y_train (pandas series): labels\n",
    "      rand_state (int): rand\n",
    "    Return:\n",
    "      rand_search (RandomizedSearchCV): fitted obj\n",
    "    '''\n",
    "\n",
    "    param_grid = {'min_child_weight': [1, 5, 10],\n",
    "                  'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "                  'subsample': [0.6, 0.8, 1.0],\n",
    "                  'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                  'max_depth': [3, 4, 5]}\n",
    "    folds       = 5\n",
    "    param_comb  = 5\n",
    "    n_jobs      = 12\n",
    "\n",
    "    # Initialize classifier\n",
    "    # 06/11/2022: the silent parameter is deprecated, use verbosity=0\n",
    "    xgb = XGBClassifier(learning_rate=0.02, \n",
    "                        n_estimators=600, \n",
    "                        objective='binary:logistic',\n",
    "                        verbosity=1, \n",
    "                        nthread=1)\n",
    "    # Initilize stratified k fold obj\n",
    "    skf = model_selection.StratifiedKFold(n_splits=folds, \n",
    "                        shuffle = True, random_state = rand_state)\n",
    "    # initiate randomized search CV obj\n",
    "    rand_search = model_selection.RandomizedSearchCV(\n",
    "                        xgb                , param_distributions = param_grid, \n",
    "                        n_iter = param_comb, scoring      = 'f1', \n",
    "                        n_jobs = n_jobs    , cv = skf.split(X_train,y_train), \n",
    "                        verbose = 3        , random_state =rand_state)\n",
    "    # Train\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    return rand_search\n",
    "\n",
    "\n",
    "def run_main_function(work_dir, train, test, txt_flag):\n",
    "\n",
    "    # Get the training/testing corpus and labels\n",
    "    X_train = train['txt']\n",
    "    y_train = train['label']\n",
    "    X_test  = test['txt']\n",
    "    y_test  = test['label']\n",
    "\n",
    "    # get parameter list\n",
    "    param_list  = get_hyperparameters()\n",
    "    \n",
    "    # iterate through different parameters\n",
    "    with open(work_dir / f\"scores_{txt_flag}\", \"w\") as f:\n",
    "        f.write(\"run\\ttxt_flag\\tparameters\\tnum_feat\\tcv_f1\\ttest_f1\\tmodel_name\\n\")\n",
    "        run_num = 0\n",
    "        for param in param_list:\n",
    "            print(f\"\\n#####\\nparam: {param}\")\n",
    "            best_score, num_select, model_name, test_score = run_pipeline(\n",
    "                work_dir, X_train, y_train, X_test, y_test, param, txt_flag)\n",
    "\n",
    "            f.write(f\"{run_num}\\t{txt_flag}\\t{str(param)}\\t{num_select}\\t\"+\\\n",
    "                    f\"{best_score}\\t{test_score}\\t{model_name}\\n\")\n",
    "\n",
    "            run_num += 1\n",
    "\n",
    "\n",
    "def run_pipeline(work_dir, X_train, y_train, X_test, y_test, param, txt_flag):\n",
    "    '''Carry out the major steps'''\n",
    "\n",
    "    # Get vectorizer and fitted X_train\n",
    "    print(\"  extract features by fitting a vectorizer\")\n",
    "    vectorizer, X_train_vec = extract_feat(X_train, param=param)\n",
    "    print(\"    train dim:\", X_train_vec.shape)\n",
    "\n",
    "    # Get selected feature names\n",
    "    print(\"  select features\")\n",
    "    p_threshold = param[-1]\n",
    "    X_names     = select_feat(X_train_vec, y_train, vectorizer, p_threshold)\n",
    "    num_select  = len(X_names)\n",
    "    print('    total selected:', num_select)\n",
    "\n",
    "    # Refit vectorizer with selected features and re-transform X_train\n",
    "    print(\"  refit vectorizer with training data and transform\")\n",
    "    vectorizer_sel, X_train_vec_sel = extract_feat(X_train, vocab=X_names)\n",
    "    print(\"    train dim:\", X_train_vec_sel.shape)\n",
    "\n",
    "    # Also apply the refitted vecorizer to testing data\n",
    "    print(\"  transform testing data\")\n",
    "    X_test_vec_sel = vectorizer_sel.transform(X_test)\n",
    "    print(\"    test dim:\", X_test_vec_sel.shape)\n",
    "\n",
    "    # Get xgboost model and cv results\n",
    "    print(\"  cross-validation and tuning with xgboost\")\n",
    "    rand_search = run_xgboost(X_train_vec_sel, y_train, rand_state)\n",
    "\n",
    "    best_est   = rand_search.best_estimator_\n",
    "    best_param = rand_search.best_params_\n",
    "    best_score = rand_search.best_score_\n",
    "    print(\"    best F1:\", best_score)\n",
    "    print(\"    best param:\", best_param)\n",
    "\n",
    "    # Save the best model\n",
    "    print (\"  save model\")\n",
    "    param_str  = \\\n",
    "        f\"{int(param[0])}-{'to'.join(map(str,param[1]))}-{param[2]}\"\n",
    "\n",
    "    model_name = work_dir / f'model_{txt_flag}_{param_str}.sav'\n",
    "    joblib.dump(best_est, model_name)\n",
    "\n",
    "    # Get testing results: This is not for tuning/selection purpose but because\n",
    "    # X_test is transformed by vectorizer for each parameter combination. If\n",
    "    # the testing set is not evaluated now, things just get too complicated.\n",
    "    # Keep in mind that the testing F1s will not be compared against each other.\n",
    "    print(\"  get testing f1\")\n",
    "    y_pred = best_est.predict(X_test_vec_sel)\n",
    "    test_score = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    # provide some space between runs\n",
    "    print('\\n')\n",
    "\n",
    "    return best_score, num_select, model_name, test_score\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Set up working directory and corpus file location\n",
    "proj_dir          = Path('/home/shius/projects/plant_sci_hist')\n",
    "work_dir          = proj_dir / \"2_text_classify\"\n",
    "corpus_combo_file = work_dir / \"corpus_combo\"\n",
    "\n",
    "# Split train/test for original and cleaned text\n",
    "print(\"\\nRead file and split train/test...\")\n",
    "train_ori, test_ori, train_cln, test_cln = split_train_test(\n",
    "                                                corpus_combo_file, rand_state)\n",
    "\n",
    "print(\"\\nRun main function with original data...\")\n",
    "run_main_function(work_dir, train_ori, test_ori, \"ori\")\n",
    "\n",
    "print(\"\\nRun main function with cleaned data...\")\n",
    "run_main_function(work_dir, train_cln, test_cln, \"cln\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
