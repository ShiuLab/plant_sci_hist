{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 3.3: Cluster analysis__\n",
    "\n",
    "The goals for step 3.3 are to:\n",
    "- Identify \"enriched\" vocab words in each cluster\n",
    "- Determine relations between clusters\n",
    "- Assess the number of citations in each cluster chronologically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.cluster import SpectralClustering, KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting working directory\n",
    "proj_dir   = Path.home() / \"projects/plant_sci_hist\"\n",
    "work_dir   = proj_dir / \"3_key_term_temporal/3_3_cluster_analysis\"\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# plant science corpus\n",
    "dir25       = proj_dir / \"2_text_classify/2_5_predict_pubmed\"\n",
    "corpus_file = dir25 / \"corpus_plant_421658.tsv.gz\"\n",
    "\n",
    "# qualified feature names\n",
    "dir31          = proj_dir / \"3_key_term_temporal/3_1_pubmed_vocab\"\n",
    "X_vec_file     = dir31 / \"tfidf_sparse_matrix_4542\"\n",
    "feat_name_file = dir31 / \"tfidf_feat_name_and_sum_4542\"\n",
    "\n",
    "# fitted clustering objs\n",
    "dir32            = proj_dir / \"3_key_term_temporal/3_2_tf_idf_clustering\"\n",
    "clus_kmeans_file = dir32 / 'clus_kmeans'\n",
    "cluster_num      = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Get clustering results___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kmean fit in case the session died.\n",
    "with open(clus_kmeans_file, \"rb\") as f:\n",
    "  clus_kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (421658,),\n",
       " array([345, 310, 329,  66,   8, 299, 443, 116, 297, 297, 325, 121, 499,\n",
       "        310, 162, 125, 116,  54,  67, 310], dtype=int32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_kmeans = clus_kmeans.labels_\n",
    "type(labels_kmeans), labels_kmeans.shape, labels_kmeans[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {cluster_number: [indices of docs in the cluster]}\n",
    "dict_kmeans = {}\n",
    "for i in range(len(labels_kmeans)):\n",
    "  label = labels_kmeans[i]\n",
    "  if label not in dict_kmeans:\n",
    "    dict_kmeans[label] = [i]\n",
    "  else:\n",
    "    dict_kmeans[label].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_kmeans_size = {}\n",
    "for i in dict_kmeans:\n",
    "  dict_kmeans_size[i] = len(dict_kmeans[i])\n",
    "dict_kmeans_size_df = pd.DataFrame(list(dict_kmeans_size.items()), \n",
    "                                   columns=['cluster', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>249.500000</td>\n",
       "      <td>843.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>633.347277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>124.750000</td>\n",
       "      <td>424.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>249.500000</td>\n",
       "      <td>697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>374.250000</td>\n",
       "      <td>1118.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>7915.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cluster         size\n",
       "count  500.000000   500.000000\n",
       "mean   249.500000   843.316000\n",
       "std    144.481833   633.347277\n",
       "min      0.000000    74.000000\n",
       "25%    124.750000   424.750000\n",
       "50%    249.500000   697.000000\n",
       "75%    374.250000  1118.750000\n",
       "max    499.000000  7915.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_kmeans_size_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___\"Enriched\" vocab words___\n",
    "\n",
    "- Am using Tf-idf values, so not the typically enrichment we are talking about here. More like, for a term, in a cluster X, the Tf-idf distribution is significantly differnt from Tf-idf values of the rest.\n",
    "- So need to get a Tf-idf matrix for each cluster and anotehr for the rest.\n",
    "- Do KS test, but instead of using all values, use max 10,000 values, if subsampling is needed, it is done 10 times. And the median p-value for KS test is used as the test stat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Tf-idf matrix and feat names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((421658, 4542), (4542, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sparse matrix from a pickle\n",
    "with open(X_vec_file, 'rb') as f:\n",
    "  X_vec = pickle.load(f)\n",
    "\n",
    "# Load feature names and tf-idf sum\n",
    "feat_sum = pd.read_csv(feat_name_file, sep='\\t')\n",
    "\n",
    "X_vec.shape, feat_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Tf-idx submatrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submatrices(X_vec, labels, target_label):\n",
    "\n",
    "  target_list  = []\n",
    "  nontar_list  = []\n",
    "\n",
    "  # Populate target and non-target lists with indices\n",
    "  for i in range(len(labels_kmeans)):\n",
    "    label = labels[i]\n",
    "    if label == target_label:\n",
    "      target_list.append(i)\n",
    "    else:\n",
    "      nontar_list.append(i)\n",
    "\n",
    "  # convert to numpy array\n",
    "  target_array = np.array(target_list)\n",
    "  nontar_array = np.array(nontar_list)\n",
    "\n",
    "  # Get the sparse matrix columns based on indices\n",
    "  X_vec_target = X_vec[target_array, :]\n",
    "  X_vec_nontar = X_vec[nontar_array, :]\n",
    "  #print(f\"  target:{X_vec_target.shape}, non-target:{X_vec_nontar.shape}\")\n",
    "\n",
    "  return X_vec_target, X_vec_nontar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For conducting ks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_test(label):\n",
    " \n",
    "  X_vec_target, X_vec_nontar = get_submatrices(X_vec, labels_kmeans, label)\n",
    "  num_feat = X_vec_target.shape[1]\n",
    "\n",
    "  dict_results = {} # {feat_index:[effect size, stat, pval]}\n",
    "  for feat_index in range(num_feat):\n",
    "    target_array = X_vec_target[:, feat_index].toarray().flatten()\n",
    "    nontar_array = X_vec_nontar[:, feat_index].toarray().flatten()\n",
    "\n",
    "    target_median = np.median(target_array)\n",
    "    nontar_median = np.median(nontar_array)\n",
    "    effect_size   = target_median-nontar_median\n",
    "\n",
    "    if target_median > nontar_median:\n",
    "      result = ks_2samp(target_array, nontar_array)\n",
    "      dict_results[feat_index] = [effect_size, result.statistic, result.pvalue]\n",
    "\n",
    "  return dict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test stat for kmean clsuters through parallization\n",
    "\n",
    "- https://superfastpython.com/multiprocessing-pool-apply/\n",
    "- https://clay-atlas.com/us/blog/2021/08/02/python-en-use-multi-processing-pool-progress-bar/\n",
    "- https://bentyeh.github.io/blog/20190722_Python-multiprocessing-progress.html\n",
    "- https://inside-machinelearning.com/en/parallelization-in-python-getting-the-most-out-of-your-cpu/\n",
    "- https://stackoverflow.com/questions/5666576/show-the-progress-of-a-python-multiprocessing-pool-imap-unordered-call\n",
    "\n",
    "Was using `pool.apply` and it did not parallize. Then switch to `pool.imap`. I probably setup apply wrong anyway.\n",
    "\n",
    "```Python\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "  for label in range(cluster_num):\n",
    "    dict_results = pool.apply(ks_test, args=(label,))\n",
    "    dict_results_list.append(dict_results)\n",
    "```\n",
    "\n",
    "If I run as one process, each cluster is ~2min, so 500 cluster is 1000 min which is ~16-17 hours. \n",
    "- With the code below that presumably used 16 parallel processes, this should be done in 1 hours. But it went on for six hours. \n",
    "- Do this in HPC instead and break down the process into 20 jobs each with 25 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through different clusters\n",
    "\n",
    "#dict_results_list = []\n",
    "#with mp.Pool(mp.cpu_count()) as pool:\n",
    "#  for dict_results in tqdm(pool.imap(ks_test, range(cluster_num)), \n",
    "#                           total=cluster_num):\n",
    "#    dict_results_list.append(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the results as a pickle\n",
    "#with open(work_dir / \"dict_results_list_kmeans\", 'wb') as f:\n",
    "#  pickle.dump(dict_results_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate test stat objects\n",
    "\n",
    "The 500 clusters were processed in 20 jobs. Each lead to a dict_result_list that needs to be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26\n",
      "25 26\n",
      "50 26\n",
      "75 26\n",
      "100 26\n",
      "125 26\n",
      "150 26\n",
      "175 26\n",
      "200 26\n",
      "225 26\n",
      "250 26\n",
      "275 26\n",
      "300 26\n",
      "325 26\n",
      "350 26\n",
      "375 26\n",
      "400 26\n",
      "425 26\n",
      "450 26\n",
      "475 26\n"
     ]
    }
   ],
   "source": [
    "dict_results_list_all = []\n",
    "for label in range(0, 500, 25):\n",
    "  dict_results_list_file = work_dir / \\\n",
    "                              f'dict_results_list_kmeans_C{label}-{label+25}'\n",
    "  with open(dict_results_list_file, \"rb\") as f:\n",
    "    dict_results_list = pickle.load(f)\n",
    "    print(label, len(dict_results_list))\n",
    "    dict_results_list_all.extend(dict_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___FOR LATER___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read corpus file with date and journal info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv(corpus_file, sep='\\t', compression='gzip')\n",
    "corpus_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1761d8df3801bd2a70c9560dc6d458568584f11b389930146f99ac99ddb0b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
