{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 4.2: Topic model tuning__\n",
    "\n",
    "BERTopic \n",
    "- [Step-by-step](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)\n",
    "- [Deal with situation where most docs are in the -1 topic](https://github.com/MaartenGr/BERTopic/issues/485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting working directory\n",
    "proj_dir   = Path.home() / \"projects/plant_sci_hist\"\n",
    "work_dir   = proj_dir / \"4_topic_model/4_2_tuning\"\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# plant science corpus\n",
    "dir25       = proj_dir / \"2_text_classify/2_5_predict_pubmed\"\n",
    "corpus_file = dir25 / \"corpus_plant_421658.tsv.gz\"\n",
    "\n",
    "# processed docs\n",
    "dir41            = proj_dir / \"4_topic_model/4_1_get_topics\"\n",
    "docs_clean_file  = dir41 / \"corpus_plant_421658_proc_txt.pkl\"\n",
    "\n",
    "# embedding model\n",
    "emb_model_name = \"allenai/scibert_scivocab_uncased\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Load data and get embeddings___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(docs_clean_file, \"rb\") as f:\n",
    "  docs_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421658,\n",
       " 'identification 120 mus phase decay delayed fluorescence spinach chloroplasts subchloroplast particles intrinsic back reaction . dependence level phase thylakoids internal ph . 500 mus laser flash 120 mus phase decay delayed fluorescence visible variety circumstances spinach chloroplasts subchloroplast particles enriched photosystem ii prepared means digitonin . level phase high case inhibition oxygen evolution donor side photosystem ii . comparison results babcock sauer ( 1975 ) biochim . bio-phys . acta 376 , 329-344 , indicates epr signal iif suppose due z+ , oxidized first secondary donor photosystem ii , well correlated large amplitude 120 mus phase . explain 120 mus phase intrinsic back reaction excited reaction center presence z+ , predicted van gorkom donze ( 1973 ) photochem . photobiol . 17 , 333-342. redox state z+ dependent internal ph thylakoids . results effect ph mus region compared obtained ms region .')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_clean), docs_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get doc embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/shius/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/shius/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "emb_model = SentenceTransformer(emb_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc27bdb60bd64099ade0ec461bc25231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = emb_model.encode(docs_clean, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output embeddings\n",
    "with open(work_dir / \"embeddings_scibert.pickle\", \"wb\") as f:\n",
    "  pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with open(work_dir / \"embeddings_scibert.pickle\", \"rb\") as f:\n",
    "  embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (421658, 768))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Run BERTopic___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN clustering setting\n",
    "min_cluster_size         = 500 \n",
    "metric                   = 'euclidean' \n",
    "cluster_selection_method ='eom' \n",
    "prediction_data          = True \n",
    "min_samples              = 5\n",
    "\n",
    "# BERTopic setting\n",
    "calculate_probabilities = True\n",
    "n_neighbors             = 10  \n",
    "nr_topics               = 500\n",
    "n_gram_range            = (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize HDBSCAN\n",
    "\n",
    "For reducing outliers, following [this instruction](https://maartengr.github.io/BERTopic/faq.html#how-do-i-reduce-topic-outliers)\n",
    "- Also see [HDBSCAN doc](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#what-about-different-metrics)\n",
    "- Comparison of [distance metrics](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                        metric=metric, \n",
    "                        cluster_selection_method=cluster_selection_method, \n",
    "                        prediction_data=prediction_data, \n",
    "                        min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize and train topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(hdbscan_model=hdbscan_model,\n",
    "                       calculate_probabilities=calculate_probabilities,\n",
    "                       n_gram_range=n_gram_range,\n",
    "                       nr_topics=nr_topics,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2022-07-20 15:10:02,980 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 15:14:40,126 - BERTopic - Clustered reduced embeddings\n",
      "2022-07-20 15:21:22,908 - BERTopic - Reduced number of topics from 91 to 91\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(docs_clean,\n",
    "                                          embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model, topics, and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already save the embeddings, so won't save it again\n",
    "topic_model.save(work_dir / 'topic_model')\n",
    "\n",
    "with open(work_dir / 'probs.pickle', \"wb\") as f:\n",
    "  pickle.dump(probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model\n",
    "topic_model = BERTopic.load(work_dir / 'topic_model')\n",
    "\n",
    "# load prob\n",
    "with open(work_dir / 'probs.pickle', \"rb\") as f:\n",
    "  probs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BERTopic in module bertopic._bertopic object:\n",
      "\n",
      "class BERTopic(builtins.object)\n",
      " |  BERTopic(language: str = 'english', top_n_words: int = 10, n_gram_range: Tuple[int, int] = (1, 1), min_topic_size: int = 10, nr_topics: Union[int, str] = None, low_memory: bool = False, calculate_probabilities: bool = False, diversity: float = None, seed_topic_list: List[List[str]] = None, embedding_model=None, umap_model: umap.umap_.UMAP = None, hdbscan_model: hdbscan.hdbscan_.HDBSCAN = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, verbose: bool = False)\n",
      " |  \n",
      " |  BERTopic is a topic modeling technique that leverages BERT embeddings and\n",
      " |  c-TF-IDF to create dense clusters allowing for easily interpretable topics\n",
      " |  whilst keeping important words in the topic descriptions.\n",
      " |  \n",
      " |  The default embedding model is `all-MiniLM-L6-v2` when selecting `language=\"english\"` \n",
      " |  and `paraphrase-multilingual-MiniLM-L12-v2` when selecting `language=\"multilingual\"`.\n",
      " |  \n",
      " |  Usage:\n",
      " |  \n",
      " |  ```python\n",
      " |  from bertopic import BERTopic\n",
      " |  from sklearn.datasets import fetch_20newsgroups\n",
      " |  \n",
      " |  docs = fetch_20newsgroups(subset='all')['data']\n",
      " |  topic_model = BERTopic()\n",
      " |  topics, probabilities = topic_model.fit_transform(docs)\n",
      " |  ```\n",
      " |  \n",
      " |  If you want to use your own embedding model, use it as follows:\n",
      " |  \n",
      " |  ```python\n",
      " |  from bertopic import BERTopic\n",
      " |  from sklearn.datasets import fetch_20newsgroups\n",
      " |  from sentence_transformers import SentenceTransformer\n",
      " |  \n",
      " |  docs = fetch_20newsgroups(subset='all')['data']\n",
      " |  sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |  topic_model = BERTopic(embedding_model=sentence_model)\n",
      " |  ```\n",
      " |  \n",
      " |  Due to the stochastisch nature of UMAP, the results from BERTopic might differ\n",
      " |  and the quality can degrade. Using your own embeddings allows you to\n",
      " |  try out BERTopic several times until you find the topics that suit\n",
      " |  you best.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, language: str = 'english', top_n_words: int = 10, n_gram_range: Tuple[int, int] = (1, 1), min_topic_size: int = 10, nr_topics: Union[int, str] = None, low_memory: bool = False, calculate_probabilities: bool = False, diversity: float = None, seed_topic_list: List[List[str]] = None, embedding_model=None, umap_model: umap.umap_.UMAP = None, hdbscan_model: hdbscan.hdbscan_.HDBSCAN = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, verbose: bool = False)\n",
      " |      BERTopic initialization\n",
      " |      \n",
      " |      Arguments:\n",
      " |          language: The main language used in your documents. The default sentence-transformers \n",
      " |                    model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of\n",
      " |                    supported languages see bertopic.backend.languages. Select\n",
      " |                    \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2`\n",
      " |                    sentence-tranformers model that supports 50+ languages.\n",
      " |          top_n_words: The number of words per topic to extract. Setting this\n",
      " |                       too high can negatively impact topic embeddings as topics\n",
      " |                       are typically best represented by at most 10 words.\n",
      " |          n_gram_range: The n-gram range for the CountVectorizer.\n",
      " |                        Advised to keep high values between 1 and 3.\n",
      " |                        More would likely lead to memory issues.\n",
      " |                        NOTE: This param will not be used if you pass in your own\n",
      " |                        CountVectorizer.\n",
      " |          min_topic_size: The minimum size of the topic. Increasing this value will lead\n",
      " |                          to a lower number of clusters/topics.\n",
      " |          nr_topics: Specifying the number of topics will reduce the initial\n",
      " |                     number of topics to the value specified. This reduction can take\n",
      " |                     a while as each reduction in topics (-1) activates a c-TF-IDF\n",
      " |                     calculation. If this is set to None, no reduction is applied. Use\n",
      " |                     \"auto\" to automatically reduce topics using HDBSCAN.\n",
      " |          low_memory: Sets UMAP low memory to True to make sure less memory is used.\n",
      " |                      NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP\n",
      " |                      this parameter will not be used.\n",
      " |          calculate_probabilities: Whether to calculate the probabilities of all topics\n",
      " |                                   per document instead of the probability of the assigned\n",
      " |                                   topic per document. This could slow down the extraction\n",
      " |                                   of topics if you have many documents (> 100_000). Set this\n",
      " |                                   only to True if you have a low amount of documents or if\n",
      " |                                   you do not mind more computation time.\n",
      " |                                   NOTE: If false you cannot use the corresponding\n",
      " |                                   visualization method `visualize_probabilities`.\n",
      " |          diversity: Whether to use MMR to diversify the resulting topic representations.\n",
      " |                     If set to None, MMR will not be used. Accepted values lie between\n",
      " |                     0 and 1 with 0 being not at all diverse and 1 being very diverse.\n",
      " |          seed_topic_list: A list of seed words per topic to converge around\n",
      " |          verbose: Changes the verbosity of the model, Set to True if you want\n",
      " |                   to track the stages of the model.\n",
      " |          embedding_model: Use a custom embedding model.\n",
      " |                           The following backends are currently supported\n",
      " |                             * SentenceTransformers\n",
      " |                             * Flair\n",
      " |                             * Spacy\n",
      " |                             * Gensim\n",
      " |                             * USE (TF-Hub)\n",
      " |                           You can also pass in a string that points to one of the following\n",
      " |                           sentence-transformers models:\n",
      " |                             * https://www.sbert.net/docs/pretrained_models.html\n",
      " |          umap_model: Pass in a UMAP model to be used instead of the default.\n",
      " |                      NOTE: You can also pass in any dimensionality reduction algorithm as long\n",
      " |                      as it has `.fit` and `.transform` functions.\n",
      " |          hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default\n",
      " |                         NOTE: You can also pass in any clustering algorithm as long as it has\n",
      " |                         `.fit` and `.predict` functions along with the `.labels_` variable.\n",
      " |          vectorizer_model: Pass in a CountVectorizer instead of the default\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: Human readable representation of the most important model parameters.\n",
      " |               The parameters that represent models are ignored due to their\n",
      " |  \n",
      " |  find_topics(self, search_term: str, top_n: int = 5) -> Tuple[List[int], List[float]]\n",
      " |      Find topics most similar to a search_term\n",
      " |      \n",
      " |      Creates an embedding for search_term and compares that with\n",
      " |      the topic embeddings. The most similar topics are returned\n",
      " |      along with their similarity values.\n",
      " |      \n",
      " |      The search_term can be of any size but since it compares\n",
      " |      with the topic representation it is advised to keep it\n",
      " |      below 5 words.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          search_term: the term you want to use to search for topics\n",
      " |          top_n: the number of topics to return\n",
      " |      \n",
      " |      Returns:\n",
      " |          similar_topics: the most similar topics from high to low\n",
      " |          similarity: the similarity scores from high to low\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      You can use the underlying embedding model to find topics that\n",
      " |      best represent the search term:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics, similarity = topic_model.find_topics(\"sports\", top_n=5)\n",
      " |      ```\n",
      " |      \n",
      " |      Note that the search query is typically more accurate if the\n",
      " |      search_term consists of a phrase or multiple words.\n",
      " |  \n",
      " |  fit(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None)\n",
      " |      Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic().fit(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings, use it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  fit_transform(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None) -> Tuple[List[int], Optional[numpy.ndarray]]\n",
      " |      Fit the models on a collection of documents, generate topics, and return the docs with topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Returns:\n",
      " |          predictions: Topic predictions for each documents\n",
      " |          probabilities: The probability of the assigned topic per document.\n",
      " |                         If `calculate_probabilities` in BERTopic is set to True, then\n",
      " |                         it calculates the probabilities of all topics across all documents\n",
      " |                         instead of only the assigned topic. This, however, slows down\n",
      " |                         computation and may increase memory usage.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings, use it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  generate_topic_labels(self, nr_words: int = 3, topic_prefix: bool = True, word_length: int = None, separator: str = '_') -> List[str]\n",
      " |      Get labels for each topic in a user-defined format\n",
      " |      \n",
      " |      Arguments:\n",
      " |          original_labels:\n",
      " |          nr_words: Top `n` words per topic to use\n",
      " |          topic_prefix: Whether to use the topic ID as a prefix.\n",
      " |                      If set to True, the topic ID will be separated\n",
      " |                      using the `separator`\n",
      " |          word_length: The maximum length of each word in the topic label.\n",
      " |                      Some words might be relatively long and setting this\n",
      " |                      value helps to make sure that all labels have relatively\n",
      " |                      similar lengths.\n",
      " |          separator: The string with which the words and topic prefix will be\n",
      " |                  separated. Underscores are the default but a nice alternative\n",
      " |                  is `\", \"`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic_labels: A list of topic labels sorted from the lowest topic ID to the highest.\n",
      " |                      If the topic model was trained using HDBSCAN, the lowest topic ID is -1,\n",
      " |                      otherwise it is 0.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To create our custom topic labels, usage is rather straightforward:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \")\n",
      " |      ```\n",
      " |  \n",
      " |  get_params(self, deep: bool = False) -> Mapping[str, Any]\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Adapted from:\n",
      " |          https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178\n",
      " |      \n",
      " |      Arguments:\n",
      " |          deep: bool, default=True\n",
      " |                If True, will return the parameters for this estimator and\n",
      " |                contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          out: Parameter names mapped to their values.\n",
      " |  \n",
      " |  get_representative_docs(self, topic: int = None) -> List[str]\n",
      " |      Extract representative documents per topic\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want\n",
      " |                 the representative documents\n",
      " |      \n",
      " |      Returns:\n",
      " |          Representative documents of the chosen topic\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To extract the representative docs of all topics:\n",
      " |      \n",
      " |      ```python\n",
      " |      representative_docs = topic_model.get_representative_docs()\n",
      " |      ```\n",
      " |      \n",
      " |      To get the representative docs of a single topic:\n",
      " |      \n",
      " |      ```python\n",
      " |      representative_docs = topic_model.get_representative_docs(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic(self, topic: int) -> Union[Mapping[str, Tuple[str, float]], bool]\n",
      " |      Return top n words for a specific topic and their c-TF-IDF scores\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want its representation\n",
      " |      \n",
      " |      Returns:\n",
      " |          The top n words for a specific word and its respective c-TF-IDF scores\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic = topic_model.get_topic(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic_freq(self, topic: int = None) -> Union[pandas.core.frame.DataFrame, int]\n",
      " |      Return the the size of topics (descending order)\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want the frequency\n",
      " |      \n",
      " |      Returns:\n",
      " |          Either the frequency of a single topic or dataframe with\n",
      " |          the frequencies of all topics\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To extract the frequency of all topics:\n",
      " |      \n",
      " |      ```python\n",
      " |      frequency = topic_model.get_topic_freq()\n",
      " |      ```\n",
      " |      \n",
      " |      To get the frequency of a single topic:\n",
      " |      \n",
      " |      ```python\n",
      " |      frequency = topic_model.get_topic_freq(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic_info(self, topic: int = None) -> pandas.core.frame.DataFrame\n",
      " |      Get information about each topic including its ID, frequency, and name.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want the frequency\n",
      " |      \n",
      " |      Returns:\n",
      " |          info: The information relating to either a single topic or all topics\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      info_df = topic_model.get_topic_info()\n",
      " |      ```\n",
      " |  \n",
      " |  get_topics(self) -> Mapping[str, Tuple[str, float]]\n",
      " |      Return topics with top n words and their c-TF-IDF score\n",
      " |      \n",
      " |      Returns:\n",
      " |          self.topic: The top n words per topic and the corresponding c-TF-IDF score\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      all_topics = topic_model.get_topics()\n",
      " |      ```\n",
      " |  \n",
      " |  hierarchical_topics(self, docs: List[int], topics: List[int], linkage_function: Callable[[scipy.sparse.csr.csr_matrix], numpy.ndarray] = None, distance_function: Callable[[scipy.sparse.csr.csr_matrix], scipy.sparse.csr.csr_matrix] = None) -> pandas.core.frame.DataFrame\n",
      " |      Create a hierarchy of topics\n",
      " |      \n",
      " |      To create this hierarchy, BERTopic needs to be already fitted once.\n",
      " |      Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF\n",
      " |      representation using `scipy.cluster.hierarchy.linkage`.\n",
      " |      \n",
      " |      Based on that hierarchy, we calculate the topic representation at each\n",
      " |      merged step. This is a local representation, as we only assume that the\n",
      " |      chosen step is merged and not all others which typically improves the\n",
      " |      topic representation.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          linkage_function: The linkage function to use. Default is:\n",
      " |                          `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\n",
      " |          distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\n",
      " |                              `lambda x: 1 - cosine_similarity(x)`\n",
      " |      \n",
      " |      Returns:\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      ```\n",
      " |      \n",
      " |      A custom linkage function can be used as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from scipy.cluster import hierarchy as sch\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      \n",
      " |      # Hierarchical topics\n",
      " |      linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics, linkage_function=linkage_function)\n",
      " |      ```\n",
      " |  \n",
      " |  merge_topics(self, docs: List[str], topics: List[int], topics_to_merge: List[Union[Iterable[int], int]]) -> None\n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          topics_to_merge: Either a list of topics or a list of list of topics\n",
      " |                          to merge. For example:\n",
      " |                              [1, 2, 3] will merge topics 1, 2 and 3\n",
      " |                              [[1, 2], [3, 4]] will merge topics 1 and 2, and\n",
      " |                              separately merge topics 3 and 4.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      If you want to merge topics 1, 2, and 3:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_to_merge = [1, 2, 3]\n",
      " |      topic_model.merge_topics(docs, topics, topics_to_merge)\n",
      " |      ```\n",
      " |      \n",
      " |      or if you want to merge topics 1 and 2, and separately\n",
      " |      merge topics 3 and 4:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_to_merge = [[1, 2]\n",
      " |                          [3, 4]]\n",
      " |      topic_model.merge_topics(docs, topics, topics_to_merge)\n",
      " |      ```\n",
      " |  \n",
      " |  reduce_topics(self, docs: List[str], topics: List[int], probabilities: numpy.ndarray = None, nr_topics: int = 20) -> Tuple[List[int], numpy.ndarray]\n",
      " |      Further reduce the number of topics to nr_topics.\n",
      " |      \n",
      " |      The number of topics is further reduced by calculating the c-TF-IDF matrix\n",
      " |      of the documents and then reducing them by iteratively merging the least\n",
      " |      frequent topic with the most similar one based on their c-TF-IDF matrices.\n",
      " |      The topics, their sizes, and representations are updated.\n",
      " |      \n",
      " |      The reasoning for putting `docs`, `topics`, and `probs` as parameters is that\n",
      " |      these values are not saved within BERTopic on purpose. If you were to have a\n",
      " |      million documents, it seems very inefficient to save those in BERTopic\n",
      " |      instead of a dedicated database.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The docs you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`\n",
      " |          nr_topics: The number of topics you want reduced to\n",
      " |      \n",
      " |      Returns:\n",
      " |          new_topics: Updated topics\n",
      " |          new_probabilities: Updated probabilities\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      You can further reduce the topics by passing the documents with its\n",
      " |      topics and probabilities (if they were calculated):\n",
      " |      \n",
      " |      ```python\n",
      " |      new_topics, new_probs = topic_model.reduce_topics(docs, topics, probabilities, nr_topics=30)\n",
      " |      ```\n",
      " |      \n",
      " |      If probabilities were not calculated simply run the function without them:\n",
      " |      \n",
      " |      ```python\n",
      " |      new_topics, new_probs = topic_model.reduce_topics(docs, topics, nr_topics=30)\n",
      " |      ```\n",
      " |  \n",
      " |  save(self, path: str, save_embedding_model: bool = True) -> None\n",
      " |      Saves the model to the specified path\n",
      " |      \n",
      " |      Arguments:\n",
      " |          path: the location and name of the file you want to save\n",
      " |          save_embedding_model: Whether to save the embedding model in this class\n",
      " |                                as you might have selected a local model or one that\n",
      " |                                is downloaded automatically from the cloud.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.save(\"my_model\")\n",
      " |      ```\n",
      " |      \n",
      " |      or if you do not want the embedding_model to be saved locally:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.save(\"my_model\", save_embedding_model=False)\n",
      " |      ```\n",
      " |  \n",
      " |  set_topic_labels(self, topic_labels: Union[List[str], Mapping[int, str]]) -> None\n",
      " |      Set custom topic labels in your fitted BERTopic model\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_labels: If a list of topic labels, it should contain the same number\n",
      " |                      of labels as there are topics. This must be ordered\n",
      " |                      from the topic with the lowest ID to the highest ID,\n",
      " |                      including topic -1 if it exists.\n",
      " |                      If a dictionary of `topic ID`: `topic_label`, it can have\n",
      " |                      any number of topics as it will only map the topics found\n",
      " |                      in the dictionary.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      First, we define our topic labels with `.get_topic_labels` in which\n",
      " |      we can customize our topic labels:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_labels = topic_model.get_topic_labels(nr_words=2,\n",
      " |                                                  topic_prefix=True,\n",
      " |                                                  word_length=10,\n",
      " |                                                  separator=\", \")\n",
      " |      ```\n",
      " |      \n",
      " |      Then, we pass these `topic_labels` to our topic model which\n",
      " |      can be accessed at any time with `.custom_labels`:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.set_topic_labels(topic_labels)\n",
      " |      topic_model.custom_labels\n",
      " |      ```\n",
      " |      \n",
      " |      You might want to change only a few topic labels instead of all of them.\n",
      " |      To do so, you can pass a dictionary where the keys are the topic IDs and\n",
      " |      its keys the topic labels:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"})\n",
      " |      topic_model.custom_labels\n",
      " |      ```\n",
      " |  \n",
      " |  topics_over_time(self, docs: List[str], topics: List[int], timestamps: Union[List[str], List[int]], nr_bins: int = None, datetime_format: str = None, evolution_tuning: bool = True, global_tuning: bool = True) -> pandas.core.frame.DataFrame\n",
      " |      Create topics over time\n",
      " |      \n",
      " |      To create the topics over time, BERTopic needs to be already fitted once.\n",
      " |      From the fitted models, the c-TF-IDF representations are calculate at\n",
      " |      each timestamp t. Then, the c-TF-IDF representations at timestamp t are\n",
      " |      averaged with the global c-TF-IDF representations in order to fine-tune the\n",
      " |      local representations.\n",
      " |      \n",
      " |      NOTE:\n",
      " |          Make sure to use a limited number of unique timestamps (<100) as the\n",
      " |          c-TF-IDF representation will be calculated at each single unique timestamp.\n",
      " |          Having a large number of unique timestamps can take some time to be calculated.\n",
      " |          Moreover, there aren't many use-cased where you would like to see the difference\n",
      " |          in topic representations over more than 100 different timestamps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          timestamps: The timestamp of each document. This can be either a list of strings or ints.\n",
      " |                      If it is a list of strings, then the datetime format will be automatically\n",
      " |                      inferred. If it is a list of ints, then the documents will be ordered by\n",
      " |                      ascending order.\n",
      " |          nr_bins: The number of bins you want to create for the timestamps. The left interval will\n",
      " |                   be chosen as the timestamp. An additional column will be created with the\n",
      " |                   entire interval.\n",
      " |          datetime_format: The datetime format of the timestamps if they are strings, eg “%d/%m/%Y”.\n",
      " |                           Set this to None if you want to have it automatically detect the format.\n",
      " |                           See strftime documentation for more information on choices:\n",
      " |                           https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
      " |          evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its\n",
      " |                            c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates\n",
      " |                            evolutionary topic representations.\n",
      " |          global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix\n",
      " |                     with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
      " |                     topic representations that could not be found in the documents at timestamp t.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topics_over_time: A dataframe that contains the topic, words, and frequency of topic\n",
      " |                            at timestamp t.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      The timestamps variable represent the timestamp of each document. If you have over\n",
      " |      100 unique timestamps, it is advised to bin the timestamps as shown below:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20)\n",
      " |      ```\n",
      " |  \n",
      " |  topics_per_class(self, docs: List[str], topics: List[int], classes: Union[List[int], List[str]], global_tuning: bool = True) -> pandas.core.frame.DataFrame\n",
      " |      Create topics per class\n",
      " |      \n",
      " |      To create the topics per class, BERTopic needs to be already fitted once.\n",
      " |      From the fitted models, the c-TF-IDF representations are calculate at\n",
      " |      each class c. Then, the c-TF-IDF representations at class c are\n",
      " |      averaged with the global c-TF-IDF representations in order to fine-tune the\n",
      " |      local representations. This can be turned off if the pure representation is\n",
      " |      needed.\n",
      " |      \n",
      " |      NOTE:\n",
      " |          Make sure to use a limited number of unique classes (<100) as the\n",
      " |          c-TF-IDF representation will be calculated at each single unique class.\n",
      " |          Having a large number of unique classes can take some time to be calculated.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          classes: The class of each document. This can be either a list of strings or ints.\n",
      " |          global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix\n",
      " |                     with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
      " |                     topic representations that could not be found in the documents at timestamp t.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topics_per_class: A dataframe that contains the topic, words, and frequency of topics\n",
      " |                            for each class.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      topics_per_class = topic_model.topics_per_class(docs, topics, classes)\n",
      " |      ```\n",
      " |  \n",
      " |  transform(self, documents: Union[str, List[str]], embeddings: numpy.ndarray = None) -> Tuple[List[int], numpy.ndarray]\n",
      " |      After having fit a model, use transform to predict new instances\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A single document or a list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          predictions: Topic predictions for each documents\n",
      " |          probabilities: The topic probability distribution which is returned by default.\n",
      " |                         If `calculate_probabilities` in BERTopic is set to False, then the\n",
      " |                         probabilities are not calculated to speed up computation and\n",
      " |                         decrease memory usage.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic().fit(docs)\n",
      " |      topics, probs = topic_model.transform(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      topics, probs = topic_model.transform(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  update_topics(self, docs: List[str], topics: List[int], n_gram_range: Tuple[int, int] = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None)\n",
      " |      Updates the topic representation by recalculating c-TF-IDF with the new\n",
      " |      parameters as defined in this function.\n",
      " |      \n",
      " |      When you have trained a model and viewed the topics and the words that represent them,\n",
      " |      you might not be satisfied with the representation. Perhaps you forgot to remove\n",
      " |      stop_words or you want to try out a different n_gram_range. This function allows you\n",
      " |      to update the topic representation after they have been formed.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          n_gram_range: The n-gram range for the CountVectorizer.\n",
      " |          vectorizer_model: Pass in your own CountVectorizer from scikit-learn\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      In order to update the topic representation, you will need to first fit the topic\n",
      " |      model and extract topics from them. Based on these, you can update the representation:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.update_topics(docs, topics, n_gram_range=(2, 3))\n",
      " |      ```\n",
      " |      \n",
      " |      YOu can also use a custom vectorizer to update the representation:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.feature_extraction.text import CountVectorizer\n",
      " |      vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
      " |      topic_model.update_topics(docs, topics, vectorizer_model=vectorizer_model)\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_barchart(self, topics: List[int] = None, top_n_topics: int = 8, n_words: int = 5, width: int = 250, height: int = 250) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a barchart of selected topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize.\n",
      " |          top_n_topics: Only select the top n most frequent topics.\n",
      " |          n_words: Number of words to show in a topic\n",
      " |          width: The width of each figure.\n",
      " |          height: The height of each figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the barchart of selected topics\n",
      " |      simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_barchart()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_barchart()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_distribution(self, probabilities: numpy.ndarray, min_probability: float = 0.015, custom_labels: bool = False, width: int = 800, height: int = 600) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize the distribution of topic probabilities\n",
      " |      \n",
      " |      Arguments:\n",
      " |          probabilities: An array of probability scores\n",
      " |          min_probability: The minimum probability score to visualize.\n",
      " |                           All others are ignored.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                         `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      Make sure to fit the model before and only input the\n",
      " |      probabilities of a single document:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_distribution(probabilities[0])\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_distribution(probabilities[0])\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_documents(self, docs: List[str], topics: List[int] = None, embeddings: numpy.ndarray = None, reduced_embeddings: numpy.ndarray = None, sample: float = None, hide_annotations: bool = False, hide_document_hover: bool = False, custom_labels: bool = False, width: int = 1200, height: int = 750) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize documents and their topics in 2D\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_model: A fitted BERTopic instance.\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: A selection of topics to visualize.\n",
      " |                  Not to be confused with the topics that you get from `.fit_transform`.\n",
      " |                  For example, if you want to visualize only topics 1 through 5:\n",
      " |                  `topics = [1, 2, 3, 4, 5]`.\n",
      " |          embeddings: The embeddings of all documents in `docs`.\n",
      " |          reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
      " |          sample: The percentage of documents in each topic that you would like to keep.\n",
      " |                  Value can be between 0 and 1. Setting this value to, for example,\n",
      " |                  0.1 (10% of documents in each topic) makes it easier to visualize\n",
      " |                  millions of documents as a subset is chosen.\n",
      " |          hide_annotations: Hide the names of the traces on top of each cluster.\n",
      " |          hide_document_hover: Hide the content of the documents when hovering over\n",
      " |                              specific points. Helps to speed up generation of visualization.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_documents(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      Do note that this re-calculates the embeddings and reduces them to 2D.\n",
      " |      The advised and prefered pipeline for using this function is as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      from bertopic import BERTopic\n",
      " |      from umap import UMAP\n",
      " |      \n",
      " |      # Prepare embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
      " |      \n",
      " |      # Train BERTopic\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      \n",
      " |      # Reduce dimensionality of embeddings, this step is optional\n",
      " |      # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
      " |      \n",
      " |      # Run the visualization with the original embeddings\n",
      " |      topic_model.visualize_documents(docs, embeddings=embeddings)\n",
      " |      \n",
      " |      # Or, if you have reduced the original embeddings already:\n",
      " |      topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      <iframe src=\"../../getting_started/visualization/documents.html\"\n",
      " |      style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_heatmap(self, topics: List[int] = None, top_n_topics: int = None, n_clusters: int = None, custom_labels: bool = False, width: int = 800, height: int = 800) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a heatmap of the topic's similarity matrix\n",
      " |      \n",
      " |      Based on the cosine similarity matrix between topic embeddings,\n",
      " |      a heatmap is created showing the similarity between topics.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize.\n",
      " |          top_n_topics: Only select the top n most frequent topics.\n",
      " |          n_clusters: Create n clusters and order the similarity\n",
      " |                      matrix by those clusters.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the similarity matrix of\n",
      " |      topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_heatmap()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_heatmap()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_hierarchical_documents(self, docs: List[str], hierarchical_topics: pandas.core.frame.DataFrame, topics: List[int] = None, embeddings: numpy.ndarray = None, reduced_embeddings: numpy.ndarray = None, sample: Union[float, int] = None, hide_annotations: bool = False, hide_document_hover: bool = True, nr_levels: int = 10, custom_labels: bool = False, width: int = 1200, height: int = 750) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize documents and their topics in 2D at different levels of hierarchy\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children\n",
      " |          topics: A selection of topics to visualize.\n",
      " |                  Not to be confused with the topics that you get from `.fit_transform`.\n",
      " |                  For example, if you want to visualize only topics 1 through 5:\n",
      " |                  `topics = [1, 2, 3, 4, 5]`.\n",
      " |          embeddings: The embeddings of all documents in `docs`.\n",
      " |          reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
      " |          sample: The percentage of documents in each topic that you would like to keep.\n",
      " |                  Value can be between 0 and 1. Setting this value to, for example,\n",
      " |                  0.1 (10% of documents in each topic) makes it easier to visualize\n",
      " |                  millions of documents as a subset is chosen.\n",
      " |          hide_annotations: Hide the names of the traces on top of each cluster.\n",
      " |          hide_document_hover: Hide the content of the documents when hovering over\n",
      " |                              specific points. Helps to speed up generation of visualizations.\n",
      " |          nr_levels: The number of levels to be visualized in the hierarchy. First, the distances\n",
      " |                  in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with\n",
      " |                  equal length. Then, for each list of distances, the merged topics are selected that\n",
      " |                  have a distance less or equal to the maximum distance of the selected list of distances.\n",
      " |                  NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to\n",
      " |                  the length of `hierarchical_topics`.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                         `topic_model.set_topic_labels`.\n",
      " |                         NOTE: Custom labels are only generated for the original\n",
      " |                         un-merged topics.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics)\n",
      " |      ```\n",
      " |      \n",
      " |      Do note that this re-calculates the embeddings and reduces them to 2D.\n",
      " |      The advised and prefered pipeline for using this function is as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      from bertopic import BERTopic\n",
      " |      from umap import UMAP\n",
      " |      \n",
      " |      # Prepare embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
      " |      \n",
      " |      # Train BERTopic and extract hierarchical topics\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      \n",
      " |      # Reduce dimensionality of embeddings, this step is optional\n",
      " |      # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
      " |      \n",
      " |      # Run the visualization with the original embeddings\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings)\n",
      " |      \n",
      " |      # Or, if you have reduced the original embeddings already:\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\"\n",
      " |      style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_hierarchy(self, orientation: str = 'left', topics: List[int] = None, top_n_topics: int = None, custom_labels: bool = False, width: int = 1000, height: int = 600, hierarchical_topics: pandas.core.frame.DataFrame = None, linkage_function: Callable[[scipy.sparse.csr.csr_matrix], numpy.ndarray] = None, distance_function: Callable[[scipy.sparse.csr.csr_matrix], scipy.sparse.csr.csr_matrix] = None, color_threshold: int = 1) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a hierarchical structure of the topics\n",
      " |      \n",
      " |      A ward linkage function is used to perform the\n",
      " |      hierarchical clustering based on the cosine distance\n",
      " |      matrix between topic embeddings.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_model: A fitted BERTopic instance.\n",
      " |          orientation: The orientation of the figure.\n",
      " |                      Either 'left' or 'bottom'\n",
      " |          topics: A selection of topics to visualize\n",
      " |          top_n_topics: Only select the top n most frequent topics\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |                     NOTE: Custom labels are only generated for the original\n",
      " |                     un-merged topics.\n",
      " |          width: The width of the figure. Only works if orientation is set to 'left'\n",
      " |          height: The height of the figure. Only works if orientation is set to 'bottom'\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children.\n",
      " |                              NOTE: The hierarchical topic names are only visualized\n",
      " |                              if both `topics` and `top_n_topics` are not set.\n",
      " |          linkage_function: The linkage function to use. Default is:\n",
      " |                          `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\n",
      " |                          NOTE: Make sure to use the same `linkage_function` as used\n",
      " |                          in `topic_model.hierarchical_topics`.\n",
      " |          distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\n",
      " |                          `lambda x: 1 - cosine_similarity(x)`\n",
      " |                          NOTE: Make sure to use the same `distance_function` as used\n",
      " |                          in `topic_model.hierarchical_topics`.\n",
      " |          color_threshold: Value at which the separation of clusters will be made which\n",
      " |                       will result in different colors for different clusters.\n",
      " |                       A higher value will typically lead in less colored clusters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the hierarchical structure of\n",
      " |      topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_hierarchy()\n",
      " |      ```\n",
      " |      \n",
      " |      If you also want the labels visualized of hierarchical topics,\n",
      " |      run the following:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Extract hierarchical topics and their representations\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      \n",
      " |      # Visualize these representations\n",
      " |      topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_hierarchy()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      <iframe src=\"../../getting_started/visualization/hierarchy.html\"\n",
      " |      style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_term_rank(self, topics: List[int] = None, log_scale: bool = False, custom_labels: bool = False, width: int = 800, height: int = 500) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize the ranks of all terms across all topics\n",
      " |      \n",
      " |      Each topic is represented by a set of words. These words, however,\n",
      " |      do not all equally represent the topic. This visualization shows\n",
      " |      how many words are needed to represent a topic and at which point\n",
      " |      the beneficial effect of adding words starts to decline.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize. These will be colored\n",
      " |                  red where all others will be colored black.\n",
      " |          log_scale: Whether to represent the ranking on a log scale\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the ranks of all words across\n",
      " |      all topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_term_rank()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_term_rank()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      Reference:\n",
      " |      \n",
      " |      This visualization was heavily inspired by the\n",
      " |      \"Term Probability Decline\" visualization found in an\n",
      " |      analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/).\n",
      " |      Reference to that specific analysis can be found\n",
      " |      [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html).\n",
      " |  \n",
      " |  visualize_topics(self, topics: List[int] = None, top_n_topics: int = None, width: int = 650, height: int = 650) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics, their sizes, and their corresponding words\n",
      " |      \n",
      " |      This visualization is highly inspired by LDAvis, a great visualization\n",
      " |      technique typically reserved for LDA.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize\n",
      " |          top_n_topics: Only select the top n most frequent topics\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_topics()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_topics_over_time(self, topics_over_time: pandas.core.frame.DataFrame, top_n_topics: int = None, topics: List[int] = None, normalize_frequency: bool = False, custom_labels: bool = False, width: int = 1250, height: int = 450) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics over time\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics_over_time: The topics you would like to be visualized with the\n",
      " |                            corresponding topic representation\n",
      " |          top_n_topics: To visualize the most frequent topics instead of all\n",
      " |          topics: Select which topics you would like to be visualized\n",
      " |          normalize_frequency: Whether to normalize each topic's frequency individually\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A plotly.graph_objects.Figure including all traces\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics over time, simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_over_time = topic_model.topics_over_time(docs, topics, timestamps)\n",
      " |      topic_model.visualize_topics_over_time(topics_over_time)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics_over_time(topics_over_time)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_topics_per_class(self, topics_per_class: pandas.core.frame.DataFrame, top_n_topics: int = 10, topics: List[int] = None, normalize_frequency: bool = False, custom_labels: bool = False, width: int = 1250, height: int = 900) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics per class\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics_per_class: The topics you would like to be visualized with the\n",
      " |                            corresponding topic representation\n",
      " |          top_n_topics: To visualize the most frequent topics instead of all\n",
      " |          topics: Select which topics you would like to be visualized\n",
      " |          normalize_frequency: Whether to normalize each topic's frequency individually\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A plotly.graph_objects.Figure including all traces\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics per class, simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_per_class = topic_model.topics_per_class(docs, topics, classes)\n",
      " |      topic_model.visualize_topics_per_class(topics_per_class)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics_per_class(topics_per_class)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(path: str, embedding_model=None) from builtins.type\n",
      " |      Loads the model from the specified path\n",
      " |      \n",
      " |      Arguments:\n",
      " |          path: the location and name of the BERTopic file you want to load\n",
      " |          embedding_model: If the embedding_model was not saved to save space or to load\n",
      " |                           it in from the cloud, you can load it in by specifying it here.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      BERTopic.load(\"my_model\")\n",
      " |      ```\n",
      " |      \n",
      " |      or if you did not save the embedding model:\n",
      " |      \n",
      " |      ```python\n",
      " |      BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_topic_tree(hier_topics: pandas.core.frame.DataFrame, max_distance: float = None, tight_layout: bool = False) -> str\n",
      " |      Extract the topic tree such that it can be printed\n",
      " |      \n",
      " |      Arguments:\n",
      " |          hier_topics: A dataframe containing the structure of the topic tree.\n",
      " |                      This is the output of `topic_model.hierachical_topics()`\n",
      " |          max_distance: The maximum distance between two topics. This value is\n",
      " |                      based on the Distance column in `hier_topics`.\n",
      " |          tight_layout: Whether to use a tight layout (narrow width) for\n",
      " |                      easier readability if you have hundreds of topics.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tree that has the following structure when printed:\n",
      " |              .\n",
      " |              .\n",
      " |              └─health_medical_disease_patients_hiv\n",
      " |                  ├─patients_medical_disease_candida_health\n",
      " |                  │    ├─■──candida_yeast_infection_gonorrhea_infections ── Topic: 48\n",
      " |                  │    └─patients_disease_cancer_medical_doctor\n",
      " |                  │         ├─■──hiv_medical_cancer_patients_doctor ── Topic: 34\n",
      " |                  │         └─■──pain_drug_patients_disease_diet ── Topic: 26\n",
      " |                  └─■──health_newsgroup_tobacco_vote_votes ── Topic: 9\n",
      " |      \n",
      " |          The blocks (■) indicate that the topic is one you can directly access\n",
      " |          from `topic_model.get_topic`. In other words, they are the original un-grouped topics.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Train model\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      \n",
      " |      # Print topic tree\n",
      " |      tree = topic_model.get_topic_tree(hierarchical_topics)\n",
      " |      print(tree)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>241567</td>\n",
       "      <td>-1_plant_plants_species_growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>0_allergen_allergens_pollen_ige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3976</td>\n",
       "      <td>1_medium_callus_regeneration_culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1111</td>\n",
       "      <td>2_dots_fluorescence_detection_carbon dots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>859</td>\n",
       "      <td>3_glyphosate_herbicide_resistance_herbicides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>85</td>\n",
       "      <td>825</td>\n",
       "      <td>85_soil_yield_nitrogen_fertilizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>86</td>\n",
       "      <td>567</td>\n",
       "      <td>86_inbreeding_depression_inbreeding depression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>87</td>\n",
       "      <td>2828</td>\n",
       "      <td>87_pollen_pollination_flowers_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>1849</td>\n",
       "      <td>88_populations_genetic_diversity_genetic diver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>89</td>\n",
       "      <td>1107</td>\n",
       "      <td>89_clade_phylogenetic_species_genera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic   Count                                               Name\n",
       "0      -1  241567                     -1_plant_plants_species_growth\n",
       "1       0     919                    0_allergen_allergens_pollen_ige\n",
       "2       1    3976               1_medium_callus_regeneration_culture\n",
       "3       2    1111          2_dots_fluorescence_detection_carbon dots\n",
       "4       3     859       3_glyphosate_herbicide_resistance_herbicides\n",
       "..    ...     ...                                                ...\n",
       "86     85     825                  85_soil_yield_nitrogen_fertilizer\n",
       "87     86     567  86_inbreeding_depression_inbreeding depression...\n",
       "88     87    2828               87_pollen_pollination_flowers_floral\n",
       "89     88    1849  88_populations_genetic_diversity_genetic diver...\n",
       "90     89    1107               89_clade_phylogenetic_species_genera\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Assign outliers to topics___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine probability distributions of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((421658, 90), (421658,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, probs[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.216580e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.686914e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.724943e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.414574e-308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.323693e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.881805e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.384614e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count   4.216580e+05\n",
       "mean    4.686914e-03\n",
       "std     3.724943e-02\n",
       "min    4.414574e-308\n",
       "25%     1.323693e-03\n",
       "50%     2.881805e-03\n",
       "75%     4.384614e-03\n",
       "max     1.000000e+00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(probs[:,0], bins=200, log_scale=True)\n",
    "pd.DataFrame(probs[:,0]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.216580e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.411141e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.244737e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.014457e-308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.727241e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.033303e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.625952e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count   4.216580e+05\n",
       "mean    6.411141e-03\n",
       "std     4.244737e-02\n",
       "min    5.014457e-308\n",
       "25%     1.727241e-03\n",
       "50%     4.033303e-03\n",
       "75%     5.625952e-03\n",
       "max     1.000000e+00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD8CAYAAABKKbKtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXoUlEQVR4nO3df7DddX3n8edLIsiqYAKBYRPYYKEzAjtFSSPqtusWB1i7u6CDbWxX0p3MxmWxo1u3s6B/oDCZlVlbHNpCB5csP8YKWYoDuiKmQeu0i0BwqfySJS0oKSxELotpXcDge/84nysnl5Obe5N8zr2E52PmO+d73t/v53ve554zeeX745yTqkKSpL3tNXPdgCRp32TASJK6MGAkSV0YMJKkLgwYSVIXBowkqYsFc93AfHHooYfWsmXL5roNSXpFufvuu39YVYtHLTNgmmXLlrFp06a5bkOSXlGSfH9nyzxEJknqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLPwcjSa8yVcXExAQAixYtIkmXx3EPRpJeZSYmJjj7so2cfdnGnwVND+7BSNKr0P6vP6j7Y7gHI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKmLbgGT5HVJ7kzyV0nuT/LpVl+UZEOSh9vtwqEx5yfZnOShJKcN1U9Kcm9bdmnaz68lOSDJ9a1+R5JlQ2NWtcd4OMmqXs9TkjRazz2Y54FfqapfAE4ETk9yMnAesLGqjgU2tvskOQ5YCRwPnA5clmS/tq3LgTXAsW06vdVXA89U1THAJcDFbVuLgAuAtwMrgAuGg0yS1F+3gKmBv2t3X9umAs4Arm71q4Ez2/wZwHVV9XxVPQJsBlYkOQI4qKpur6oCrpkyZnJbNwCntL2b04ANVTVRVc8AG3gplCRJY9D1HEyS/ZLcAzzF4B/8O4DDq+oJgHZ7WFt9CfDY0PAtrbakzU+t7zCmqrYDzwKHTLOtqf2tSbIpyaatW7fuwTOVJE3VNWCq6sWqOhFYymBv5IRpVs+oTUxT390xw/1dUVXLq2r54sWLp2lNkjRbY7mKrKr+L/BNBoepnmyHvWi3T7XVtgBHDg1bCjze6ktH1HcYk2QBcDAwMc22JElj0vMqssVJ3tTmDwTeA3wPuBmYvKprFXBTm78ZWNmuDDuawcn8O9thtG1JTm7nV86eMmZyW2cBt7XzNLcCpyZZ2E7un9pqkqQxWdBx20cAV7crwV4DrK+qryS5HVifZDXwA+ADAFV1f5L1wAPAduDcqnqxbesc4CrgQOCWNgFcCVybZDODPZeVbVsTSS4C7mrrXVhVEx2fqyRpim4BU1XfBd46ov40cMpOxqwF1o6obwJedv6mqp6jBdSIZeuAdbPrWpK0t/hJfklSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSuugWMEmOTPKNJA8muT/JR1v9U0n+Nsk9bXrv0Jjzk2xO8lCS04bqJyW5ty27NEla/YAk17f6HUmWDY1ZleThNq3q9TwlSaMt6Ljt7cDHq+o7Sd4I3J1kQ1t2SVV9dnjlJMcBK4HjgX8I/FmSn6+qF4HLgTXAt4GvAqcDtwCrgWeq6pgkK4GLgV9Psgi4AFgOVHvsm6vqmY7PV5I0pNseTFU9UVXfafPbgAeBJdMMOQO4rqqer6pHgM3AiiRHAAdV1e1VVcA1wJlDY65u8zcAp7S9m9OADVU10UJlA4NQkiSNyVjOwbRDV28F7miljyT5bpJ1SRa22hLgsaFhW1ptSZufWt9hTFVtB54FDplmW1P7WpNkU5JNW7du3f0nKEl6me4Bk+QNwJ8CH6uqHzE43PVzwInAE8DvTa46YnhNU9/dMS8Vqq6oquVVtXzx4sXTPQ1J0ix1DZgkr2UQLl+oqhsBqurJqnqxqn4KfB5Y0VbfAhw5NHwp8HirLx1R32FMkgXAwcDENNuSJI1Jz6vIAlwJPFhVvz9UP2JotfcB97X5m4GV7cqwo4FjgTur6glgW5KT2zbPBm4aGjN5hdhZwG3tPM2twKlJFrZDcKe2miRpTHpeRfYu4EPAvUnuabVPAB9MciKDQ1aPAh8GqKr7k6wHHmBwBdq57QoygHOAq4ADGVw9dkurXwlcm2Qzgz2XlW1bE0kuAu5q611YVRNdnqUkaaRuAVNVf8HocyFfnWbMWmDtiPom4IQR9eeAD+xkW+uAdTPtV5K0d/lJfklSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSuugWMEmOTPKNJA8muT/JR1t9UZINSR5utwuHxpyfZHOSh5KcNlQ/Kcm9bdmlSdLqByS5vtXvSLJsaMyq9hgPJ1nV63lKkkbruQezHfh4Vb0FOBk4N8lxwHnAxqo6FtjY7tOWrQSOB04HLkuyX9vW5cAa4Ng2nd7qq4FnquoY4BLg4ratRcAFwNuBFcAFw0EmSeqvW8BU1RNV9Z02vw14EFgCnAFc3Va7GjizzZ8BXFdVz1fVI8BmYEWSI4CDqur2qirgmiljJrd1A3BK27s5DdhQVRNV9QywgZdCSZI0BmM5B9MOXb0VuAM4vKqegEEIAYe11ZYAjw0N29JqS9r81PoOY6pqO/AscMg025ra15okm5Js2rp16x48Q0nSVN0DJskbgD8FPlZVP5pu1RG1mqa+u2NeKlRdUVXLq2r54sWLp2lNkjRbXQMmyWsZhMsXqurGVn6yHfai3T7V6luAI4eGLwUeb/WlI+o7jEmyADgYmJhmW5KkMel5FVmAK4EHq+r3hxbdDExe1bUKuGmovrJdGXY0g5P5d7bDaNuSnNy2efaUMZPbOgu4rZ2nuRU4NcnCdnL/1FaTJI3JgpmslORdVfWXu6pN8S7gQ8C9Se5ptU8AnwHWJ1kN/AD4AEBV3Z9kPfAAgyvQzq2qF9u4c4CrgAOBW9oEgwC7NslmBnsuK9u2JpJcBNzV1ruwqiZm8lwlSXvHjAIG+APgbTOo/UxV/QWjz4UAnLKTMWuBtSPqm4ATRtSfowXUiGXrgHU760+S1Ne0AZPkHcA7gcVJfmdo0UHAfqNHSZK06z2Y/YE3tPXeOFT/EYNzHpIkjTRtwFTVnwN/nuSqqvr+mHqSJO0DZnoO5oAkVwDLhsdU1a/0aEqS9Mo304D578AfA/8VeHEX60qSNOOA2V5Vl3ftRJK0T5npBy2/nOTfJzmifd3+ovaNxZIkjTTTPZjJT8v/7lCtgDfv3XYkSfuKGQVMVR3duxFJ0r5lpl8Vc/aoelVds3fbkSTtK2Z6iOwXh+Zfx+CrXr7D4Me/JEl6mZkeIvvt4ftJDgau7dKRJGmfsLtf1/9jBl+nL0nSSDM9B/NlXvpFyP2AtwDrezUlSXrlm+k5mM8OzW8Hvl9VWzr0I0naR8zoEFn70svvMfhG5YXACz2bkiS98s0oYJL8GnAngx/3+jXgjiR+Xb8kaadmeojsk8AvVtVTAEkWA38G3NCrMUnSK9tMryJ7zWS4NE/PYqwk6VVopnswX0tyK/DFdv/Xga/2aUmStC+YNmCSHAMcXlW/m+T9wD8BAtwOfGEM/UmSXqF2dZjrc8A2gKq6sap+p6r+A4O9l8/1bU2S9Eq2q4BZVlXfnVqsqk0Mfj55p5KsS/JUkvuGap9K8rdJ7mnTe4eWnZ9kc5KHkpw2VD8pyb1t2aVJ0uoHJLm+1e9IsmxozKokD7dp8qcGJEljtKuAed00yw7cxdirgNNH1C+pqhPb9FWAJMcBK4Hj25jLkuzX1r8cWMPgq2mOHdrmauCZqjoGuAS4uG1rEXAB8HZgBXBBkoW76FWStJftKmDuSvJvpxaTrAbunm5gVX0LmJhhH2cA11XV81X1CLAZWJHkCOCgqrq9qorBtzefOTTm6jZ/A3BK27s5DdhQVRNV9QywgdFBJ0nqaFdXkX0M+FKS3+SlQFkO7A+8bzcf8yPt92U2AR9vIbAE+PbQOlta7SdtfmqddvsYQFVtT/IscMhwfcSYHSRZw2DviKOOOmo3n44kaZRp92Cq6smqeifwaeDRNn26qt5RVf9nNx7vcuDngBOBJ4Dfa/WMevhp6rs7Zsdi1RVVtbyqli9evHiatiVJszXT34P5BvCNPX2wqnpycj7J54GvtLtbgCOHVl0KPN7qS0fUh8dsSbIAOJjBIbktwLunjPnmnvYuSZqdsX4av51TmfQ+YPIKs5uBle3KsKMZnMy/s6qeALYlObmdXzkbuGlozOQVYmcBt7XzNLcCpyZZ2E7un9pqkqQxmukn+WctyRcZ7EkcmmQLgyu73p3kRAaHrB4FPgxQVfcnWQ88wODnAM6tqhfbps5hcEXagcAtbQK4Erg2yWYGey4r27YmklwE3NXWu7CqZnqxgSRpL+kWMFX1wRHlK6dZfy2wdkR9E3DCiPpzDL7dedS21gHrZtysJGmv8wsrJUldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqolvAJFmX5Kkk9w3VFiXZkOThdrtwaNn5STYneSjJaUP1k5Lc25ZdmiStfkCS61v9jiTLhsasao/xcJJVvZ6jJGnneu7BXAWcPqV2HrCxqo4FNrb7JDkOWAkc38ZclmS/NuZyYA1wbJsmt7kaeKaqjgEuAS5u21oEXAC8HVgBXDAcZJKk8egWMFX1LWBiSvkM4Oo2fzVw5lD9uqp6vqoeATYDK5IcARxUVbdXVQHXTBkzua0bgFPa3s1pwIaqmqiqZ4ANvDzoJEmdjfsczOFV9QRAuz2s1ZcAjw2tt6XVlrT5qfUdxlTVduBZ4JBptiVJGqP5cpI/I2o1TX13x+z4oMmaJJuSbNq6deuMGpUkzcy4A+bJdtiLdvtUq28BjhxabynweKsvHVHfYUySBcDBDA7J7WxbL1NVV1TV8qpavnjx4j14WpKkqcYdMDcDk1d1rQJuGqqvbFeGHc3gZP6d7TDatiQnt/MrZ08ZM7mts4Db2nmaW4FTkyxsJ/dPbTVJ0hgt6LXhJF8E3g0cmmQLgyu7PgOsT7Ia+AHwAYCquj/JeuABYDtwblW92DZ1DoMr0g4EbmkTwJXAtUk2M9hzWdm2NZHkIuCutt6FVTX1YgNJUmfdAqaqPriTRafsZP21wNoR9U3ACSPqz9ECasSydcC6GTcrSdrr5stJfknSPsaAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDJi9oKp4+umnqaq5bkWS5o05CZgkjya5N8k9STa12qIkG5I83G4XDq1/fpLNSR5KctpQ/aS2nc1JLk2SVj8gyfWtfkeSZT2fz8TEBCs/+yUmJiZ6PowkvaLM5R7MP6uqE6tqebt/HrCxqo4FNrb7JDkOWAkcD5wOXJZkvzbmcmANcGybTm/11cAzVXUMcAlwce8ns/8/eGPvh5CkV5T5dIjsDODqNn81cOZQ/bqqer6qHgE2AyuSHAEcVFW31+DY1DVTxkxu6wbglMm9G0nSeMxVwBTw9SR3J1nTaodX1RMA7fawVl8CPDY0dkurLWnzU+s7jKmq7cCzwCEdnockaScWzNHjvquqHk9yGLAhyfemWXfUnkdNU59uzI4bHoTbGoCjjjpq+o4lSbMyJ3swVfV4u30K+BKwAniyHfai3T7VVt8CHDk0fCnweKsvHVHfYUySBcDBwMvOwFfVFVW1vKqWL168eO88OUkSMAcBk+T1Sd44OQ+cCtwH3AysaqutAm5q8zcDK9uVYUczOJl/ZzuMti3Jye38ytlTxkxu6yzgtvIaYkkaq7k4RHY48KV2zn0B8CdV9bUkdwHrk6wGfgB8AKCq7k+yHngA2A6cW1Uvtm2dA1wFHAjc0iaAK4Frk2xmsOeychxPTJL0krEHTFX9DfALI+pPA6fsZMxaYO2I+ibghBH152gBJUmaG/PpMmVJ0j7EgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXc/VVMZKkMasqJiYmxvbTIgaMJL1KTExMcPZlG3nhx9s44E2H89rX9o0AA0aSXkX2f/1BY3ssz8FIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktTFPh0wSU5P8lCSzUnOm+t+JGk+mfwBsqrqsv19NmCS7Af8EfDPgeOADyY5bm67kqT54yc/3saaK27r9guX+/IPjq0ANlfV3wAkuQ44A3igx4O98ONtY/sZUknaHRMTE7zw9z/ihR9v4zXbt/PTF/5f18fblwNmCfDY0P0twNuHV0iyBljT7v5dkof25AF//jN7MnqnDgV+2GXLu28+9gT2NRvzsSewr9nYaz0d+tl/tyfD/9HOFuzLAZMRtR0ONFbVFcAV42ln9yTZVFXL57qPYfOxJ7Cv2ZiPPYF9zcZ87GmqffYcDIM9liOH7i8FHp+jXiTpVWdfDpi7gGOTHJ1kf2AlcPMc9yRJrxr77CGyqtqe5CPArcB+wLqqun+O29od8/EQ3nzsCexrNuZjT2BfszEfe9pBel3/LEl6dduXD5FJkuaQASNJ6sKAkSR1sc+e5N/XJDkT+FXgMOCPqurrSY4C/pDBh63+N/Bt4CLgfuC6qvrmXPTV6q8HvgVcADwL/CaD99txVfXOcffU+rkMeAH4JvC/gE8BTwMbq+qGnj21vt4MfBI4uKrOarVfYuhvA/zrqevMUV9vAT7K4MN8Gxn8zX52v6ouH1NvxzHldUrybsb8Ph/R1w49AC8yxvf4NH2Nei1f9jccW0NV5dR5AtYBTwH3TamfDjwEbAbOm+G2FgJXtvn3AB9u89cA/xS4BbgKOGau+mr3LwT+E/AvhmpnTvY7B3+rDwH/ss1fD3wc+KV2/+Yx/61uGFHb4W8zap056us1U17XHe737m3U6zTb93mnvkb2MJP3+Lhfy9m+1/fmNLYHejVPwC8Dbxt+4zC4dPqvgTcD+wN/xeB/sP8Y+MqU6bChcb8HvK3NHwJ8A7gN+DfAa1r9cOALc9jXexh87ui32DFg1gMHzVFP5wMntvk/oe3dAP8F+Msx/61G/UO+w99m1Drj7gv4V8D/BH5j1P1x9DbqdWKW7/NOfY3sYerrOIf/TgwHzKze63tzGtsDvdonYNmUN847gFuH7p8PnD/N+AAXA+8Zqv1H4Jfb/PAbav9Z/APVo6+1wOeArwM3Mfhf71HA5+ewpw/Rwo7BYZXJ+n7ATePoa2i9qf+Qv+xvM9PXr2dfQ/X/Md39MfX2stdpNu/zjn39rIfZvMfn4rWczXt9b02eg5k7u/wyzil+m8GewcFJjqmqPwa+BnwqyW8AjyZ5P3Aa8CYG52bmpK+q+iRAkt8CflhVP02yGvhvc9UTcCPwh0l+FfhykmXAJ4DXM/ifXfe+khzCIHzfmuT8qvrPbdHP/jbTrDPWvto5hvcDBwBfnXp/N3ra3d6WMeV12kvv8z3ta1QPe/Ie31t9jXotl7Hn7/XdYsDMnV1+GecOC6ouBS6dUrsPmHoi+Ma57mto2VVD8xfMZU9V9fcMDiMOW8OemW1fTwMv+9ra4b/NztYZd181OHH+zSmrTr2/O2bb26NMeZ2q6kb2/H0+1Wz7elkPe/ge35m98Vo+yp6/13eLlynPnfn6ZZzzsa/52BPY1+6Yr73ZVwcGzNyZr1/GOR/7mo89gX3tjvnam331MM4TPq/WCfgi8ATwEwb/I1nd6u9l8PmVvwY+aV/zsyf72rd6s6/xTX7ZpSSpCw+RSZK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6+P84u+9ihKBSqQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(probs[:,1], bins=200, log_scale=True)\n",
    "pd.DataFrame(probs[:,1]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = 0.01\n",
    "new_topics = [np.argmax(prob) if max(prob) >= probability_threshold else -1 \n",
    "                                                            for prob in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~8 min\n",
    "topic_model.update_topics(docs_clean, new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(work_dir / 'topic_model_updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info_updated = topic_model.get_topic_info()\n",
    "topic_info_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch_default': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4255f477c905e3cafd6d08b9a6d118445dbfbaff982fd1d9831280a79a13df35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
