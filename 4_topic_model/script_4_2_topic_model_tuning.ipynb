{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 4.2: Topic model tuning__\n",
    "\n",
    "BERTopic \n",
    "- [Step-by-step](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)\n",
    "- [Deal with situation where most docs are in the -1 topic](https://github.com/MaartenGr/BERTopic/issues/485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting working directory\n",
    "proj_dir   = Path.home() / \"projects/plant_sci_hist\"\n",
    "work_dir   = proj_dir / \"4_topic_model/4_2_tuning\"\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# plant science corpus\n",
    "dir25       = proj_dir / \"2_text_classify/2_5_predict_pubmed\"\n",
    "corpus_file = dir25 / \"corpus_plant_421658.tsv.gz\"\n",
    "\n",
    "# processed docs\n",
    "dir41            = proj_dir / \"4_topic_model/4_1_get_topics\"\n",
    "docs_clean_file  = dir41 / \"corpus_plant_421658_proc_txt.pkl\"\n",
    "\n",
    "# embedding model\n",
    "emb_model_name = \"allenai/scibert_scivocab_uncased\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Load data and get embeddings___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(docs_clean_file, \"rb\") as f:\n",
    "  docs_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421658,\n",
       " 'identification 120 mus phase decay delayed fluorescence spinach chloroplasts subchloroplast particles intrinsic back reaction . dependence level phase thylakoids internal ph . 500 mus laser flash 120 mus phase decay delayed fluorescence visible variety circumstances spinach chloroplasts subchloroplast particles enriched photosystem ii prepared means digitonin . level phase high case inhibition oxygen evolution donor side photosystem ii . comparison results babcock sauer ( 1975 ) biochim . bio-phys . acta 376 , 329-344 , indicates epr signal iif suppose due z+ , oxidized first secondary donor photosystem ii , well correlated large amplitude 120 mus phase . explain 120 mus phase intrinsic back reaction excited reaction center presence z+ , predicted van gorkom donze ( 1973 ) photochem . photobiol . 17 , 333-342. redox state z+ dependent internal ph thylakoids . results effect ph mus region compared obtained ms region .')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_clean), docs_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get doc embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/shius/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/shius/.cache/torch/sentence_transformers/allenai_scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "emb_model = SentenceTransformer(emb_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc27bdb60bd64099ade0ec461bc25231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = emb_model.encode(docs_clean, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output embeddings\n",
    "with open(work_dir / \"embeddings_scibert.pickle\", \"wb\") as f:\n",
    "  pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with open(work_dir / \"embeddings_scibert.pickle\", \"rb\") as f:\n",
    "  embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (421658, 768))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Run BERTopic___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN clustering setting\n",
    "min_cluster_size         = 500 \n",
    "metric                   = 'euclidean' \n",
    "cluster_selection_method ='eom' \n",
    "prediction_data          = True \n",
    "min_samples              = 5\n",
    "\n",
    "# BERTopic setting\n",
    "calculate_probabilities = True\n",
    "n_neighbors             = 10  \n",
    "nr_topics               = 500\n",
    "n_gram_range            = (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize HDBSCAN\n",
    "\n",
    "For reducing outliers, following [this instruction](https://maartengr.github.io/BERTopic/faq.html#how-do-i-reduce-topic-outliers)\n",
    "- Also see [HDBSCAN doc](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#what-about-different-metrics)\n",
    "- Comparison of [distance metrics](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                        metric=metric, \n",
    "                        cluster_selection_method=cluster_selection_method, \n",
    "                        prediction_data=prediction_data, \n",
    "                        min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize and train topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(hdbscan_model=hdbscan_model,\n",
    "                       calculate_probabilities=calculate_probabilities,\n",
    "                       n_gram_range=n_gram_range,\n",
    "                       nr_topics=nr_topics,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2022-07-20 15:10:02,980 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 15:14:40,126 - BERTopic - Clustered reduced embeddings\n",
      "2022-07-20 15:21:22,908 - BERTopic - Reduced number of topics from 91 to 91\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(docs_clean,\n",
    "                                          embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model, topics, and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already save the embeddings, so won't save it again\n",
    "topic_model.save(work_dir / 'topic_model')\n",
    "\n",
    "with open(work_dir / 'probs.pickle', \"wb\") as f:\n",
    "  pickle.dump(probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model\n",
    "topic_model = BERTopic.load(work_dir / 'topic_model')\n",
    "\n",
    "# load prob\n",
    "with open(work_dir / 'probs.pickle', \"rb\") as f:\n",
    "  probs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BERTopic in module bertopic._bertopic object:\n",
      "\n",
      "class BERTopic(builtins.object)\n",
      " |  BERTopic(language: str = 'english', top_n_words: int = 10, n_gram_range: Tuple[int, int] = (1, 1), min_topic_size: int = 10, nr_topics: Union[int, str] = None, low_memory: bool = False, calculate_probabilities: bool = False, diversity: float = None, seed_topic_list: List[List[str]] = None, embedding_model=None, umap_model: umap.umap_.UMAP = None, hdbscan_model: hdbscan.hdbscan_.HDBSCAN = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, verbose: bool = False)\n",
      " |  \n",
      " |  BERTopic is a topic modeling technique that leverages BERT embeddings and\n",
      " |  c-TF-IDF to create dense clusters allowing for easily interpretable topics\n",
      " |  whilst keeping important words in the topic descriptions.\n",
      " |  \n",
      " |  The default embedding model is `all-MiniLM-L6-v2` when selecting `language=\"english\"` \n",
      " |  and `paraphrase-multilingual-MiniLM-L12-v2` when selecting `language=\"multilingual\"`.\n",
      " |  \n",
      " |  Usage:\n",
      " |  \n",
      " |  ```python\n",
      " |  from bertopic import BERTopic\n",
      " |  from sklearn.datasets import fetch_20newsgroups\n",
      " |  \n",
      " |  docs = fetch_20newsgroups(subset='all')['data']\n",
      " |  topic_model = BERTopic()\n",
      " |  topics, probabilities = topic_model.fit_transform(docs)\n",
      " |  ```\n",
      " |  \n",
      " |  If you want to use your own embedding model, use it as follows:\n",
      " |  \n",
      " |  ```python\n",
      " |  from bertopic import BERTopic\n",
      " |  from sklearn.datasets import fetch_20newsgroups\n",
      " |  from sentence_transformers import SentenceTransformer\n",
      " |  \n",
      " |  docs = fetch_20newsgroups(subset='all')['data']\n",
      " |  sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |  topic_model = BERTopic(embedding_model=sentence_model)\n",
      " |  ```\n",
      " |  \n",
      " |  Due to the stochastisch nature of UMAP, the results from BERTopic might differ\n",
      " |  and the quality can degrade. Using your own embeddings allows you to\n",
      " |  try out BERTopic several times until you find the topics that suit\n",
      " |  you best.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, language: str = 'english', top_n_words: int = 10, n_gram_range: Tuple[int, int] = (1, 1), min_topic_size: int = 10, nr_topics: Union[int, str] = None, low_memory: bool = False, calculate_probabilities: bool = False, diversity: float = None, seed_topic_list: List[List[str]] = None, embedding_model=None, umap_model: umap.umap_.UMAP = None, hdbscan_model: hdbscan.hdbscan_.HDBSCAN = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, verbose: bool = False)\n",
      " |      BERTopic initialization\n",
      " |      \n",
      " |      Arguments:\n",
      " |          language: The main language used in your documents. The default sentence-transformers \n",
      " |                    model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of\n",
      " |                    supported languages see bertopic.backend.languages. Select\n",
      " |                    \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2`\n",
      " |                    sentence-tranformers model that supports 50+ languages.\n",
      " |          top_n_words: The number of words per topic to extract. Setting this\n",
      " |                       too high can negatively impact topic embeddings as topics\n",
      " |                       are typically best represented by at most 10 words.\n",
      " |          n_gram_range: The n-gram range for the CountVectorizer.\n",
      " |                        Advised to keep high values between 1 and 3.\n",
      " |                        More would likely lead to memory issues.\n",
      " |                        NOTE: This param will not be used if you pass in your own\n",
      " |                        CountVectorizer.\n",
      " |          min_topic_size: The minimum size of the topic. Increasing this value will lead\n",
      " |                          to a lower number of clusters/topics.\n",
      " |          nr_topics: Specifying the number of topics will reduce the initial\n",
      " |                     number of topics to the value specified. This reduction can take\n",
      " |                     a while as each reduction in topics (-1) activates a c-TF-IDF\n",
      " |                     calculation. If this is set to None, no reduction is applied. Use\n",
      " |                     \"auto\" to automatically reduce topics using HDBSCAN.\n",
      " |          low_memory: Sets UMAP low memory to True to make sure less memory is used.\n",
      " |                      NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP\n",
      " |                      this parameter will not be used.\n",
      " |          calculate_probabilities: Whether to calculate the probabilities of all topics\n",
      " |                                   per document instead of the probability of the assigned\n",
      " |                                   topic per document. This could slow down the extraction\n",
      " |                                   of topics if you have many documents (> 100_000). Set this\n",
      " |                                   only to True if you have a low amount of documents or if\n",
      " |                                   you do not mind more computation time.\n",
      " |                                   NOTE: If false you cannot use the corresponding\n",
      " |                                   visualization method `visualize_probabilities`.\n",
      " |          diversity: Whether to use MMR to diversify the resulting topic representations.\n",
      " |                     If set to None, MMR will not be used. Accepted values lie between\n",
      " |                     0 and 1 with 0 being not at all diverse and 1 being very diverse.\n",
      " |          seed_topic_list: A list of seed words per topic to converge around\n",
      " |          verbose: Changes the verbosity of the model, Set to True if you want\n",
      " |                   to track the stages of the model.\n",
      " |          embedding_model: Use a custom embedding model.\n",
      " |                           The following backends are currently supported\n",
      " |                             * SentenceTransformers\n",
      " |                             * Flair\n",
      " |                             * Spacy\n",
      " |                             * Gensim\n",
      " |                             * USE (TF-Hub)\n",
      " |                           You can also pass in a string that points to one of the following\n",
      " |                           sentence-transformers models:\n",
      " |                             * https://www.sbert.net/docs/pretrained_models.html\n",
      " |          umap_model: Pass in a UMAP model to be used instead of the default.\n",
      " |                      NOTE: You can also pass in any dimensionality reduction algorithm as long\n",
      " |                      as it has `.fit` and `.transform` functions.\n",
      " |          hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default\n",
      " |                         NOTE: You can also pass in any clustering algorithm as long as it has\n",
      " |                         `.fit` and `.predict` functions along with the `.labels_` variable.\n",
      " |          vectorizer_model: Pass in a CountVectorizer instead of the default\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: Human readable representation of the most important model parameters.\n",
      " |               The parameters that represent models are ignored due to their\n",
      " |  \n",
      " |  find_topics(self, search_term: str, top_n: int = 5) -> Tuple[List[int], List[float]]\n",
      " |      Find topics most similar to a search_term\n",
      " |      \n",
      " |      Creates an embedding for search_term and compares that with\n",
      " |      the topic embeddings. The most similar topics are returned\n",
      " |      along with their similarity values.\n",
      " |      \n",
      " |      The search_term can be of any size but since it compares\n",
      " |      with the topic representation it is advised to keep it\n",
      " |      below 5 words.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          search_term: the term you want to use to search for topics\n",
      " |          top_n: the number of topics to return\n",
      " |      \n",
      " |      Returns:\n",
      " |          similar_topics: the most similar topics from high to low\n",
      " |          similarity: the similarity scores from high to low\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      You can use the underlying embedding model to find topics that\n",
      " |      best represent the search term:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics, similarity = topic_model.find_topics(\"sports\", top_n=5)\n",
      " |      ```\n",
      " |      \n",
      " |      Note that the search query is typically more accurate if the\n",
      " |      search_term consists of a phrase or multiple words.\n",
      " |  \n",
      " |  fit(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None)\n",
      " |      Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic().fit(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings, use it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  fit_transform(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None) -> Tuple[List[int], Optional[numpy.ndarray]]\n",
      " |      Fit the models on a collection of documents, generate topics, and return the docs with topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Returns:\n",
      " |          predictions: Topic predictions for each documents\n",
      " |          probabilities: The probability of the assigned topic per document.\n",
      " |                         If `calculate_probabilities` in BERTopic is set to True, then\n",
      " |                         it calculates the probabilities of all topics across all documents\n",
      " |                         instead of only the assigned topic. This, however, slows down\n",
      " |                         computation and may increase memory usage.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings, use it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  generate_topic_labels(self, nr_words: int = 3, topic_prefix: bool = True, word_length: int = None, separator: str = '_') -> List[str]\n",
      " |      Get labels for each topic in a user-defined format\n",
      " |      \n",
      " |      Arguments:\n",
      " |          original_labels:\n",
      " |          nr_words: Top `n` words per topic to use\n",
      " |          topic_prefix: Whether to use the topic ID as a prefix.\n",
      " |                      If set to True, the topic ID will be separated\n",
      " |                      using the `separator`\n",
      " |          word_length: The maximum length of each word in the topic label.\n",
      " |                      Some words might be relatively long and setting this\n",
      " |                      value helps to make sure that all labels have relatively\n",
      " |                      similar lengths.\n",
      " |          separator: The string with which the words and topic prefix will be\n",
      " |                  separated. Underscores are the default but a nice alternative\n",
      " |                  is `\", \"`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic_labels: A list of topic labels sorted from the lowest topic ID to the highest.\n",
      " |                      If the topic model was trained using HDBSCAN, the lowest topic ID is -1,\n",
      " |                      otherwise it is 0.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To create our custom topic labels, usage is rather straightforward:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \")\n",
      " |      ```\n",
      " |  \n",
      " |  get_params(self, deep: bool = False) -> Mapping[str, Any]\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Adapted from:\n",
      " |          https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178\n",
      " |      \n",
      " |      Arguments:\n",
      " |          deep: bool, default=True\n",
      " |                If True, will return the parameters for this estimator and\n",
      " |                contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          out: Parameter names mapped to their values.\n",
      " |  \n",
      " |  get_representative_docs(self, topic: int = None) -> List[str]\n",
      " |      Extract representative documents per topic\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want\n",
      " |                 the representative documents\n",
      " |      \n",
      " |      Returns:\n",
      " |          Representative documents of the chosen topic\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To extract the representative docs of all topics:\n",
      " |      \n",
      " |      ```python\n",
      " |      representative_docs = topic_model.get_representative_docs()\n",
      " |      ```\n",
      " |      \n",
      " |      To get the representative docs of a single topic:\n",
      " |      \n",
      " |      ```python\n",
      " |      representative_docs = topic_model.get_representative_docs(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic(self, topic: int) -> Union[Mapping[str, Tuple[str, float]], bool]\n",
      " |      Return top n words for a specific topic and their c-TF-IDF scores\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want its representation\n",
      " |      \n",
      " |      Returns:\n",
      " |          The top n words for a specific word and its respective c-TF-IDF scores\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic = topic_model.get_topic(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic_freq(self, topic: int = None) -> Union[pandas.core.frame.DataFrame, int]\n",
      " |      Return the the size of topics (descending order)\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want the frequency\n",
      " |      \n",
      " |      Returns:\n",
      " |          Either the frequency of a single topic or dataframe with\n",
      " |          the frequencies of all topics\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To extract the frequency of all topics:\n",
      " |      \n",
      " |      ```python\n",
      " |      frequency = topic_model.get_topic_freq()\n",
      " |      ```\n",
      " |      \n",
      " |      To get the frequency of a single topic:\n",
      " |      \n",
      " |      ```python\n",
      " |      frequency = topic_model.get_topic_freq(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic_info(self, topic: int = None) -> pandas.core.frame.DataFrame\n",
      " |      Get information about each topic including its ID, frequency, and name.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want the frequency\n",
      " |      \n",
      " |      Returns:\n",
      " |          info: The information relating to either a single topic or all topics\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      info_df = topic_model.get_topic_info()\n",
      " |      ```\n",
      " |  \n",
      " |  get_topics(self) -> Mapping[str, Tuple[str, float]]\n",
      " |      Return topics with top n words and their c-TF-IDF score\n",
      " |      \n",
      " |      Returns:\n",
      " |          self.topic: The top n words per topic and the corresponding c-TF-IDF score\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      all_topics = topic_model.get_topics()\n",
      " |      ```\n",
      " |  \n",
      " |  hierarchical_topics(self, docs: List[int], topics: List[int], linkage_function: Callable[[scipy.sparse.csr.csr_matrix], numpy.ndarray] = None, distance_function: Callable[[scipy.sparse.csr.csr_matrix], scipy.sparse.csr.csr_matrix] = None) -> pandas.core.frame.DataFrame\n",
      " |      Create a hierarchy of topics\n",
      " |      \n",
      " |      To create this hierarchy, BERTopic needs to be already fitted once.\n",
      " |      Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF\n",
      " |      representation using `scipy.cluster.hierarchy.linkage`.\n",
      " |      \n",
      " |      Based on that hierarchy, we calculate the topic representation at each\n",
      " |      merged step. This is a local representation, as we only assume that the\n",
      " |      chosen step is merged and not all others which typically improves the\n",
      " |      topic representation.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          linkage_function: The linkage function to use. Default is:\n",
      " |                          `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\n",
      " |          distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\n",
      " |                              `lambda x: 1 - cosine_similarity(x)`\n",
      " |      \n",
      " |      Returns:\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      ```\n",
      " |      \n",
      " |      A custom linkage function can be used as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from scipy.cluster import hierarchy as sch\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      \n",
      " |      # Hierarchical topics\n",
      " |      linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics, linkage_function=linkage_function)\n",
      " |      ```\n",
      " |  \n",
      " |  merge_topics(self, docs: List[str], topics: List[int], topics_to_merge: List[Union[Iterable[int], int]]) -> None\n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          topics_to_merge: Either a list of topics or a list of list of topics\n",
      " |                          to merge. For example:\n",
      " |                              [1, 2, 3] will merge topics 1, 2 and 3\n",
      " |                              [[1, 2], [3, 4]] will merge topics 1 and 2, and\n",
      " |                              separately merge topics 3 and 4.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      If you want to merge topics 1, 2, and 3:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_to_merge = [1, 2, 3]\n",
      " |      topic_model.merge_topics(docs, topics, topics_to_merge)\n",
      " |      ```\n",
      " |      \n",
      " |      or if you want to merge topics 1 and 2, and separately\n",
      " |      merge topics 3 and 4:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_to_merge = [[1, 2]\n",
      " |                          [3, 4]]\n",
      " |      topic_model.merge_topics(docs, topics, topics_to_merge)\n",
      " |      ```\n",
      " |  \n",
      " |  reduce_topics(self, docs: List[str], topics: List[int], probabilities: numpy.ndarray = None, nr_topics: int = 20) -> Tuple[List[int], numpy.ndarray]\n",
      " |      Further reduce the number of topics to nr_topics.\n",
      " |      \n",
      " |      The number of topics is further reduced by calculating the c-TF-IDF matrix\n",
      " |      of the documents and then reducing them by iteratively merging the least\n",
      " |      frequent topic with the most similar one based on their c-TF-IDF matrices.\n",
      " |      The topics, their sizes, and representations are updated.\n",
      " |      \n",
      " |      The reasoning for putting `docs`, `topics`, and `probs` as parameters is that\n",
      " |      these values are not saved within BERTopic on purpose. If you were to have a\n",
      " |      million documents, it seems very inefficient to save those in BERTopic\n",
      " |      instead of a dedicated database.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The docs you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`\n",
      " |          nr_topics: The number of topics you want reduced to\n",
      " |      \n",
      " |      Returns:\n",
      " |          new_topics: Updated topics\n",
      " |          new_probabilities: Updated probabilities\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      You can further reduce the topics by passing the documents with its\n",
      " |      topics and probabilities (if they were calculated):\n",
      " |      \n",
      " |      ```python\n",
      " |      new_topics, new_probs = topic_model.reduce_topics(docs, topics, probabilities, nr_topics=30)\n",
      " |      ```\n",
      " |      \n",
      " |      If probabilities were not calculated simply run the function without them:\n",
      " |      \n",
      " |      ```python\n",
      " |      new_topics, new_probs = topic_model.reduce_topics(docs, topics, nr_topics=30)\n",
      " |      ```\n",
      " |  \n",
      " |  save(self, path: str, save_embedding_model: bool = True) -> None\n",
      " |      Saves the model to the specified path\n",
      " |      \n",
      " |      Arguments:\n",
      " |          path: the location and name of the file you want to save\n",
      " |          save_embedding_model: Whether to save the embedding model in this class\n",
      " |                                as you might have selected a local model or one that\n",
      " |                                is downloaded automatically from the cloud.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.save(\"my_model\")\n",
      " |      ```\n",
      " |      \n",
      " |      or if you do not want the embedding_model to be saved locally:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.save(\"my_model\", save_embedding_model=False)\n",
      " |      ```\n",
      " |  \n",
      " |  set_topic_labels(self, topic_labels: Union[List[str], Mapping[int, str]]) -> None\n",
      " |      Set custom topic labels in your fitted BERTopic model\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_labels: If a list of topic labels, it should contain the same number\n",
      " |                      of labels as there are topics. This must be ordered\n",
      " |                      from the topic with the lowest ID to the highest ID,\n",
      " |                      including topic -1 if it exists.\n",
      " |                      If a dictionary of `topic ID`: `topic_label`, it can have\n",
      " |                      any number of topics as it will only map the topics found\n",
      " |                      in the dictionary.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      First, we define our topic labels with `.get_topic_labels` in which\n",
      " |      we can customize our topic labels:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_labels = topic_model.get_topic_labels(nr_words=2,\n",
      " |                                                  topic_prefix=True,\n",
      " |                                                  word_length=10,\n",
      " |                                                  separator=\", \")\n",
      " |      ```\n",
      " |      \n",
      " |      Then, we pass these `topic_labels` to our topic model which\n",
      " |      can be accessed at any time with `.custom_labels`:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.set_topic_labels(topic_labels)\n",
      " |      topic_model.custom_labels\n",
      " |      ```\n",
      " |      \n",
      " |      You might want to change only a few topic labels instead of all of them.\n",
      " |      To do so, you can pass a dictionary where the keys are the topic IDs and\n",
      " |      its keys the topic labels:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"})\n",
      " |      topic_model.custom_labels\n",
      " |      ```\n",
      " |  \n",
      " |  topics_over_time(self, docs: List[str], topics: List[int], timestamps: Union[List[str], List[int]], nr_bins: int = None, datetime_format: str = None, evolution_tuning: bool = True, global_tuning: bool = True) -> pandas.core.frame.DataFrame\n",
      " |      Create topics over time\n",
      " |      \n",
      " |      To create the topics over time, BERTopic needs to be already fitted once.\n",
      " |      From the fitted models, the c-TF-IDF representations are calculate at\n",
      " |      each timestamp t. Then, the c-TF-IDF representations at timestamp t are\n",
      " |      averaged with the global c-TF-IDF representations in order to fine-tune the\n",
      " |      local representations.\n",
      " |      \n",
      " |      NOTE:\n",
      " |          Make sure to use a limited number of unique timestamps (<100) as the\n",
      " |          c-TF-IDF representation will be calculated at each single unique timestamp.\n",
      " |          Having a large number of unique timestamps can take some time to be calculated.\n",
      " |          Moreover, there aren't many use-cased where you would like to see the difference\n",
      " |          in topic representations over more than 100 different timestamps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          timestamps: The timestamp of each document. This can be either a list of strings or ints.\n",
      " |                      If it is a list of strings, then the datetime format will be automatically\n",
      " |                      inferred. If it is a list of ints, then the documents will be ordered by\n",
      " |                      ascending order.\n",
      " |          nr_bins: The number of bins you want to create for the timestamps. The left interval will\n",
      " |                   be chosen as the timestamp. An additional column will be created with the\n",
      " |                   entire interval.\n",
      " |          datetime_format: The datetime format of the timestamps if they are strings, eg “%d/%m/%Y”.\n",
      " |                           Set this to None if you want to have it automatically detect the format.\n",
      " |                           See strftime documentation for more information on choices:\n",
      " |                           https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
      " |          evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its\n",
      " |                            c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates\n",
      " |                            evolutionary topic representations.\n",
      " |          global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix\n",
      " |                     with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
      " |                     topic representations that could not be found in the documents at timestamp t.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topics_over_time: A dataframe that contains the topic, words, and frequency of topic\n",
      " |                            at timestamp t.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      The timestamps variable represent the timestamp of each document. If you have over\n",
      " |      100 unique timestamps, it is advised to bin the timestamps as shown below:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20)\n",
      " |      ```\n",
      " |  \n",
      " |  topics_per_class(self, docs: List[str], topics: List[int], classes: Union[List[int], List[str]], global_tuning: bool = True) -> pandas.core.frame.DataFrame\n",
      " |      Create topics per class\n",
      " |      \n",
      " |      To create the topics per class, BERTopic needs to be already fitted once.\n",
      " |      From the fitted models, the c-TF-IDF representations are calculate at\n",
      " |      each class c. Then, the c-TF-IDF representations at class c are\n",
      " |      averaged with the global c-TF-IDF representations in order to fine-tune the\n",
      " |      local representations. This can be turned off if the pure representation is\n",
      " |      needed.\n",
      " |      \n",
      " |      NOTE:\n",
      " |          Make sure to use a limited number of unique classes (<100) as the\n",
      " |          c-TF-IDF representation will be calculated at each single unique class.\n",
      " |          Having a large number of unique classes can take some time to be calculated.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          classes: The class of each document. This can be either a list of strings or ints.\n",
      " |          global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix\n",
      " |                     with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
      " |                     topic representations that could not be found in the documents at timestamp t.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topics_per_class: A dataframe that contains the topic, words, and frequency of topics\n",
      " |                            for each class.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      topics_per_class = topic_model.topics_per_class(docs, topics, classes)\n",
      " |      ```\n",
      " |  \n",
      " |  transform(self, documents: Union[str, List[str]], embeddings: numpy.ndarray = None) -> Tuple[List[int], numpy.ndarray]\n",
      " |      After having fit a model, use transform to predict new instances\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A single document or a list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          predictions: Topic predictions for each documents\n",
      " |          probabilities: The topic probability distribution which is returned by default.\n",
      " |                         If `calculate_probabilities` in BERTopic is set to False, then the\n",
      " |                         probabilities are not calculated to speed up computation and\n",
      " |                         decrease memory usage.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic().fit(docs)\n",
      " |      topics, probs = topic_model.transform(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      topics, probs = topic_model.transform(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  update_topics(self, docs: List[str], topics: List[int], n_gram_range: Tuple[int, int] = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None)\n",
      " |      Updates the topic representation by recalculating c-TF-IDF with the new\n",
      " |      parameters as defined in this function.\n",
      " |      \n",
      " |      When you have trained a model and viewed the topics and the words that represent them,\n",
      " |      you might not be satisfied with the representation. Perhaps you forgot to remove\n",
      " |      stop_words or you want to try out a different n_gram_range. This function allows you\n",
      " |      to update the topic representation after they have been formed.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: The topics that were returned when calling either `fit` or `fit_transform`\n",
      " |          n_gram_range: The n-gram range for the CountVectorizer.\n",
      " |          vectorizer_model: Pass in your own CountVectorizer from scikit-learn\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      In order to update the topic representation, you will need to first fit the topic\n",
      " |      model and extract topics from them. Based on these, you can update the representation:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.update_topics(docs, topics, n_gram_range=(2, 3))\n",
      " |      ```\n",
      " |      \n",
      " |      YOu can also use a custom vectorizer to update the representation:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.feature_extraction.text import CountVectorizer\n",
      " |      vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
      " |      topic_model.update_topics(docs, topics, vectorizer_model=vectorizer_model)\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_barchart(self, topics: List[int] = None, top_n_topics: int = 8, n_words: int = 5, width: int = 250, height: int = 250) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a barchart of selected topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize.\n",
      " |          top_n_topics: Only select the top n most frequent topics.\n",
      " |          n_words: Number of words to show in a topic\n",
      " |          width: The width of each figure.\n",
      " |          height: The height of each figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the barchart of selected topics\n",
      " |      simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_barchart()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_barchart()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_distribution(self, probabilities: numpy.ndarray, min_probability: float = 0.015, custom_labels: bool = False, width: int = 800, height: int = 600) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize the distribution of topic probabilities\n",
      " |      \n",
      " |      Arguments:\n",
      " |          probabilities: An array of probability scores\n",
      " |          min_probability: The minimum probability score to visualize.\n",
      " |                           All others are ignored.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                         `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      Make sure to fit the model before and only input the\n",
      " |      probabilities of a single document:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_distribution(probabilities[0])\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_distribution(probabilities[0])\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_documents(self, docs: List[str], topics: List[int] = None, embeddings: numpy.ndarray = None, reduced_embeddings: numpy.ndarray = None, sample: float = None, hide_annotations: bool = False, hide_document_hover: bool = False, custom_labels: bool = False, width: int = 1200, height: int = 750) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize documents and their topics in 2D\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_model: A fitted BERTopic instance.\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: A selection of topics to visualize.\n",
      " |                  Not to be confused with the topics that you get from `.fit_transform`.\n",
      " |                  For example, if you want to visualize only topics 1 through 5:\n",
      " |                  `topics = [1, 2, 3, 4, 5]`.\n",
      " |          embeddings: The embeddings of all documents in `docs`.\n",
      " |          reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
      " |          sample: The percentage of documents in each topic that you would like to keep.\n",
      " |                  Value can be between 0 and 1. Setting this value to, for example,\n",
      " |                  0.1 (10% of documents in each topic) makes it easier to visualize\n",
      " |                  millions of documents as a subset is chosen.\n",
      " |          hide_annotations: Hide the names of the traces on top of each cluster.\n",
      " |          hide_document_hover: Hide the content of the documents when hovering over\n",
      " |                              specific points. Helps to speed up generation of visualization.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_documents(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      Do note that this re-calculates the embeddings and reduces them to 2D.\n",
      " |      The advised and prefered pipeline for using this function is as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      from bertopic import BERTopic\n",
      " |      from umap import UMAP\n",
      " |      \n",
      " |      # Prepare embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
      " |      \n",
      " |      # Train BERTopic\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      \n",
      " |      # Reduce dimensionality of embeddings, this step is optional\n",
      " |      # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
      " |      \n",
      " |      # Run the visualization with the original embeddings\n",
      " |      topic_model.visualize_documents(docs, embeddings=embeddings)\n",
      " |      \n",
      " |      # Or, if you have reduced the original embeddings already:\n",
      " |      topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      <iframe src=\"../../getting_started/visualization/documents.html\"\n",
      " |      style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_heatmap(self, topics: List[int] = None, top_n_topics: int = None, n_clusters: int = None, custom_labels: bool = False, width: int = 800, height: int = 800) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a heatmap of the topic's similarity matrix\n",
      " |      \n",
      " |      Based on the cosine similarity matrix between topic embeddings,\n",
      " |      a heatmap is created showing the similarity between topics.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize.\n",
      " |          top_n_topics: Only select the top n most frequent topics.\n",
      " |          n_clusters: Create n clusters and order the similarity\n",
      " |                      matrix by those clusters.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the similarity matrix of\n",
      " |      topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_heatmap()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_heatmap()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_hierarchical_documents(self, docs: List[str], hierarchical_topics: pandas.core.frame.DataFrame, topics: List[int] = None, embeddings: numpy.ndarray = None, reduced_embeddings: numpy.ndarray = None, sample: Union[float, int] = None, hide_annotations: bool = False, hide_document_hover: bool = True, nr_levels: int = 10, custom_labels: bool = False, width: int = 1200, height: int = 750) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize documents and their topics in 2D at different levels of hierarchy\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children\n",
      " |          topics: A selection of topics to visualize.\n",
      " |                  Not to be confused with the topics that you get from `.fit_transform`.\n",
      " |                  For example, if you want to visualize only topics 1 through 5:\n",
      " |                  `topics = [1, 2, 3, 4, 5]`.\n",
      " |          embeddings: The embeddings of all documents in `docs`.\n",
      " |          reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
      " |          sample: The percentage of documents in each topic that you would like to keep.\n",
      " |                  Value can be between 0 and 1. Setting this value to, for example,\n",
      " |                  0.1 (10% of documents in each topic) makes it easier to visualize\n",
      " |                  millions of documents as a subset is chosen.\n",
      " |          hide_annotations: Hide the names of the traces on top of each cluster.\n",
      " |          hide_document_hover: Hide the content of the documents when hovering over\n",
      " |                              specific points. Helps to speed up generation of visualizations.\n",
      " |          nr_levels: The number of levels to be visualized in the hierarchy. First, the distances\n",
      " |                  in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with\n",
      " |                  equal length. Then, for each list of distances, the merged topics are selected that\n",
      " |                  have a distance less or equal to the maximum distance of the selected list of distances.\n",
      " |                  NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to\n",
      " |                  the length of `hierarchical_topics`.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                         `topic_model.set_topic_labels`.\n",
      " |                         NOTE: Custom labels are only generated for the original\n",
      " |                         un-merged topics.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics)\n",
      " |      ```\n",
      " |      \n",
      " |      Do note that this re-calculates the embeddings and reduces them to 2D.\n",
      " |      The advised and prefered pipeline for using this function is as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      from bertopic import BERTopic\n",
      " |      from umap import UMAP\n",
      " |      \n",
      " |      # Prepare embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
      " |      \n",
      " |      # Train BERTopic and extract hierarchical topics\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      \n",
      " |      # Reduce dimensionality of embeddings, this step is optional\n",
      " |      # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
      " |      \n",
      " |      # Run the visualization with the original embeddings\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings)\n",
      " |      \n",
      " |      # Or, if you have reduced the original embeddings already:\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\"\n",
      " |      style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_hierarchy(self, orientation: str = 'left', topics: List[int] = None, top_n_topics: int = None, custom_labels: bool = False, width: int = 1000, height: int = 600, hierarchical_topics: pandas.core.frame.DataFrame = None, linkage_function: Callable[[scipy.sparse.csr.csr_matrix], numpy.ndarray] = None, distance_function: Callable[[scipy.sparse.csr.csr_matrix], scipy.sparse.csr.csr_matrix] = None, color_threshold: int = 1) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a hierarchical structure of the topics\n",
      " |      \n",
      " |      A ward linkage function is used to perform the\n",
      " |      hierarchical clustering based on the cosine distance\n",
      " |      matrix between topic embeddings.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_model: A fitted BERTopic instance.\n",
      " |          orientation: The orientation of the figure.\n",
      " |                      Either 'left' or 'bottom'\n",
      " |          topics: A selection of topics to visualize\n",
      " |          top_n_topics: Only select the top n most frequent topics\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |                     NOTE: Custom labels are only generated for the original\n",
      " |                     un-merged topics.\n",
      " |          width: The width of the figure. Only works if orientation is set to 'left'\n",
      " |          height: The height of the figure. Only works if orientation is set to 'bottom'\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children.\n",
      " |                              NOTE: The hierarchical topic names are only visualized\n",
      " |                              if both `topics` and `top_n_topics` are not set.\n",
      " |          linkage_function: The linkage function to use. Default is:\n",
      " |                          `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\n",
      " |                          NOTE: Make sure to use the same `linkage_function` as used\n",
      " |                          in `topic_model.hierarchical_topics`.\n",
      " |          distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\n",
      " |                          `lambda x: 1 - cosine_similarity(x)`\n",
      " |                          NOTE: Make sure to use the same `distance_function` as used\n",
      " |                          in `topic_model.hierarchical_topics`.\n",
      " |          color_threshold: Value at which the separation of clusters will be made which\n",
      " |                       will result in different colors for different clusters.\n",
      " |                       A higher value will typically lead in less colored clusters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the hierarchical structure of\n",
      " |      topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_hierarchy()\n",
      " |      ```\n",
      " |      \n",
      " |      If you also want the labels visualized of hierarchical topics,\n",
      " |      run the following:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Extract hierarchical topics and their representations\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      \n",
      " |      # Visualize these representations\n",
      " |      topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_hierarchy()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      <iframe src=\"../../getting_started/visualization/hierarchy.html\"\n",
      " |      style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_term_rank(self, topics: List[int] = None, log_scale: bool = False, custom_labels: bool = False, width: int = 800, height: int = 500) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize the ranks of all terms across all topics\n",
      " |      \n",
      " |      Each topic is represented by a set of words. These words, however,\n",
      " |      do not all equally represent the topic. This visualization shows\n",
      " |      how many words are needed to represent a topic and at which point\n",
      " |      the beneficial effect of adding words starts to decline.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize. These will be colored\n",
      " |                  red where all others will be colored black.\n",
      " |          log_scale: Whether to represent the ranking on a log scale\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the ranks of all words across\n",
      " |      all topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_term_rank()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_term_rank()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      Reference:\n",
      " |      \n",
      " |      This visualization was heavily inspired by the\n",
      " |      \"Term Probability Decline\" visualization found in an\n",
      " |      analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/).\n",
      " |      Reference to that specific analysis can be found\n",
      " |      [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html).\n",
      " |  \n",
      " |  visualize_topics(self, topics: List[int] = None, top_n_topics: int = None, width: int = 650, height: int = 650) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics, their sizes, and their corresponding words\n",
      " |      \n",
      " |      This visualization is highly inspired by LDAvis, a great visualization\n",
      " |      technique typically reserved for LDA.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize\n",
      " |          top_n_topics: Only select the top n most frequent topics\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_topics()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_topics_over_time(self, topics_over_time: pandas.core.frame.DataFrame, top_n_topics: int = None, topics: List[int] = None, normalize_frequency: bool = False, custom_labels: bool = False, width: int = 1250, height: int = 450) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics over time\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics_over_time: The topics you would like to be visualized with the\n",
      " |                            corresponding topic representation\n",
      " |          top_n_topics: To visualize the most frequent topics instead of all\n",
      " |          topics: Select which topics you would like to be visualized\n",
      " |          normalize_frequency: Whether to normalize each topic's frequency individually\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A plotly.graph_objects.Figure including all traces\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics over time, simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_over_time = topic_model.topics_over_time(docs, topics, timestamps)\n",
      " |      topic_model.visualize_topics_over_time(topics_over_time)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics_over_time(topics_over_time)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_topics_per_class(self, topics_per_class: pandas.core.frame.DataFrame, top_n_topics: int = 10, topics: List[int] = None, normalize_frequency: bool = False, custom_labels: bool = False, width: int = 1250, height: int = 900) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics per class\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics_per_class: The topics you would like to be visualized with the\n",
      " |                            corresponding topic representation\n",
      " |          top_n_topics: To visualize the most frequent topics instead of all\n",
      " |          topics: Select which topics you would like to be visualized\n",
      " |          normalize_frequency: Whether to normalize each topic's frequency individually\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A plotly.graph_objects.Figure including all traces\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      To visualize the topics per class, simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_per_class = topic_model.topics_per_class(docs, topics, classes)\n",
      " |      topic_model.visualize_topics_per_class(topics_per_class)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics_per_class(topics_per_class)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(path: str, embedding_model=None) from builtins.type\n",
      " |      Loads the model from the specified path\n",
      " |      \n",
      " |      Arguments:\n",
      " |          path: the location and name of the BERTopic file you want to load\n",
      " |          embedding_model: If the embedding_model was not saved to save space or to load\n",
      " |                           it in from the cloud, you can load it in by specifying it here.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      BERTopic.load(\"my_model\")\n",
      " |      ```\n",
      " |      \n",
      " |      or if you did not save the embedding model:\n",
      " |      \n",
      " |      ```python\n",
      " |      BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_topic_tree(hier_topics: pandas.core.frame.DataFrame, max_distance: float = None, tight_layout: bool = False) -> str\n",
      " |      Extract the topic tree such that it can be printed\n",
      " |      \n",
      " |      Arguments:\n",
      " |          hier_topics: A dataframe containing the structure of the topic tree.\n",
      " |                      This is the output of `topic_model.hierachical_topics()`\n",
      " |          max_distance: The maximum distance between two topics. This value is\n",
      " |                      based on the Distance column in `hier_topics`.\n",
      " |          tight_layout: Whether to use a tight layout (narrow width) for\n",
      " |                      easier readability if you have hundreds of topics.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tree that has the following structure when printed:\n",
      " |              .\n",
      " |              .\n",
      " |              └─health_medical_disease_patients_hiv\n",
      " |                  ├─patients_medical_disease_candida_health\n",
      " |                  │    ├─■──candida_yeast_infection_gonorrhea_infections ── Topic: 48\n",
      " |                  │    └─patients_disease_cancer_medical_doctor\n",
      " |                  │         ├─■──hiv_medical_cancer_patients_doctor ── Topic: 34\n",
      " |                  │         └─■──pain_drug_patients_disease_diet ── Topic: 26\n",
      " |                  └─■──health_newsgroup_tobacco_vote_votes ── Topic: 9\n",
      " |      \n",
      " |          The blocks (■) indicate that the topic is one you can directly access\n",
      " |          from `topic_model.get_topic`. In other words, they are the original un-grouped topics.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Train model\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, topics)\n",
      " |      \n",
      " |      # Print topic tree\n",
      " |      tree = topic_model.get_topic_tree(hierarchical_topics)\n",
      " |      print(tree)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>241567</td>\n",
       "      <td>-1_plant_plants_species_growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>0_allergen_allergens_pollen_ige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3976</td>\n",
       "      <td>1_medium_callus_regeneration_culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1111</td>\n",
       "      <td>2_dots_fluorescence_detection_carbon dots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>859</td>\n",
       "      <td>3_glyphosate_herbicide_resistance_herbicides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>85</td>\n",
       "      <td>825</td>\n",
       "      <td>85_soil_yield_nitrogen_fertilizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>86</td>\n",
       "      <td>567</td>\n",
       "      <td>86_inbreeding_depression_inbreeding depression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>87</td>\n",
       "      <td>2828</td>\n",
       "      <td>87_pollen_pollination_flowers_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>1849</td>\n",
       "      <td>88_populations_genetic_diversity_genetic diver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>89</td>\n",
       "      <td>1107</td>\n",
       "      <td>89_clade_phylogenetic_species_genera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic   Count                                               Name\n",
       "0      -1  241567                     -1_plant_plants_species_growth\n",
       "1       0     919                    0_allergen_allergens_pollen_ige\n",
       "2       1    3976               1_medium_callus_regeneration_culture\n",
       "3       2    1111          2_dots_fluorescence_detection_carbon dots\n",
       "4       3     859       3_glyphosate_herbicide_resistance_herbicides\n",
       "..    ...     ...                                                ...\n",
       "86     85     825                  85_soil_yield_nitrogen_fertilizer\n",
       "87     86     567  86_inbreeding_depression_inbreeding depression...\n",
       "88     87    2828               87_pollen_pollination_flowers_floral\n",
       "89     88    1849  88_populations_genetic_diversity_genetic diver...\n",
       "90     89    1107               89_clade_phylogenetic_species_genera\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Assign outliers to topics___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine probability distributions of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((421658, 90), (421658,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, probs[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.216580e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.686914e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.724943e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.414574e-308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.323693e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.881805e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.384614e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count   4.216580e+05\n",
       "mean    4.686914e-03\n",
       "std     3.724943e-02\n",
       "min    4.414574e-308\n",
       "25%     1.323693e-03\n",
       "50%     2.881805e-03\n",
       "75%     4.384614e-03\n",
       "max     1.000000e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(probs[:,0], log_scale=True)\n",
    "plt.xlim(1e-5, 1e-1)\n",
    "pd.DataFrame(probs[:,0]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.216580e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.411141e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.244737e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.014457e-308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.727241e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.033303e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.625952e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count   4.216580e+05\n",
       "mean    6.411141e-03\n",
       "std     4.244737e-02\n",
       "min    5.014457e-308\n",
       "25%     1.727241e-03\n",
       "50%     4.033303e-03\n",
       "75%     5.625952e-03\n",
       "max     1.000000e+00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(probs[:,1], log_scale=True)\n",
    "plt.xlim(1e-5, 1e-1)\n",
    "pd.DataFrame(probs[:,1]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>241567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>11209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>8942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>7685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>6913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>30</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>25</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>78</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>38</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic   Count\n",
       "0      -1  241567\n",
       "1      61   11209\n",
       "2      12    8942\n",
       "3      69    7685\n",
       "4      35    6913\n",
       "..    ...     ...\n",
       "86     30     522\n",
       "87     25     508\n",
       "88     78     506\n",
       "89     38     503\n",
       "90      4     502\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_freq = topic_model.get_topic_freq()\n",
    "topic_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignments\n",
    "\n",
    "With probability treshold of:\n",
    "- 0.0067 (~75 percentiles): topic(-1)=34622\n",
    "- 0.0155 (~95 percentiles): topic(-1)=49228 <-- go with this...\n",
    "  - At this threshold, 11.7% of the documents are not assigned to topic.\n",
    "- 0.0434 (~99 percentiles): topic(-1)=124648 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006735027420849654, 0.015512210159378426, 0.04337546078552455)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(probs, 75), np.percentile(probs, 95), np.percentile(probs, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = np.percentile(probs, 99)\n",
    "new_topics = [np.argmax(prob) if max(prob) >= probability_threshold else -1 \n",
    "                                                            for prob in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1     124648\n",
       " 61     15518\n",
       " 35     11032\n",
       " 69      9268\n",
       " 12      9226\n",
       "        ...  \n",
       " 57       826\n",
       " 21       755\n",
       " 66       670\n",
       " 3        650\n",
       " 5        553\n",
       "Length: 91, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(new_topics).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11674864463617433"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49228/len(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~8 min\n",
    "topic_model.update_topics(docs_clean, new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(work_dir / 'topic_model_updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>241567</td>\n",
       "      <td>-1_plant_plants_genes_cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>0_allergen_allergens_pollen_ige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3976</td>\n",
       "      <td>1_medium_callus_regeneration_mgl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1111</td>\n",
       "      <td>2_dots_fluorescence_detection_carbon dots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>859</td>\n",
       "      <td>3_glyphosate_resistance_herbicide_herbicides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>85</td>\n",
       "      <td>825</td>\n",
       "      <td>85_soil_yield_nitrogen_water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>86</td>\n",
       "      <td>567</td>\n",
       "      <td>86_populations_genetic_selection_inbreeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>87</td>\n",
       "      <td>2828</td>\n",
       "      <td>87_pollen_pollination_flowers_floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>1849</td>\n",
       "      <td>88_populations_genetic_population_species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>89</td>\n",
       "      <td>1107</td>\n",
       "      <td>89_phylogenetic_species_phylogeny_clade</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic   Count                                          Name\n",
       "0      -1  241567                    -1_plant_plants_genes_cell\n",
       "1       0     919               0_allergen_allergens_pollen_ige\n",
       "2       1    3976              1_medium_callus_regeneration_mgl\n",
       "3       2    1111     2_dots_fluorescence_detection_carbon dots\n",
       "4       3     859  3_glyphosate_resistance_herbicide_herbicides\n",
       "..    ...     ...                                           ...\n",
       "86     85     825                  85_soil_yield_nitrogen_water\n",
       "87     86     567   86_populations_genetic_selection_inbreeding\n",
       "88     87    2828          87_pollen_pollination_flowers_floral\n",
       "89     88    1849     88_populations_genetic_population_species\n",
       "90     89    1107       89_phylogenetic_species_phylogeny_clade\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info_updated = topic_model.get_topic_info()\n",
    "topic_info_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch_default': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4255f477c905e3cafd6d08b9a6d118445dbfbaff982fd1d9831280a79a13df35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
