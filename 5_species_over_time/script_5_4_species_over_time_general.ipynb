{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 5.4: Species over time for general use__\n",
    "\n",
    "This was modified from script_5_1 for general use.\n",
    "\n",
    "Goals here:\n",
    "- Determine overall genus mention\n",
    "- Determine genus mention over time\n",
    "- Same analysis at other taxonmic levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, nltk, re, multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, lil_matrix, coo_matrix, dok_matrix\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import OrderedDict, Counter\n",
    "from bisect import bisect\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shinhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shinhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/shinhan/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting working directory\n",
    "proj_dir   = Path.home() / \"projects/plant_sci_hist\"\n",
    "work_dir   = proj_dir / \"5_species_over_time/5_1_sp_time\"\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# species information\n",
    "# NEED TO SPECIFY\n",
    "# NEED TO SPECIFY\n",
    "dir1           = proj_dir / \"1_obtaining_corpus\"\n",
    "usda_plant_db  = dir1 / \"usda/USDA_Plants_Database.txt\"\n",
    "\n",
    "names_dmp_path = work_dir / \"taxonomy/names.dmp\"\n",
    "nodes_dmp_path = work_dir / \"taxonomy/nodes.dmp\"\n",
    "\n",
    "# plant science corpus with date and other info\n",
    "dir2        = proj_dir / \"2_text_classify/2_5_predict_pubmed\"\n",
    "corpus_file = dir2 / \"corpus_plant_421658.tsv.gz\"\n",
    "\n",
    "# timestamp bins\n",
    "dir44            = proj_dir / \"4_topic_model/4_4_over_time\"\n",
    "ts_for_bins_file = dir44 / \"table4_4_bin_timestamp_date.tsv\"\n",
    "\n",
    "# So PDF is saved in a format properly\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Get plant names___\n",
    "\n",
    "In `1_obtaining_corpus`, plant names are from two sources:\n",
    "- NCBI: the taxonomy database with mention of all taxa levels\n",
    "  - This will also contain synonyms for different levels.\n",
    "- USDA: plant common names with species information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI taxonomy\n",
    "\n",
    "Functions modified from:\n",
    "- `1_obtaining_corpus\\script_get_plant_taxa.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse name_dmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_dict(names_dmp_path, target):\n",
    "  '''Get the tax_id of Viridiplantae and generate a dictionary.\n",
    "  Args:\n",
    "    names_dmp_file - The Path object to the names.dmp file from NCBI taxonomy.\n",
    "    target - Target taxon name.\n",
    "  Return:\n",
    "    target_id - The NCBI taxon ID for the taxon.\n",
    "    names_dic - A dictionary with: {tax_id:{name_class:[names]}\n",
    "  '''\n",
    "  target_id = \"\"\n",
    "  names_dmp = open(names_dmp_path)\n",
    "  L         = names_dmp.readline()\n",
    "  names_dic = {}\n",
    "  while L != \"\":\n",
    "    L = L.strip().split(\"\\t\")\n",
    "    tax_id = L[0]\n",
    "    name   = L[2]\n",
    "    name_c = L[6]\n",
    "    if L[2] == target:\n",
    "      print(f\"{target} tax_id:\",tax_id)\n",
    "      target_id = tax_id\n",
    "\n",
    "    if tax_id not in names_dic:\n",
    "      names_dic[tax_id] = {name_c:[name]}\n",
    "    elif name_c not in names_dic[tax_id]:\n",
    "      names_dic[tax_id][name_c] = [name]\n",
    "    else:\n",
    "      names_dic[tax_id][name_c].append(name)\n",
    "    L = names_dmp.readline()\n",
    "  return target_id, names_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viridiplantae tax_id: 33090\n"
     ]
    }
   ],
   "source": [
    "viridi_id, viridi_names_dic = get_name_dict(names_dmp_path, 'Viridiplantae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'authority': ['Chlorobionta Jeffrey, 1982',\n",
       "   'Chloroplastida Adl et al. 2005',\n",
       "   'Viridiplantae Cavalier-Smith, 1981'],\n",
       "  'synonym': ['Chlorobionta', 'Chloroplastida'],\n",
       "  'equivalent name': ['Chlorophyta/Embryophyta group',\n",
       "   'chlorophyte/embryophyte group'],\n",
       "  'blast name': ['green plants'],\n",
       "  'common name': ['green plants'],\n",
       "  'scientific name': ['Viridiplantae']},\n",
       " {'authority': ['Stipeae Dumort., 1824'], 'scientific name': ['Stipeae']})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viridi_names_dic['33090'], viridi_names_dic['147383']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse node_dmp\n",
    "\n",
    "This is to get:\n",
    "- Parent-child relation\n",
    "- Child-parent relation\n",
    "- Rank count:\n",
    "- Taxa_id-rank relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_child(nodes_dmp_path):\n",
    "    '''Get the parent-child relationships from nodes.dmp file.\n",
    "    Args:\n",
    "      nodes_dmp_path - The Path object to the nodes.dmp file from NCBI taxonomy.\n",
    "    Return:\n",
    "      parent_child - A dictionary with {parent:[children]}\n",
    "    '''\n",
    "    nodes_dmp    = open(nodes_dmp_path)\n",
    "    L            = nodes_dmp.readline()\n",
    "    rank_d       = {} # {rank: count}\n",
    "    taxa_rank    = {} # {taxa_id: rank}\n",
    "    rank_taxa    = {} # {rank: taxa_id}\n",
    "    parent_child = {}\n",
    "    child_parent = {}\n",
    "    target_ranks = ['genus', 'family', 'order']\n",
    "\n",
    "    debug_count  = 0\n",
    "    debug_list   = []\n",
    "    while L != \"\":\n",
    "        L = L.strip().split(\"\\t\")\n",
    "        tax_id = L[0]\n",
    "        par_id = L[2]\n",
    "        rank   = L[4]\n",
    "        if rank not in rank_d:\n",
    "            rank_d[rank] = 1\n",
    "        else:\n",
    "            rank_d[rank]+= 1\n",
    "        \n",
    "        # Don't want any species or taxon with no rank\n",
    "        # 9/20/22: actually, do not want no rank result in problem. Am example\n",
    "        #   is taxid=2822797, child of 147368, this lead to some taxa missing.\n",
    "        #   so removed.\n",
    "        #if rank not in [\"no rank\", \"species\"]:\n",
    "        if rank != \"species\":\n",
    "            # debug\n",
    "            if par_id == '147383':\n",
    "                debug_count += 1\n",
    "                debug_list.append(\n",
    "                                viridi_names_dic[tax_id]['scientific name'][0])\n",
    "                #print(debug_count, tax_id, names_dic[tax_id]['scientific name'])\n",
    "\n",
    "            # populate parent_child dict\n",
    "            if par_id not in parent_child:\n",
    "                parent_child[par_id] = [tax_id]\n",
    "            else:\n",
    "                parent_child[par_id].append(tax_id)\n",
    "            \n",
    "             # populate child_parent dict\n",
    "            if tax_id not in child_parent:\n",
    "                child_parent[tax_id] = par_id\n",
    "            else:\n",
    "                print(f\"ERR: {tax_id} with >1 parents\",\n",
    "                        child_parent[tax_id], par_id)\n",
    "            \n",
    "            # populate taxa_rank and rank_taxa dicts\n",
    "            taxa_rank[tax_id] = rank\n",
    "            \n",
    "            if rank not in rank_taxa:\n",
    "                rank_taxa[rank] = [tax_id]\n",
    "            else:\n",
    "                rank_taxa[rank].append(tax_id)\n",
    "            \n",
    "        L = nodes_dmp.readline()\n",
    "        \n",
    "    return parent_child, child_parent, rank_d, taxa_rank, rank_taxa, debug_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child, child_parent, rank_d, taxa_rank, rank_taxa, debug_list = \\\n",
    "                                              get_parent_child(nodes_dmp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get offsprings of Viridiplantae\n",
    "\n",
    "These are the names to search for, after adding the USDA names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsprings(p, parent_child, offsprings, debug=0):\n",
    "    '''Get the offsprings of a parent.\n",
    "    Args:\n",
    "        p - The parent taxa ID to get children for.\n",
    "        paren_child - The dictionary returned from get_parent_child().\n",
    "        offsprings - An initially empty list to append offspring IDs.\n",
    "    Return:\n",
    "        offsprings - The populated offspring list.\n",
    "    '''\n",
    "    if debug:\n",
    "        print(p)\n",
    "    if p in parent_child:\n",
    "        # Initialize c with an empty element for debugging purpose\n",
    "        #c = [\"\"]\n",
    "        c = parent_child[p]\n",
    "        if debug:\n",
    "            print(\"\",p, c)\n",
    "            if p == \"147383\":\n",
    "                print(\"debug parent found\")\n",
    "\n",
    "        offsprings.extend(c)\n",
    "        for a_c in c:\n",
    "            get_offsprings(a_c, parent_child, offsprings)\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\" NO CHILD\")\n",
    "    return offsprings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(alist):\n",
    "  '''Check if there is any duplicate name'''\n",
    "  dup = [item for item, count in Counter(alist).items() if count > 1]\n",
    "  print(\"duplicated:\", dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num viridi_offspr_names: 26782\n",
      "duplicated: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "viridi_offspr_names_file = work_dir / \"viridiplantae_offspring_names.pickle\"\n",
    "\n",
    "if not viridi_offspr_names_file.is_file():\n",
    "    viridi_offspr = get_offsprings(viridi_id, parent_child, [])\n",
    "    len(viridi_offspr)\n",
    "    # Convert taxa id into scientific names\n",
    "    viridi_offspr_names = []\n",
    "    redun = {}\n",
    "    for o in viridi_offspr:\n",
    "        if o in viridi_names_dic:\n",
    "            for nc in viridi_names_dic[o]: # for each name_class\n",
    "                if nc != 'authority': \n",
    "                    for name in viridi_names_dic[o][nc]:\n",
    "                        if name not in redun:\n",
    "                            viridi_offspr_names.append(name)\n",
    "                            redun[name] = 0\n",
    "                        #else:\n",
    "                        #    print(\"Redun:\", name)\n",
    "\n",
    "    # Note that this number is larger than offspring_33090 which contain indicies\n",
    "    # This is because there are other names, like synonyms for each index.\n",
    "    print(\"Num viridi_offspr_names:\", len(viridi_offspr_names))\n",
    "    check_duplicate(viridi_offspr_names)\n",
    "\n",
    "    # Save as pickle\n",
    "    with open(viridi_offspr_names_file, \"wb\") as f:\n",
    "        pickle.dump(viridi_offspr_names, f)\n",
    "else:\n",
    "    print(\"load viridi_offspr_names\")\n",
    "    with open(viridi_offspr_names_file, \"rb\") as f:\n",
    "        viridi_offspr_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USDA common names\n",
    "\n",
    "Functions modified from:\n",
    "- `1_obtaining_corpus\\script_get_plant_common_names.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common name file\n",
    "cnames_file = work_dir / \"usda_common_names_dict.pickle\"\n",
    "\n",
    "if not cnames_file.is_file():\n",
    "    cnames = {} # {common_name:[scientific name, family]}\n",
    "\n",
    "    with open(usda_plant_db) as f:\n",
    "        f.readline() # header, don't need it\n",
    "        L = f.readline()\n",
    "        while L != \"\":\n",
    "            L = L.strip()\n",
    "            # There is empty line in the file.\n",
    "            if L == \"\":\n",
    "                break\n",
    "            #print(L.split(\",\"))\n",
    "            try:\n",
    "                # some names have \",\" in there. So need to split with \"\"\\,\"\n",
    "                [symbol, synonym, sname, cname, fam] = L.split(\"\\\",\")\n",
    "            except ValueError:\n",
    "                print(\"ValueError:\",[L])\n",
    "                break\n",
    "            # rid of quotes\n",
    "            [symbol, synonym, sname, cname, fam] = [symbol.split(\"\\\"\")[1], \n",
    "                                                    synonym.split(\"\\\"\")[1], \n",
    "                                                    sname.split(\"\\\"\")[1], \n",
    "                                                    cname.split(\"\\\"\")[1], \n",
    "                                                    fam.split(\"\\\"\")[1]]\n",
    "            #print([symbol, synonym, sname, cname, fam])\n",
    "            # Get genus name out\n",
    "            genus = sname.split(\" \")[0]\n",
    "            if cname != \"\":\n",
    "                if cname not in cnames:\n",
    "                    cnames[cname] = [genus, fam]\n",
    "                #else:\n",
    "                #    print(\"Redun cname:\", [cname], cnames[cname], [sname,fam])        \n",
    "            L = f.readline()\n",
    "    # Save as pickle\n",
    "    with open(cnames_file, \"wb\") as f:\n",
    "        pickle.dump(cnames, f)\n",
    "\n",
    "else:\n",
    "    print(\"load cnames\")\n",
    "    with open(cnames_file, \"rb\") as f:\n",
    "        cnames = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rid of USDA names not found in the NCBI list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all USDA genus names are found in NCBI\n",
    "# This helped identified issues with the parent_child script and missing data\n",
    "# due to the use of older NCBI taxa dump file. Currently, missings ones are \n",
    "# fungal and are excluded.\n",
    "cnames_overlap = {}\n",
    "for cname in cnames:\n",
    "  genus = cnames[cname][0]\n",
    "  if genus in viridi_offspr_names:\n",
    "    cnames_overlap[cname] = genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_names = list(cnames_overlap.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Find names in corpus___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on Mauro Di Pietro (2020):\n",
    "#  https://towardsdatascience.com/text-classification-with-no-model-training-935fe0e42180\n",
    "# For the purpose here, did not do lower-casing\n",
    "def utils_preprocess_text(text, lst_stopwords, flg_stemm=False, flg_lemm=True):\n",
    "    '''\n",
    "    Preprocess a string.\n",
    "    :parameter\n",
    "        :param text: string - name of column containing text\n",
    "        :param lst_stopwords: list - list of stopwords to remove\n",
    "        :param flg_stemm: bool - whether stemming is to be applied\n",
    "        :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    "    :return\n",
    "        cleaned text\n",
    "    '''\n",
    "    ## clean: stripping, then removing punctuations.\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # RE: replace any character that is not alphanumeric, underscore, whitespace\n",
    "    #  with ''. Originally this is it, but realized that biological terms have\n",
    "    #  special characters including roman numerals, dash, and \",\". So they are\n",
    "    #  not removed.\n",
    "    #text = re.sub(r'[^\\w\\s(α-ωΑ-Ω)-,]', '', text)\n",
    "    # Use the original method\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    \n",
    "    \n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(corpus_file, compression='gzip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean text: 100%|██████████| 421658/421658 [04:07<00:00, 1704.20it/s]\n"
     ]
    }
   ],
   "source": [
    "txt_clean_file = work_dir / \"txt_clean.pickle\"\n",
    "\n",
    "if not txt_clean_file.is_file():\n",
    "  tqdm.pandas(desc=\"Clean text\")\n",
    "  lst_stopwords    = nltk.corpus.stopwords.words(\"english\")\n",
    "  txt_clean_values = corpus[\"txt\"].progress_apply(lambda x: \n",
    "                                        utils_preprocess_text(x, lst_stopwords))\n",
    "  with open(txt_clean_file, \"wb\") as f:\n",
    "    pickle.dump(txt_clean_values, f)              \n",
    "else:\n",
    "  print(\"load cleaned txt...\")\n",
    "  with open(txt_clean_file, \"rb\") as f:\n",
    "    txt_clean_values = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>QualifiedName</th>\n",
       "      <th>txt</th>\n",
       "      <th>reg_article</th>\n",
       "      <th>y_prob</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>txt_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363373</th>\n",
       "      <td>1295650</td>\n",
       "      <td>30780667</td>\n",
       "      <td>2007-09-01</td>\n",
       "      <td>Plant disease</td>\n",
       "      <td>First Report of the Anamorph of Glomerella acu...</td>\n",
       "      <td>Mexico is a major avocado (Persea americana) p...</td>\n",
       "      <td>avocado</td>\n",
       "      <td>First Report of the Anamorph of Glomerella acu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878942</td>\n",
       "      <td>1</td>\n",
       "      <td>First Report Anamorph Glomerella acutata Causi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212063</th>\n",
       "      <td>840918</td>\n",
       "      <td>23316114</td>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>Molecular breeding : new strategies in plant i...</td>\n",
       "      <td>Quantitative trait loci pyramiding for fruit q...</td>\n",
       "      <td>Fruit quality is a major focus for most conven...</td>\n",
       "      <td>plants</td>\n",
       "      <td>Quantitative trait loci pyramiding for fruit q...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843479</td>\n",
       "      <td>1</td>\n",
       "      <td>Quantitative trait locus pyramiding fruit qual...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      PMID        Date  \\\n",
       "363373     1295650  30780667  2007-09-01   \n",
       "212063      840918  23316114  2013-01-15   \n",
       "\n",
       "                                                  Journal  \\\n",
       "363373                                      Plant disease   \n",
       "212063  Molecular breeding : new strategies in plant i...   \n",
       "\n",
       "                                                    Title  \\\n",
       "363373  First Report of the Anamorph of Glomerella acu...   \n",
       "212063  Quantitative trait loci pyramiding for fruit q...   \n",
       "\n",
       "                                                 Abstract QualifiedName  \\\n",
       "363373  Mexico is a major avocado (Persea americana) p...       avocado   \n",
       "212063  Fruit quality is a major focus for most conven...        plants   \n",
       "\n",
       "                                                      txt  reg_article  \\\n",
       "363373  First Report of the Anamorph of Glomerella acu...            1   \n",
       "212063  Quantitative trait loci pyramiding for fruit q...            1   \n",
       "\n",
       "          y_prob  y_pred                                          txt_clean  \n",
       "363373  0.878942       1  First Report Anamorph Glomerella acutata Causi...  \n",
       "212063  0.843479       1  Quantitative trait locus pyramiding fruit qual...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"txt_clean\"] = txt_clean_values\n",
    "corpus.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421658, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find names and load csr\n",
    "\n",
    "- Update csr values\n",
    "  - https://stackoverflow.com/questions/56981077/how-to-update-value-in-csr-matrix\n",
    "- See code testing section on the different functions tried\n",
    "- Move the code from to get matches `script_5_1a_find_names.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_csr(txt):\n",
    "  print(txt.shape)\n",
    "  with multiprocessing.Pool(processes=15) as pool:\n",
    "    results_ncbi_list = list(tqdm(pool.imap(task, enumerate(txt), chunksize=1), \n",
    "                                  total=len(txt)))\n",
    "\n",
    "  row_idx   = []\n",
    "  col_idx   = []\n",
    "  csr_val   = []\n",
    "  for row, results_ncbi in enumerate(results_ncbi_list):\n",
    "    non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    row_idx.extend([row]*len(non0_idx))\n",
    "    col_idx.extend(non0_idx)\n",
    "    csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "  # create a sparse matrix with shape=(num_docs, num_names)\n",
    "  match_csr = csr_matrix((csr_val, (row_idx, col_idx)),\n",
    "                         shape=(txt.shape[0], len(offspring_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  return match_csr\n",
    "\n",
    "def task(item):\n",
    "  '''Task to parallelize\n",
    "  Args:\n",
    "    item (tuple): (row_number, doc)\n",
    "  Return:\n",
    "    results_ncbi (list): an offspring_name is present in the doc (1) or not(1)\n",
    "  '''\n",
    "  (row, doc) = item\n",
    "\n",
    "  # pad the doc so if qualified name is at the beginning or end will still match\n",
    "  doc = f\" {doc} \" \n",
    "  # Get the matching common names as a list\n",
    "  # Get lower case because the common name can be the 1st word.\n",
    "  doc_lower = doc.lower()\n",
    "  results_usda = [name for name in common_names if(f\" {name} \" in doc_lower)]\n",
    "\n",
    "  # Add the results to doc\n",
    "  for cname in results_usda:  # for each common name\n",
    "    genus = cnames[cname][0]  # get the genus name\n",
    "    doc += f\" {genus}\"        # add the genus name to doc\n",
    "  \n",
    "  # Match to NCBI names\n",
    "  results_ncbi = [1 if(f\" {name} \" in doc) else 0 for name in offspring_names]\n",
    "\n",
    "  return results_ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read saved objects...\n"
     ]
    }
   ],
   "source": [
    "print(\"Read saved objects...\")\n",
    "'''\n",
    "txt_clean  = pd.read_csv(work_dir / \"txt_clean.csv\", index_col=0)\n",
    "\n",
    "with open(work_dir / \"viridiplantae_offspring_names.pickle\", \"rb\") as f:\n",
    "  offspring_names = pickle.load(f)\n",
    "\n",
    "# Save as pickle\n",
    "with open(work_dir / \"usda_common_names.pickle\", \"rb\") as f:\n",
    "  common_names = pickle.load(f)\n",
    "\n",
    "with open(work_dir / \"usda_common_names_dict.pickle\", \"rb\") as f:\n",
    "  cnames = pickle.load(f)\n",
    "'''\n",
    "txt_clean       = txt_clean_values\n",
    "offspring_names = viridi_offspr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get match_csr...\n",
      " [0, 50000)\n",
      "(50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [02:51<00:00, 291.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [50000, 100000)\n",
      "(50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [03:00<00:00, 277.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [100000, 150000)\n",
      "(50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [02:59<00:00, 278.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [150000, 200000)\n",
      "(50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 15766/50000 [00:57<02:04, 274.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bertopic/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    854\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb Cell 36\u001b[0m line \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m   txt        \u001b[39m=\u001b[39m txt_clean[idx:(idx\u001b[39m+\u001b[39mn_subset)]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39m# get csr\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m   match_csr  \u001b[39m=\u001b[39m get_match_csr(txt)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m   csr_list\u001b[39m.\u001b[39mappend(match_csr)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# stack csr\u001b[39;00m\n",
      "\u001b[1;32m/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb Cell 36\u001b[0m line \u001b[0;36mget_match_csr\u001b[0;34m(txt)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(txt\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m multiprocessing\u001b[39m.\u001b[39mPool(processes\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m   results_ncbi_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(tqdm(pool\u001b[39m.\u001b[39;49mimap(task, \u001b[39menumerate\u001b[39;49m(txt), chunksize\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                                 total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(txt)))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m row_idx   \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/shinhan/github/plant_sci_hist/5_species_over_time/script_5_4_species_over_time_general.ipynb#Y513sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m col_idx   \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/bertopic/lib/python3.9/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1180\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1181\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1182\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bertopic/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 858\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    859\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/miniconda3/envs/bertopic/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Get match_csr...\")\n",
    "\n",
    "# Send a subset docs at a time so no memtory issue\n",
    "n_subset = 50000\n",
    "csr_list = []\n",
    "for idx in range(0, txt_clean.shape[0], n_subset):\n",
    "  print(f\" [{idx}, {idx+n_subset})\")\n",
    "  # get subset of docs\n",
    "  txt        = txt_clean[idx:(idx+n_subset)]\n",
    "  # get csr\n",
    "  match_csr  = get_match_csr(txt)\n",
    "  csr_list.append(match_csr)\n",
    "\n",
    "# stack csr\n",
    "match_csr_all = vstack(csr_list)\n",
    "print(\"Final csr:\", match_csr_all.shape)\n",
    "\n",
    "with open(work_dir / \"match_csr.pickle\", \"wb\") as f:\n",
    "  pickle.dump(match_csr_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved csr\n",
    "with open(work_dir / \"match_csr.pickle\", \"rb\") as f:\n",
    "  match_csr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_csr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Time bins___\n",
    "\n",
    "Based on codes from `script_4_4_over_time.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get binned timestamp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = corpus['Date']\n",
    "\n",
    "# Turn all dates into timestamps \n",
    "ts_for_bins = []\n",
    "for date in dates:\n",
    "  [yr, mo, da] = date.split('-') # year, month, day\n",
    "  dt   = datetime(int(yr), int(mo), int(da))\n",
    "  ts   = dt.timestamp()\n",
    "  ts_for_bins.append(ts)\n",
    "ts_for_bins.sort()\n",
    "\n",
    "# bin size\n",
    "bin_size       = int(len(ts_for_bins)/50)\n",
    "\n",
    "# index values of every 2 percentile (because the data is broken into 50 parts)\n",
    "bin_idxs       = [idx for idx in range(0, len(ts_for_bins), bin_size)]\n",
    "\n",
    "# timestamp values at bin_idxs\n",
    "bin_timestamps = [ts_for_bins[idx] for idx in bin_idxs]\n",
    "\n",
    "# Modify the last value to be the max timestamp value + 1. This is otherwise\n",
    "# because of the bin_size is rounded down the last value be smaller than the max\n",
    "# timestamp values. Also, +1 to the max value, otherwise, the last entries will\n",
    "# be in its own bin.\n",
    "max_timestamp      = max(ts_for_bins) + 1\n",
    "bin_timestamps[-1] = max_timestamp\n",
    "\n",
    "# dates correspond to the different timestamp\n",
    "bin_dates = [datetime.fromtimestamp(ts) for ts in bin_timestamps]\n",
    "\n",
    "# Put idx, timestamp, and date into a dataframe and save it.\n",
    "bin_df    = pd.DataFrame(list(zip(bin_idxs, bin_timestamps, bin_dates)),\n",
    "            columns=['bin_start_idx', 'bin_start_timestamp', 'bin_start_date'])\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the binned timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new timestamps based on the bin timestamp values\n",
    "ts_in_bins = []\n",
    "for date in dates:\n",
    "  [yr, mo, da] = date.split('-') # year, month, day\n",
    "  dt   = datetime(int(yr), int(mo), int(da))\n",
    "  ts   = dt.timestamp()\n",
    "\n",
    "  bin_idx = bisect(bin_timestamps, ts)\n",
    "\n",
    "  if bin_idx < len(bin_timestamps):\n",
    "    ts2     = bin_timestamps[bin_idx]\n",
    "  # Deal with the last bin\n",
    "  else:\n",
    "    ts2     = datetime(2022, 12, 31).timestamp()\n",
    "  ts_in_bins.append(ts2) \n",
    "\n",
    "print(len(ts_in_bins), ts_in_bins[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include timestamps and bin range info in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are binned values\n",
    "timestamps = ts_in_bins\n",
    "\n",
    "# Create a new dataframe with PMID, date, txt_clean, bin, and bin_left\n",
    "documents = pd.DataFrame({\"PMID\": corpus['PMID'], \"Date\": corpus['Date'], \n",
    "                          \"txt_clean\": corpus['txt_clean'], \n",
    "                          \"Timestamps\":timestamps})\n",
    "documents = documents.sort_values(\"Timestamps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of tuples showing the bin range (+/-1 of the unique val)\n",
    "ts_bins   = [pd.Interval(left=ts-1, right=ts+1) for ts in timestamps] \n",
    "\n",
    "documents[\"Bins\"]      = ts_bins\n",
    "documents[\"Bins_left\"] = documents.apply(lambda row: row.Bins.left, 1)\n",
    "documents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.statology.org/pandas-select-rows-by-index/\n",
    "documents.loc[242511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_unique = documents.Bins_left.unique()\n",
    "ts_unique.sort()\n",
    "len(ts_unique), ts_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Genus level counts___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get genus level tax_id and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of genus tax_ids\n",
    "genus_taxids = rank_taxa[\"genus\"]\n",
    "len(genus_taxids), genus_taxids[:3], check_duplicate(genus_taxids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tax_ids to scientific names, note that this contain ALL genus names\n",
    "# not restricted to those in viridiplantae\n",
    "genus_names_dict = {viridi_names_dic[tax_id]['scientific name'][0]:1\n",
    "                    for tax_id in genus_taxids}\n",
    "genus_names = list(genus_names_dict.keys())\n",
    "len(genus_names), genus_names[:3], check_duplicate(genus_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get match_csr column index for genus names: there are 17512 genus names in\n",
    "# among Viridiplantae taxa\n",
    "\n",
    "# convert list to dict for more efficient operation\n",
    "viridi_offspr_dict = {name:1 for name in viridi_offspr_names}\n",
    "\n",
    "#https://stackoverflow.com/questions/50756085/how-to-print-the-progress-of-a-list-comprehension-in-python\n",
    "genus_csr_idx   = [viridi_offspr_names.index(name) for name in tqdm(genus_names)\n",
    "                   if name in viridi_offspr_dict]\n",
    "len(genus_csr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also compile the names of genus in Viridiplantae\n",
    "viridi_genus_names = [name for name in tqdm(genus_names)\n",
    "                         if name in viridi_offspr_dict]\n",
    "len(viridi_genus_names), check_duplicate(viridi_genus_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the genus sub-csr\n",
    "genus_csr = match_csr[:, genus_csr_idx]\n",
    "genus_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumped for later use in 5_3\n",
    "\n",
    "# output match_csr for genus\n",
    "with open(work_dir / \"match_csr_genus.pickle\", \"wb\") as f:\n",
    "  pickle.dump(genus_csr, f)\n",
    "\n",
    "# output genus name in the same order as match_csr_genus\n",
    "with open(work_dir / \"match_csr_genus_names.pickle\", \"wb\") as f:\n",
    "  pickle.dump(viridi_genus_names, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total of reach genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/3337301/numpy-matrix-to-array\n",
    "col_sum = np.squeeze(np.asarray(genus_csr.sum(axis=0)))\n",
    "col_sum[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(viridi_genus_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_count_file = work_dir / \"Table_5_1_genus_count.txt\"\n",
    "\n",
    "if not genus_count_file.is_file():\n",
    "  genus_count_df = pd.DataFrame(list(zip(viridi_genus_names, col_sum)),\n",
    "                                columns=(\"Taxa names\", \"Total\"))\n",
    "  genus_count_df.to_csv(genus_count_file, sep='\\t')\n",
    "else:\n",
    "  print(\"read genus_count_df...\")\n",
    "  genus_count_df = pd.read_csv(genus_count_file, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove certain genus\n",
    "\n",
    "- Matthiola: common name=stock\n",
    "- California: location issue\n",
    "- Dichrostachys: common name=aroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_count_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/27965295/dropping-rows-from-dataframe-based-on-a-not-in-condition\n",
    "drop = ['Matthiola', 'California', 'Dichrostachys']\n",
    "genus_count_df = genus_count_df[~genus_count_df['Taxa names'].isin(drop)]\n",
    "genus_count_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for counts over time bins for top X genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topX_ts_bin_counts(topX, taxa_csr, level_offspr, level_parent, count_df, \n",
    "                           level_offspr_names):\n",
    "  '''Get topX taxa, then count the numbers of docs in each bin\n",
    "  Args:\n",
    "    topX (int): number of top taxa to look at\n",
    "    taxa_csr (csr): csr with counts for the offspring taxa of interests\n",
    "    level_offspr (str): taxonmic level of the offsprings to focus on, e.g. genus\n",
    "    level_paraent (str): the parent level, e.g. viridiplantae. This is for\n",
    "      naming the output file.\n",
    "    count_df (DataFrame): two columns, \"Taxa names\" and \"Total\"\n",
    "    level_offspr_names (list): a list of names of the level of interest that\n",
    "      are also in the viridi_offspr_dict (with names of Viridiplantae offspring\n",
    "      taxa).\n",
    "  Return:\n",
    "    ts_count_df(DataFrame): timestamp as rows, taxa as columns, counts as vals\n",
    "    level_ts (DataFrame): taxa as rows, timestamp as columns, min-max\n",
    "      normalized counts as values\n",
    "  '''      \n",
    "  # Get the sub-dataframe for topX\n",
    "  count_df_topX = count_df.nlargest(topX, 'Total')\n",
    "  #print(count_df_topX)\n",
    "  \n",
    "  topX_names    = count_df_topX['Taxa names'].tolist()\n",
    "  #print(topX_names)\n",
    "  \n",
    "  # Get the csr indices for the topX names\n",
    "  topX_idx = [idx for idx, name in enumerate(level_offspr_names)\n",
    "                if name in topX_names]\n",
    "  print(\"#topX_idx:\", len(topX_idx))\n",
    "  #print(topX_idx)\n",
    "\n",
    "  # Create a dict with {top_name:{timestamp:count}\n",
    "  topX_ts_count = {}\n",
    "  # the beginning ts of bins of all docs\n",
    "  ts_all        = documents[\"Bins_left\"] \n",
    "\n",
    "  # Go through each column. Only look at the topX\n",
    "  for col_idx in tqdm(topX_idx):\n",
    "    # Get column values\n",
    "    col_val  = taxa_csr[:,col_idx].toarray().ravel()\n",
    "    top_name = level_offspr_names[col_idx]\n",
    "    topX_ts_count[top_name] = OrderedDict()\n",
    "    for idx, ts in enumerate(ts_all):\n",
    "      doc_val = col_val[idx]\n",
    "      if ts not in topX_ts_count[top_name]:\n",
    "        topX_ts_count[top_name][ts] = doc_val\n",
    "      else:\n",
    "        topX_ts_count[top_name][ts]+= doc_val\n",
    "  \n",
    "  # Convert to dataframe and save as tsv\n",
    "  ts_count_df   = pd.DataFrame(topX_ts_count)\n",
    "  ts_count_file = \\\n",
    "      f\"Table_5_1_{level_offspr}-of-{level_parent}_top{topX}_count_ts_bins.txt\"\n",
    "  ts_count_df.to_csv((work_dir / ts_count_file), sep='\\t')\n",
    "\n",
    "  # Sort df based on timestamps\n",
    "  ts_count_df.sort_index(inplace=True)\n",
    "\n",
    "  # Do min-max scaling\n",
    "  ts_count_minmax_df   = minmax_scaling(ts_count_df,\n",
    "                                              columns=ts_count_df.columns)\n",
    "  ts_count_minmax_file = \\\n",
    "      f\"Table_5_1_{level_offspr}-of-{level_parent}_top{topX}\" + \\\n",
    "      \"_count_ts_bins_minmax.txt\"\n",
    "  ts_count_minmax_df.to_csv((work_dir / ts_count_minmax_file), sep='\\t')\n",
    "  \n",
    "  # Transpose  ts_genus_count_minmax_df so the genus are in rows, ts in columns.\n",
    "  level_ts = ts_count_minmax_df.transpose()\n",
    "\n",
    "  return ts_count_df, level_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to set heatmap boundX/Y and xticklabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bounds_and_xtick(topX):\n",
    "  # Set heatmap x-axis\n",
    "  ts_begin = datetime(1971, 1, 1).timestamp()\n",
    "  boundsX  = np.insert(ts_unique, 0, ts_begin)\n",
    "\n",
    "  # Set heatmap y-axis\n",
    "  toc_nooutlier = np.arange(topX) \n",
    "  midpointsY = (toc_nooutlier[:-1] + toc_nooutlier[1:]) / 2\n",
    "  boundsY    = np.concatenate([[2*midpointsY[0]-midpointsY[1]], \n",
    "                              midpointsY, \n",
    "                              [2*midpointsY[-1]-midpointsY[-2]]])\n",
    "\n",
    "  # xtick labels\n",
    "  ts_unique_dts = [datetime.fromtimestamp(ts) for ts in ts_unique]\n",
    "  ts_unique_dts = [f\"{dt.year}-{dt.month}-{dt.day}\" for dt in ts_unique_dts]\n",
    "  xticklabels   = [\"1917-1-1\"] + ts_unique_dts\n",
    "\n",
    "  return boundsX, boundsY, xticklabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot scaled heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "def plot_scaled_heatmap(boundsX, boundsY, xticklabels, yticklabels, values,\n",
    "                        method, level_offspr, level_parent, topX):\n",
    "  '''Plot scaled heatmap based on a csr matrix\n",
    "  Args:\n",
    "    boundsX (): the boundaries for the x-axis bins\n",
    "    boundsY (): the boundaries for the y-axis bins\n",
    "    xticklabels (list): strings of ordered dates of timestamp bins\n",
    "    yticklabels (list): strings of ordered names based on whatever criteria\n",
    "    values (csr): the count value matrix\n",
    "    method (str): how the y-axis is ordered for naming output file.\n",
    "    level_offspr (str): taxonmic level of the offsprings to focus on, e.g. genus\n",
    "    level_paraent (str): the parent level, e.g. viridiplantae. This is for\n",
    "      naming the output file.\n",
    "    topX (int): number of top taxa that are drawn, for output name.\n",
    "  '''\n",
    "  height  = 18*(boundsY.shape[0]/100) # 18 inch for 100 bins\n",
    "  fig, ax = plt.subplots(figsize=(10,height))\n",
    "  plot    = ax.pcolormesh(boundsX, boundsY, values, cmap=\"RdPu\")\n",
    "  ax.set_xticks(boundsX)\n",
    "  ax.set_xticklabels(xticklabels, rotation=90)\n",
    "  ax.set_yticks(np.arange(values.shape[0]))\n",
    "  ax.set_yticklabels(yticklabels)\n",
    "  ax.grid(False)\n",
    "  fig.colorbar(plot)\n",
    "  figure_file = f'fig5_1_{level_offspr}-of-{level_parent}_top{topX}'+ \\\n",
    "                f'_heatmap_{method}_xcaled.pdf'\n",
    "  plt.savefig(work_dir / figure_file)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to do barplot of topX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topX_barplot(topX, df, level_offspr, level_parent):\n",
    "\n",
    "  # Sort value in ascending order\n",
    "  df   = df.sort_values(\"Total\")\n",
    "\n",
    "  # Get total counts and total for topX\n",
    "  total = sum(df['Total'])\n",
    "  df[\"Percent total\"] = df['Total']/total*100\n",
    "\n",
    "  # Get the topX subset and the total for the subset\n",
    "  df_subset = df.iloc[-topX:]\n",
    "  total_subset = sum(df_subset['Total'])\n",
    "\n",
    "  # Add a row for \"others\"\n",
    "  #https://www.geeksforgeeks.org/add-a-row-at-top-in-pandas-dataframe/\n",
    "  row_other = pd.DataFrame({\"Taxa names\":\"Others\", \n",
    "                            \"Total\":(total-total_subset),\n",
    "                            \"Percent total\":(100-total_subset/total*100)},\n",
    "                            index=[df.shape[0]])\n",
    "  df_subset = pd.concat([row_other, df_subset])\n",
    "  #print(df_subset)\n",
    "\n",
    "  # plot\n",
    "  df_subset.plot.barh(x='Taxa names', y='Percent total', rot=0, \n",
    "                      figsize=(6,6*topX/15))\n",
    "  plt.xlabel(\"Percent total\")\n",
    "  figure_file = f'fig5_1_{level_offspr}-of-{level_parent}_top{topX}'+ \\\n",
    "                f'_barplot.pdf'\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(work_dir / figure_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX_barplot(40, genus_count_df, \"genus\", \"viridiplantae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 genus, ordered with clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get count dataframe, bounds, and xticklabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX = 100\n",
    "level_offspr = \"genus\"\n",
    "level_parent = \"viridiplantae\"\n",
    "_, genus_ts100  = get_topX_ts_bin_counts(topX, match_csr, level_offspr, \n",
    "                             level_parent, genus_count_df, viridi_offspr_names)\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "genus_ts100.shape, boundsX.shape, boundsY.shape, len(xticklabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set yticklabels after building a prelim heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a preliminary heatmap, not scaled to get y-ordering\n",
    "method = \"centroid\"\n",
    "\n",
    "cgrid = sns.clustermap(genus_ts100, cmap=\"RdPu\", \n",
    "                       row_cluster=True, col_cluster=False,\n",
    "                       method=method,\n",
    "                       xticklabels=xticklabels,\n",
    "                       yticklabels=True)\n",
    "# Set size  \n",
    "plt.gcf().set_size_inches(10, 18)\n",
    "\n",
    "# Save the heatmap\n",
    "figure_file = f'fig5_1_{level_offspr}-of-{level_parent}_top{topX}'+ \\\n",
    "              f'_heatmap_{method}_prelim.pdf'\n",
    "cgrid.savefig(work_dir / figure_file)\n",
    "\n",
    "# Get the row order\n",
    "order_idx = cgrid.dendrogram_row.reordered_ind\n",
    "order_idx[0], order_idx[1], genus_ts100.index[order_idx[0]], genus_ts100.index[order_idx[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytick labels\n",
    "yticklabels = [genus_ts100.index[i] for i in order_idx]\n",
    "genus_ts100_reordered = genus_ts100.reindex(yticklabels)\n",
    "genus_ts100_reordered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot scaled heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scaled_heatmap(boundsX, boundsY, xticklabels, yticklabels, \n",
    "                genus_ts100_reordered, method, level_offspr, level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: order heatmap based on moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to calculate moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg_reorder_and_plot(values, step_size, level_offspr, level_parent, \n",
    "                                topX):\n",
    "  dict_max_val_ts = {} # {1st_timestamp_in_window:{max_value:[taxa]}}\n",
    "\n",
    "  #print(\"In moving_avg_reorder_and_plot:\", values.shape)\n",
    "  count = 0\n",
    "  print(type(values))\n",
    "  for taxa in values.index:\n",
    "    # series for genus\n",
    "    val_series = values.loc[taxa]\n",
    "    val_max    = 0 # for setting value maximum\n",
    "    val_max_ts = 0 # for setting last_timestamp_in_window for value maximum\n",
    "\n",
    "    # Go through steps\n",
    "    for idx in range(0, val_series.shape[0], step_size):\n",
    "      vals     = val_series.iloc[idx:idx+step_size]\n",
    "      val_mean = np.mean(vals)\n",
    "      # new max found, set values\n",
    "      if val_mean > val_max:\n",
    "        val_max    = val_mean\n",
    "        val_max_ts = vals.index[0]\n",
    "      #print(val_max, val_max_ts)\n",
    "    \n",
    "    # set values in dict_max_val_ts\n",
    "    if val_max_ts not in dict_max_val_ts:\n",
    "      dict_max_val_ts[val_max_ts] = {val_max:[taxa]}\n",
    "    elif val_max not in dict_max_val_ts[val_max_ts]:\n",
    "      dict_max_val_ts[val_max_ts][val_max] = [taxa]\n",
    "    # for topx=100, step_size=2, one val_max is the same\n",
    "    else:\n",
    "      print(\"Same val_max:\", val_max_ts, val_max, taxa)\n",
    "      dict_max_val_ts[val_max_ts][val_max].append(taxa)\n",
    "\n",
    "  # Get the new yticklabels based on the ordered ts_max_val\n",
    "  # was doing list(dict_max_val_ts.keys()), but this gives:\n",
    "  #  TypeError: list indices must be integers or slices, not float\n",
    "  max_val_ts   = [key for key in dict_max_val_ts.keys()]\n",
    "  max_val_ts.sort()\n",
    "  max_val_ts.reverse()\n",
    "  new_yticks = []\n",
    "  for ts in max_val_ts:\n",
    "    #print(ts)\n",
    "    val_maxs = [key for key in dict_max_val_ts[ts].keys()]\n",
    "    val_maxs.sort()\n",
    "    for val_max in val_maxs:\n",
    "      #print(\"\", val_max, len(dict_max_val_ts[ts][val_max]))\n",
    "      #print(ts, val_max, dict_max_val_ts[ts][val_max])\n",
    "      for taxa in dict_max_val_ts[ts][val_max]:\n",
    "        new_yticks.append(taxa)\n",
    "  #print(len(new_yticks))\n",
    "\n",
    "  # Reorder dataframe\n",
    "  values_reordered = values.reindex(new_yticks)\n",
    "  #print(\"after reordering:\", values_reordered.shape)\n",
    "\n",
    "  # set bounts and xticks\n",
    "  boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "  plot_scaled_heatmap(boundsX, boundsY, xticklabels, new_yticks, \n",
    "    values_reordered, f\"window{step_size}\", level_offspr, level_parent, topX)\n",
    "\n",
    "  return new_yticks, values_reordered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 100, step_size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX      = 100\n",
    "step_size = 3\n",
    "new_yticks3, val_reordered3 = moving_avg_reorder_and_plot(\n",
    "                      genus_ts100, step_size, level_offspr, level_parent, topX)\n",
    "len(new_yticks3), new_yticks3[:5], val_reordered3.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 100, step size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX       = 100\n",
    "step_size  = 2\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "_, _       = moving_avg_reorder_and_plot(genus_ts100, step_size, level_offspr, \n",
    "                                         level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20, step size 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX       = 20\n",
    "step_size  = 3\n",
    "_, genus_ts20 = get_topX_ts_bin_counts(topX, match_csr, level_offspr, \n",
    "                                  level_parent, genus_count_df, viridi_offspr_names)\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "_, _       = moving_avg_reorder_and_plot(genus_ts20, step_size, level_offspr, \n",
    "                                         level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20, step size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX       = 20\n",
    "step_size  = 2\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "_, _       = moving_avg_reorder_and_plot(genus_ts20, step_size, level_offspr, \n",
    "                                         level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 40, step size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX       = 40\n",
    "step_size  = 2\n",
    "_, genus_ts40 = get_topX_ts_bin_counts(topX, match_csr, level_offspr, level_parent, \n",
    "                                  genus_count_df, viridi_offspr_names)\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "_, _       = moving_avg_reorder_and_plot(genus_ts40, step_size, level_offspr, \n",
    "                                         level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All genus over time\n",
    "\n",
    "This is to get a sense whether there are more genus being studied over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genus = genus_count_df[genus_count_df['Total'] > 0].shape[0]\n",
    "print(\"Number of genus:\",num_genus)\n",
    "\n",
    "ts_genusALL_file = work_dir / \"Table5_1_ts_genusALL_count.txt\"\n",
    "genus_tsALL_file = work_dir / \"Table5_1_genus_tsALL_minmax.txt\"\n",
    "\n",
    "if not ts_genusALL_file.is_file():\n",
    "  topX = num_genus\n",
    "  level_offspr = \"genus\"\n",
    "  level_parent = \"viridiplantae\"\n",
    "  ts_genusALL, genus_tsALL  = get_topX_ts_bin_counts(topX, match_csr, \n",
    "              level_offspr, level_parent, genus_count_df, viridi_offspr_names)\n",
    "\n",
    "  ts_genusALL.to_csv(ts_genusALL_file)\n",
    "  genus_tsALL.to_csv(genus_tsALL_file)\n",
    "\n",
    "else:\n",
    "  # Load table\n",
    "  genus_tsALL = pd.read_csv(genus_tsALL_file, index_col=0)\n",
    "\n",
    "  # once it is read in all column names are now strings, assign to the int ones.\n",
    "  genus_tsALL.columns = ts_unique\n",
    "  print(genus_tsALL.shape, type(genus_tsALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of genus mentioned in each time bin\n",
    "genus_ts_counts = []\n",
    "for ts in ts_unique:\n",
    "  genus_ts_vals = genus_tsALL[ts].values\n",
    "  genus_ts_counts.append(np.count_nonzero(genus_ts_vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to string, this include the 1st date\n",
    "ts_yr_mo = [d.strftime(\"%y-%m-%d\") for d in bin_dates]\n",
    "len(ts_yr_mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_yr_mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize count based on bin size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the norm factor is the number of years\n",
    "norm_factor_list = []\n",
    "for idx in range(0, len(ts_yr_mo)-1):\n",
    "  d1 = datetime.strptime(ts_yr_mo[idx]  , '%y-%m-%d')\n",
    "  d2 = datetime.strptime(ts_yr_mo[idx+1], '%y-%m-%d')\n",
    "  \n",
    "  # deal with 17 being interpreted as 2017 instead of 1917\n",
    "  #https://stackoverflow.com/questions/16600548/how-to-parse-string-dates-with-2-digit-year\n",
    "  if idx == 0:\n",
    "    d1 = d1.replace(year=d1.year-100)\n",
    "\n",
    "  # Get relative delta in days then converted to year:\n",
    "  #https://stackoverflow.com/questions/27908090/dateutil-relativedelta-how-to-get-duration-in-days \n",
    "  # The above does not work try 2nd solution\n",
    "  d_diff = (d2-d1).days/365\n",
    "  norm_factor_list.append(d_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't remember what this is for... \n",
    "\n",
    "#genus_ts_counts_norm = []\n",
    "#for idx, count in enumerate(genus_ts_counts):\n",
    "#  genus_ts_counts_norm.a\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://matplotlib.org/stable/gallery/lines_bars_and_markers/barh.html\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "barWidth = 0.8\n",
    "x_pos = np.arange(len(ts_yr_mo))\n",
    "ax.bar(x_pos[1:], genus_ts_counts, width=barWidth)\n",
    "ax.set_xticks([r + barWidth-0.3 for r in x_pos], labels=ts_yr_mo, rotation=90)\n",
    "ax.set_ylabel('Number of genus')\n",
    "ax.set_xlabel('Time bins (end points)')\n",
    "plt.savefig(work_dir / 'fig5_1_genus_count_over_time.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New genus over time\n",
    "\n",
    "This is to get a sense how many new genus are being published on over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAke a deep copy to modify\n",
    "genus_tsALL_new = deepcopy(genus_tsALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find when a genus is 1st present, set all values in later time bins to 0\n",
    "genus_indices = genus_tsALL_new.index\n",
    "for idx in genus_indices:\n",
    "  vals     = genus_tsALL_new.loc[idx].values\n",
    "  new_vals = np.zeros(len(vals))\n",
    "\n",
    "  # get the index of the 1st non-zero in a row of values, \n",
    "  idx_1st_nonzero = (vals!=0).argmax()\n",
    "  # assign 1 for that index in new_vals\n",
    "  new_vals[idx_1st_nonzero] = 1\n",
    "\n",
    "  # replace old values for the specific idx with the new_vals\n",
    "  genus_tsALL_new.loc[idx] = new_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of genus mentioned in each time bin\n",
    "genus_ts_counts_new = []\n",
    "for ts in ts_unique:\n",
    "  genus_ts_vals_new = genus_tsALL_new[ts].values\n",
    "  genus_ts_counts_new.append(np.count_nonzero(genus_ts_vals_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "barWidth = 0.8\n",
    "x_pos = np.arange(len(ts_yr_mo))\n",
    "ax.bar(x_pos[1:], genus_ts_counts_new, width=barWidth, color=\"orange\")\n",
    "ax.set_xticks([r + barWidth-0.3 for r in x_pos], labels=ts_yr_mo, rotation=90)\n",
    "ax.set_ylabel('Number of new genus')\n",
    "ax.set_xlabel('Time bins (end points)')\n",
    "plt.savefig(work_dir / 'fig5_1_genus_count_new_over_time.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Family, order, special levels___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tax ids and names for a taxonomic level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_names(level, offsprings):\n",
    "  '''Get taxids and names for a particular taxonomic level, e.g., genus\n",
    "  Args:\n",
    "    level (str): taxonomic level specified in NCBI taxonomy to be counted\n",
    "    offsprings (list): all taxon names that are offspring of a parent taxid, \n",
    "      this parent should be above the level specified. This is not checked. So\n",
    "      need to make sure it is the case.\n",
    "  Return:\n",
    "    taxids_filter (list): tax ids that filtered against the offsprings list\n",
    "    names_all (list): a list of taxon names corresponding to the tax ids\n",
    "  '''\n",
    "  \n",
    "  # a list of all tax_ids\n",
    "  taxids = rank_taxa[level]\n",
    "\n",
    "  # filter taxids based on the list of viridiplantae ids\n",
    "  taxids_filter = [tax_id for tax_id in taxids if tax_id in offsprings]\n",
    "\n",
    "  names_dict, names_all = get_names(taxids_filter)\n",
    "  print(f\"level={level}, #taxids={len(taxids_filter)}, #names={len(names_all)}\")\n",
    "\n",
    "  return taxids_filter, names_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(taxids):\n",
    "  '''Get names based on a list of taxids'''\n",
    "  taxids_names_dict = {viridi_names_dic[tax_id]['scientific name'][0]:1 \n",
    "                       for tax_id in taxids}\n",
    "  taxids_names  = list(taxids_names_dict.keys())\n",
    "\n",
    "  return taxids_names_dict, taxids_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate counts of the offsprings of each taxa\n",
    "\n",
    "The idea behind this is that, if I want a count for a family F, then:\n",
    "- Any mention of taxa that is offspring of F in an article will be counted as F.\n",
    "- If multiple mention of taxa in F is mentioned in a single doc, then it is counted as 1.\n",
    "- F should also be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_counts(taxids):\n",
    "  '''Iterate through a list of taxids and consolidate the offspring counts of\n",
    "     each taxid. E.g., the list can be for different families. Then each family\n",
    "     is iterated to get the counts for the family and all its offsprings. The\n",
    "     count is then summed for each family and set to 1 if the count is >0.\n",
    "  Args:\n",
    "    taxids (list): a list of taxids to iterate through\n",
    "  Return:\n",
    "    taxids_csr (csr): a matrix of consolidated counts, with column the same as\n",
    "      the length of taxids.\n",
    "    taxids_csr_csum (list): column sum of the above csr for debugging purpose \n",
    "  '''\n",
    "  # Go through each taxa to get offsprings of that taxa\n",
    "  taxids_sum = []\n",
    "\n",
    "  for tax_id in tqdm(taxids):\n",
    "    #print(tax_id, names_dic[tax_id]['scientific name'][0])\n",
    "    # get offsprings of the taxa\n",
    "    taxid_offspr          = get_offsprings(tax_id, parent_child, [])\n",
    "    taxid_offspr_dict     = {name:1 for name in taxid_offspr}\n",
    "    _, taxid_offspr_names = get_names(taxid_offspr)\n",
    "    \n",
    "    # Include the parent name so it can be counted\n",
    "    parent = viridi_names_dic[tax_id]['scientific name'][0]\n",
    "    parent_offspr_names = [parent] + taxid_offspr_names\n",
    "\n",
    "    # Get the index of the parent and offsprings in the csr matrix\n",
    "    csr_idx = [viridi_offspr_names.index(name) for name in parent_offspr_names\n",
    "               if name in viridi_offspr_dict]\n",
    "    sub_csr = match_csr[:, csr_idx]\n",
    "\n",
    "    # Consolidate the columns\n",
    "    sub_csr_sum = sub_csr.sum(axis=1)\n",
    "    #print(sub_csr_sum.shape)\n",
    "\n",
    "    # Add each sum array to a list\n",
    "    taxids_sum.append(sub_csr_sum)\n",
    "\n",
    "  # Convert the list to an array, then to csr\n",
    "  #https://stackoverflow.com/questions/7200878/python-list-of-np-arrays-to-array\n",
    "  taxids_array = np.hstack(taxids_sum)\n",
    "  taxids_csr   = csr_matrix(taxids_array)\n",
    "  #print(\"Col sum, before:\", taxids_csr.sum(axis=0).ravel())\n",
    "\n",
    "  # set any value >= 1 as 1\n",
    "  taxids_csr[taxids_csr > 1] = 1\n",
    "  taxids_csr_csum = np.squeeze(np.asarray(taxids_csr.sum(axis=0)))\n",
    "  #print(\"Col sum, after :\", taxids_csr_c_sum)\n",
    "\n",
    "  return taxids_csr, taxids_csr_csum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_ids and names\n",
    "fam_ids, fam_names    = get_level_names(\"family\", viridi_offspr)\n",
    "# count matrix, and column sum \n",
    "fam_csr, fam_csr_csum = get_consolidated_counts(fam_ids)\n",
    "# family counts\n",
    "fam_count_df = pd.DataFrame(list(zip(fam_names, fam_csr_csum)),\n",
    "                        columns=(\"Taxa names\", \"Total\"))\n",
    "fam_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_count_file = work_dir / \"Table_5_1_family_count.txt\"\n",
    "fam_count_df.to_csv(fam_count_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumped for later use in 5_3\n",
    "\n",
    "# output match_csr for genus\n",
    "with open(work_dir / \"match_csr_family.pickle\", \"wb\") as f:\n",
    "  pickle.dump(fam_csr, f)\n",
    "\n",
    "# output genus name in the same order as match_csr_genus\n",
    "with open(work_dir / \"match_csr_family_names.pickle\", \"wb\") as f:\n",
    "  pickle.dump(fam_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 100 fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, genus_ts20 = get_topX_ts_bin_counts(topX, match_csr, level_offspr, \n",
    "                                  level_parent, genus_count_df, viridi_offspr_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set topX and get counts in bins\n",
    "topX = 100\n",
    "level_offspr = \"family\"\n",
    "level_parent = \"viridiplantae\"\n",
    "_, fam_ts100 = get_topX_ts_bin_counts(topX, fam_csr, level_offspr, level_parent, \n",
    "                                  fam_count_df, fam_names)\n",
    "# Set bounts and xticks\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fam_ts100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set step size and plot\n",
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(fam_ts100, step_size, level_offspr, \n",
    "                                    level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 40 fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set topX and get counts in bins\n",
    "topX = 40\n",
    "level_offspr = \"family\"\n",
    "level_parent = \"viridiplantae\"\n",
    "_, fam_ts40 = get_topX_ts_bin_counts(topX, fam_csr, level_offspr, level_parent, \n",
    "                                  fam_count_df, fam_names)\n",
    "# Set bounts and xticks\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "# Set step size and plot\n",
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(fam_ts40, step_size, level_offspr, \n",
    "                                    level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set topX and get counts in bins\n",
    "topX = 20\n",
    "level_offspr = \"family\"\n",
    "level_parent = \"viridiplantae\"\n",
    "_, fam_ts20 = get_topX_ts_bin_counts(topX, fam_csr, level_offspr, level_parent, \n",
    "                                  fam_count_df, fam_names)\n",
    "# Set bounts and xticks\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "# Set step size and plot\n",
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(fam_ts20, step_size, level_offspr, \n",
    "                                    level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_ids and names\n",
    "order_ids, order_names    = get_level_names(\"order\", viridi_offspr)\n",
    "# count matrix, and column sum \n",
    "order_csr, order_csr_csum = get_consolidated_counts(order_ids)\n",
    "# order counts\n",
    "order_count_df = pd.DataFrame(list(zip(order_names, order_csr_csum)),\n",
    "                        columns=(\"Taxa names\", \"Total\"))\n",
    "\n",
    "order_count_file = work_dir / \"Table_5_1_order_count.txt\"\n",
    "order_count_df.to_csv(order_count_file, sep='\\t')\n",
    "\n",
    "# Set topX and get counts in bins\n",
    "topX = 20\n",
    "level_offspr = \"order\"\n",
    "level_parent = \"viridiplantae\"\n",
    "_, order_ts20 = get_topX_ts_bin_counts(topX, order_csr, level_offspr, level_parent, \n",
    "                                  order_count_df, order_names)\n",
    "# Set bounts and xticks\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "# Set step size and plot\n",
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(order_ts20, step_size, level_offspr, \n",
    "                                    level_parent, topX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumped for later use in 5_3\n",
    "\n",
    "# output match_csr for genus\n",
    "with open(work_dir / \"match_csr_order.pickle\", \"wb\") as f:\n",
    "  pickle.dump(order_csr, f)\n",
    "\n",
    "# output genus name in the same order as match_csr_genus\n",
    "with open(work_dir / \"match_csr_order_names.pickle\", \"wb\") as f:\n",
    "  pickle.dump(order_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phyla/mix level\n",
    "\n",
    "- Chlorophyta\n",
    "- Charophyceae\n",
    "- Zygnemophyceae \n",
    "- Anthocerotophyta (hornworts)\n",
    "- Bryophyta (mosses)\n",
    "- Marchantiophyta (liverworts)  \n",
    "- Lycopodiopsida (clubmosses)\n",
    "- Polypodiopsida (including horsetail)\n",
    "- Acrogymnospermae \n",
    "- eudicotyledons\n",
    "- Liliopsida (monocot)\n",
    "- Magnoliopsida (flowering plants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Chlorophyta', 'Charophyceae', 'Zygnemophyceae', 'Anthocerotophyta',\n",
    "           'Bryophyta', 'Marchantiophyta', 'Lycopodiopsida', 'Polypodiopsida',\n",
    "           'Acrogymnospermae', 'eudicotyledons', 'Liliopsida', 'Magnoliopsida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tax_id of targets\n",
    "target_id_dict = {}\n",
    "for tax_id in tqdm(viridi_names_dic):\n",
    "  name = viridi_names_dic[tax_id]['scientific name'][0]\n",
    "  if name in targets:\n",
    "    target_id_dict[name] = tax_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids   = list(target_id_dict.values())\n",
    "target_names = list(target_id_dict.keys())\n",
    "\n",
    "target_csr, target_csr_csum = get_consolidated_counts(target_ids)\n",
    "# family counts\n",
    "target_count_df = pd.DataFrame(list(zip(target_names, target_csr_csum)),\n",
    "                        columns=(\"Taxa names\", \"Total\"))\n",
    "\n",
    "target_count_file = work_dir / \"Table_5_1_special_count.txt\"\n",
    "target_count_df.to_csv(target_count_file, sep='\\t')\n",
    "target_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set topX and get counts in bins\n",
    "topX = 12\n",
    "level_offspr = \"special\"\n",
    "level_parent = \"viridiplantae\"\n",
    "target_ts = get_topX_ts_bin_counts(topX, target_csr, level_offspr, level_parent, \n",
    "                                   target_count_df, target_names)\n",
    "# Set bounts and xticks\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "# Set step size and plot\n",
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(target_ts, step_size, level_offspr, \n",
    "                                    level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Genus of a higher taxonomic level___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_genus_of_a_higher_level(level_parent, topX, step_size):\n",
    "  '''Count and plot numbers of pubs for taxa that belong to a higher level taxon\n",
    "  Args:\n",
    "    level_parent (str): the higher level taxon\n",
    "    topX (int): the topX genus to plot\n",
    "    step_size (int): the sliding window step size for sorting genus\n",
    "  '''\n",
    "  \n",
    "  # This could be set as a parameter so any level would work. But this require\n",
    "  # additional thoughts about consolidating counts with a function I wrote\n",
    "  # earlier. Got what I want any way and no time. So did not do it.\n",
    "  level_offspr = \"genus\"\n",
    "\n",
    "  # id of the higher leve\n",
    "  tax_id, _    = get_name_dict(names_dmp_path, level_parent)\n",
    "  # offsprings of the higher level taxa       l\n",
    "  tax_offspr   = get_offsprings(tax_id, parent_child, [])\n",
    "  # tax_ids and names of offsprings at the level_offspr\n",
    "  tax_offspr_ids, tax_offspr_names = get_level_names(level_offspr, tax_offspr)\n",
    "\n",
    "  # convert higher level taxon offspring names into a dict \n",
    "  tax_offspr_dict = {name:1 for name in tax_offspr_names}\n",
    "\n",
    "  # Get indices of data for higher level taxon offspring names in the big csr\n",
    "  # offspring_names is the list for all offspring of Viridiplantae, so will\n",
    "  # will inclue offspring of any higher level taxon as long as it is below \n",
    "  # Viridiplantae\n",
    "  tax_offspr_csr_idx  = [viridi_offspr_names.index(name) \n",
    "                        if name in viridi_offspr_dict else print(\"Not found:\", name)\n",
    "                        for name in tqdm(tax_offspr_names)] \n",
    "\n",
    "  # get count matrix, and column sum \n",
    "  tax_offspr_csr      = match_csr[:, tax_offspr_csr_idx]\n",
    "  tax_offspr_csr_csum = np.squeeze(np.asarray(tax_offspr_csr.sum(axis=0)))\n",
    "\n",
    "  # counts of offspring taxa for the higher level taxon\n",
    "  tax_offspr_count_df = pd.DataFrame(\n",
    "                          list(zip(tax_offspr_names, tax_offspr_csr_csum)),\n",
    "                          columns=(\"Taxa names\", \"Total\"))\n",
    "\n",
    "  # Three taxa with names that can get confusing\n",
    "  drop = ['Matthiola', 'California', 'Dichrostachys']\n",
    "  tax_offspr_count_df = tax_offspr_count_df[\n",
    "                                  ~tax_offspr_count_df['Taxa names'].isin(drop)]\n",
    "\n",
    "  tax_offspr_count_file = f\"Table_5_1_{level_parent}_{level_offspr}_count.txt\"\n",
    "  tax_offspr_count_df.to_csv(work_dir / tax_offspr_count_file, sep='\\t')\n",
    "\n",
    "  # get counts in bins of topX offspring taxa\n",
    "  tax_offspr_ts = get_topX_ts_bin_counts(topX, tax_offspr_csr, level_offspr,\n",
    "                            level_parent, tax_offspr_count_df, tax_offspr_names)\n",
    "\n",
    "  # Set bounts and xticks\n",
    "  boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "  print(len(boundsY))\n",
    "  # plot heatmap\n",
    "  _, _  = moving_avg_reorder_and_plot(tax_offspr_ts, step_size, level_offspr, \n",
    "                                      level_parent, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopX genus in Brassicaceae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_parent = \"Brassicaceae\" \n",
    "topX         = 10\n",
    "step_size    = 2\n",
    "count_genus_of_a_higher_level(level_parent, topX, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopX genus in Solanaceae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_parent = \"Solanaceae\" \n",
    "topX         = 10\n",
    "step_size    = 2\n",
    "count_genus_of_a_higher_level(level_parent, topX, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopX genus in Fabaceae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_parent = \"Fabaceae\" \n",
    "topX         = 20\n",
    "step_size    = 2\n",
    "count_genus_of_a_higher_level(level_parent, topX, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopX genus in Poaceae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "level_parent = \"Poaceae\" \n",
    "topX         = 20\n",
    "step_size    = 2\n",
    "count_genus_of_a_higher_level(level_parent, topX, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Code testing___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing parent-child parsing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'37868' in viridi_offspr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Achnatherum' in offspring_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot check parent-child\n",
    "len(parent_child['147383']), '37868' in parent_child['147383']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_147383 = '''\n",
    "    Achnatherum   \n",
    "    Aciachne   \n",
    "    Amelichloa   \n",
    "    Anatherostipa   \n",
    "    Anemanthele   \n",
    "    Austrostipa   \n",
    "    Barkworthia   \n",
    "    Celtica   \n",
    "    Eriocoma   \n",
    "    Hesperostipa   \n",
    "    Jarava   \n",
    "    Lorenzochloa   \n",
    "    Macrochloa   \n",
    "    Nassella   \n",
    "    Neotrinia   \n",
    "    Oloptum   \n",
    "    Ortachne   \n",
    "    Oryzopsis (ricegrass)   \n",
    "    Pappostipa   \n",
    "    Patis   \n",
    "    Piptatheropsis   \n",
    "    Piptatherum   \n",
    "    Piptochaetium   \n",
    "    Psammochloa   \n",
    "    Pseudoeriocoma   \n",
    "    Ptilagrostiella   \n",
    "    Ptilagrostis   \n",
    "    Stipa   \n",
    "    Stipellula   \n",
    "    Thorneochloa   \n",
    "    Timouria   \n",
    "    Trikeraia   \n",
    "    x Eriosella'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test string preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus['txt'][1]\n",
    "doc = str(doc).strip()\n",
    "doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_147383_parsed = []\n",
    "for child in children_147383.split(\"\\n\"):\n",
    "  children_147383_parsed.append(child.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(children_147383_parsed), len(parent_child['147383'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child['147368']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still missing 1, this is a synonym\n",
    "for child in children_147383_parsed:\n",
    "  if child not in debug_list:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot check child_parent\n",
    "for child in parent_child[target_id]:\n",
    "  print(child, taxa_rank[child], child_parent[child])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was missing in the 2021 taxa dump file, now it is there\n",
    "child_parent['2950019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing find names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus['txt_clean'][1]\n",
    "results_ncbi = [name for name in viridi_offspr_names if(name in doc)]\n",
    "print(results_ncbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ncbi = [1 if(name in doc) else 0 for name in viridi_offspr_names]\n",
    "print(sum(results_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that here I add two spaces to pad the names, to prevent matching to\n",
    "# substring.\n",
    "results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "print(results_usda)\n",
    "\n",
    "#results_usda = [1 if(f\" {name} \" in doc) else 0 for name in common_names ]\n",
    "#print(sum(results_usda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get genus names for common matches and add the doc \n",
    "for cname in results_usda:\n",
    "  genus = cnames[cname][0]\n",
    "  doc += f\" {genus}\"\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the count again\n",
    "results_ncbi = [1 if(name in doc) else 0 for name in offspring_names]\n",
    "print(sum(results_ncbi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the non-zero indices\n",
    "non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "type(non0_idx), non0_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col_idx = [1,2]\n",
    "test_col_idx.extend(non0_idx)\n",
    "test_col_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ordered dict comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_names = [\"eggplant\", \"corn\", \"spinach\"]\n",
    "[name if(f\" {name} \" in doc) else 0 for name in test_names ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict((name, 1) if(f\" {name} \" in doc) else (name, 0) \n",
    "                                              for name in test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(common_names), \"eggplant\" in common_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_usda = OrderedDict(\n",
    "  (name, 1) if(f\" {name} \" in doc) else (name, 0) for name in common_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_usda['eggplant'], results_usda['corn'], results_usda['spinach']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `get_match_csr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_txt_clean = corpus['txt_clean'][:5]\n",
    "test_match_csr = get_match_csr(test_txt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected 421628 x 26782\n",
    "test_match_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sum = test_match_csr.sum(axis=0)\n",
    "col_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non0_idx = col_sum.nonzero()[1]\n",
    "non0_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in non0_idx:\n",
    "  print(viridi_offspr_names[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spinach\n",
    "test_txt_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solanum, Zea\n",
    "test_txt_clean[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arachis, Sesbania  \n",
    "test_txt_clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barley  \n",
    "test_txt_clean[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soybean , Sesbania  \n",
    "test_txt_clean[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `get_match_csr` to get run time estimate\n",
    "\n",
    "https://stackoverflow.com/questions/28427236/set-row-of-csr-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lists of values, row_idx, and col idx, then create csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_csr_v1(txt):\n",
    "  row_idx   = []\n",
    "  col_idx   = []\n",
    "  csr_val   = []\n",
    "  for row, doc in enumerate(tqdm(txt)):\n",
    "    # Get the matching common names as a list\n",
    "    results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "\n",
    "    # Add the results to doc\n",
    "    for cname in results_usda:  # for each common name\n",
    "      genus = cnames[cname][0]  # get the genus name\n",
    "      doc += f\" {genus}\"        # add the genus name to doc\n",
    "    \n",
    "    # Match to NCBI names\n",
    "    results_ncbi = [1 if(name in doc) else 0 for name in offspring_names]\n",
    "\n",
    "    # Assign row_idx, col_idx, and values for non-zero results_ncbi\n",
    "    non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    row_idx.extend([row]*len(non0_idx))\n",
    "    col_idx.extend(non0_idx)\n",
    "    csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "  # create a sparse matrix with shape=(num_docs, num_names)\n",
    "  match_csr = csr_matrix((csr_val, (row_idx, col_idx)),\n",
    "                         shape=(corpus.shape[0], len(viridi_offspr_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  return match_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100 = corpus['txt_clean'][:100]\n",
    "t = time()\n",
    "test100_csr1 = get_match_csr_v1(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create empty match_csr first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty match_csr, then create a tmp_csr with row_idx, col_idx, values,\n",
    "# then add match_csr with the tmp_csr\n",
    "def get_match_csr_v2(txt):\n",
    "\n",
    "  # create an empty sparse matrix with shape=(num_docs, num_names)\n",
    "  match_csr = csr_matrix((corpus.shape[0], len(viridi_offspr_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  for row, doc in enumerate(tqdm(txt)):\n",
    "    # Get the matching common names as a list\n",
    "    results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "\n",
    "    # Add the results to doc\n",
    "    for cname in results_usda:  # for each common name\n",
    "      genus = cnames[cname][0]  # get the genus name\n",
    "      doc += f\" {genus}\"        # add the genus name to doc\n",
    "    \n",
    "    # Match to NCBI names\n",
    "    results_ncbi = [1 if(name in doc) else 0 for name in viridi_offspr_names]\n",
    "\n",
    "    # Assign row_idx, col_idx, and values for non-zero results_ncbi\n",
    "    non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    row_idx  = [row]*len(non0_idx)\n",
    "    col_idx  = non0_idx\n",
    "    csr_val  = [1]*len(non0_idx)\n",
    "\n",
    "    # Create a tmp csr to hold this row\n",
    "    tmp_csr = csr_matrix((csr_val, (row_idx, col_idx)),\n",
    "                         shape=(corpus.shape[0], len(offspring_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "    # Update match_csr by adding tmp_csr to it\n",
    "    match_csr = match_csr + tmp_csr\n",
    "\n",
    "  return match_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "test100_csr_v2 = get_match_csr_v2(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign a list to a row in match_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_csr_v3(txt):\n",
    "\n",
    "  # create an empty sparse matrix with shape=(num_docs, num_names)\n",
    "  match_csr = csr_matrix((corpus.shape[0], len(offspring_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  row_idx   = []\n",
    "  col_idx   = []\n",
    "  csr_val   = []\n",
    "  for row, doc in enumerate(tqdm(txt)):\n",
    "    # Get the matching common names as a list\n",
    "    results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "\n",
    "    # Add the results to doc\n",
    "    for cname in results_usda:  # for each common name\n",
    "      genus = cnames[cname][0]  # get the genus name\n",
    "      doc += f\" {genus}\"        # add the genus name to doc\n",
    "    \n",
    "    # Match to NCBI names\n",
    "    results_ncbi = [1 if(name in doc) else 0 for name in viridi_offspr_names]\n",
    "\n",
    "    # Assign row_idx, col_idx, and values for non-zero results_ncbi\n",
    "    #non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    #row_idx.extend([row]*len(non0_idx))\n",
    "    #col_idx.extend(non0_idx)\n",
    "    #csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "    # Assign new row values to match_csr\n",
    "    match_csr[row, :] = results_ncbi\n",
    "\n",
    "  return match_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "test100_csr_v3 = get_match_csr_v3(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate lil matrix instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to v3, but use lil matrix instead, tried coo also, but does not work\n",
    "def get_match_lil_v4(txt):\n",
    "\n",
    "  # create an empty sparse matrix with shape=(num_docs, num_names)\n",
    "  # instead of csr, use lil\n",
    "  match_lil = lil_matrix((corpus.shape[0], len(viridi_offspr_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  #row_idx   = []\n",
    "  #col_idx   = []\n",
    "  #csr_val   = []\n",
    "  for row, doc in enumerate(tqdm(txt)):\n",
    "    # Get the matching common names as a list\n",
    "    results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "\n",
    "    # Add the results to doc\n",
    "    for cname in results_usda:  # for each common name\n",
    "      genus = cnames[cname][0]  # get the genus name\n",
    "      doc += f\" {genus}\"        # add the genus name to doc\n",
    "    \n",
    "    # Match to NCBI names\n",
    "    results_ncbi = [1 if(name in doc) else 0 for name in viridi_offspr_names]\n",
    "\n",
    "    # Assign row_idx, col_idx, and values for non-zero results_ncbi\n",
    "    #non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    #row_idx.extend([row]*len(non0_idx))\n",
    "    #col_idx.extend(non0_idx)\n",
    "    #csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "    # Assign new row values to match_csr\n",
    "    match_lil[row, :] = np.asarray(results_ncbi)\n",
    "\n",
    "  return match_lil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "test100_lil_v4 = get_match_lil_v4(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_dok_v5(txt):\n",
    "\n",
    "  # create an empty sparse matrix with shape=(num_docs, num_names)\n",
    "  # instead of csr, use lil\n",
    "  match_dok = dok_matrix((corpus.shape[0], len(offspring_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  row_idx   = []\n",
    "  col_idx   = []\n",
    "  csr_val   = []\n",
    "  for row, doc in enumerate(tqdm(txt)):\n",
    "    # Get the matching common names as a list\n",
    "    results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "\n",
    "    # Add the results to doc\n",
    "    for cname in results_usda:  # for each common name\n",
    "      genus = cnames[cname][0]  # get the genus name\n",
    "      doc += f\" {genus}\"        # add the genus name to doc\n",
    "    \n",
    "    # Match to NCBI names\n",
    "    results_ncbi = [1 if(name in doc) else 0 for name in offspring_names]\n",
    "\n",
    "    # Assign row_idx, col_idx, and values for non-zero results_ncbi\n",
    "    #non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    #row_idx.extend([row]*len(non0_idx))\n",
    "    #col_idx.extend(non0_idx)\n",
    "    #csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "    # Assign new row values to match_csr\n",
    "    match_dok[row, :] = results_ncbi\n",
    "\n",
    "  return match_dok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "test100_dok_v5 = get_match_dok_v5(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ordered dictionary comprehension\n",
    "\n",
    "This is extremely slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonpool.com/python-ordereddict/\n",
    "def get_match_csr_v6(txt):\n",
    "  row_idx   = []\n",
    "  col_idx   = []\n",
    "  csr_val   = []\n",
    "  for row, doc in enumerate(tqdm(txt)):\n",
    "    # Get the matching common names as an ordered dict\n",
    "    results_usda = OrderedDict((name, 1) if(f\" {name} \" in doc) else (name, 0) \n",
    "                               for name in common_names)\n",
    "\n",
    "    # Add the results to doc\n",
    "    for cname in results_usda:  # for each common name\n",
    "      genus = cnames[cname][0]  # get the genus name\n",
    "      doc += f\" {genus}\"        # add the genus name to doc\n",
    "    \n",
    "    # Match to NCBI names as an ordered dict\n",
    "    results_ncbi = OrderedDict((name, 1) if(name in doc) else (name, 0) \n",
    "                               for name in offspring_names)\n",
    "\n",
    "    # Assign row_idx, col_idx, and values for non-zero results_ncbi\n",
    "    non0_idx = np.nonzero(results_ncbi.values())[0].tolist()\n",
    "    row_idx.extend([row]*len(non0_idx))\n",
    "    col_idx.extend(non0_idx)\n",
    "    csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "  # create a sparse matrix with shape=(num_docs, num_names)\n",
    "  match_csr = csr_matrix((csr_val, (row_idx, col_idx)),\n",
    "                         shape=(corpus.shape[0], len(offspring_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  return match_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "test100_csr6 = get_match_csr_v6(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try multiprocessing\n",
    "\n",
    "- https://superfastpython.com/multiprocessing-pool-for-loop/\n",
    "- https://stackoverflow.com/questions/42749772/multiprocessing-how-to-use-pool-map-on-a-list-and-function-with-arguments\n",
    "- https://python.omics.wiki/multiprocessing_map/multiprocessing_partial_function_multiple_arguments\n",
    "- https://stackoverflow.com/questions/41920124/multiprocessing-use-tqdm-to-display-a-progress-bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = [ [3,5], [4,3], [7,3], [1,6] ]\n",
    "\n",
    "def myfunc(p):\n",
    "  product_of_list = np.prod(p)\n",
    "  return product_of_list\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "result_list = pool.map(myfunc, data_pairs)\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in enumerate(test100):\n",
    "  print(type(i))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_csr_v7(txt):\n",
    "\n",
    "  with multiprocessing.Pool(processes=15) as pool:\n",
    "    results_ncbi_list = list(tqdm(pool.imap(task, enumerate(txt)), \n",
    "                                  total=len(txt)))\n",
    "\n",
    "  row_idx   = []\n",
    "  col_idx   = []\n",
    "  csr_val   = []\n",
    "  for row, results_ncbi in enumerate(results_ncbi_list):\n",
    "    non0_idx = np.nonzero(results_ncbi)[0].tolist()\n",
    "    row_idx.extend([row]*len(non0_idx))\n",
    "    col_idx.extend(non0_idx)\n",
    "    csr_val.extend([1]*len(non0_idx))\n",
    "\n",
    "  # create a sparse matrix with shape=(num_docs, num_names)\n",
    "  match_csr = csr_matrix((csr_val, (row_idx, col_idx)),\n",
    "                         shape=(txt.shape[0], len(offspring_names)), \n",
    "                         dtype=np.int0)\n",
    "\n",
    "  return match_csr\n",
    "\n",
    "def task(item):\n",
    "  '''Task to parallelize\n",
    "  Args:\n",
    "    item (tuple): (row_number, doc)\n",
    "  Return:\n",
    "    results_ncbi (list): an offspring_name is present in the doc (1) or not(1)\n",
    "  '''\n",
    "  (row, doc) = item\n",
    "  # Get the matching common names as a list\n",
    "  results_usda = [name for name in common_names if(f\" {name} \" in doc)]\n",
    "\n",
    "  # Add the results to doc\n",
    "  for cname in results_usda:  # for each common name\n",
    "    genus = cnames[cname][0]  # get the genus name\n",
    "    doc += f\" {genus}\"        # add the genus name to doc\n",
    "  \n",
    "  # Match to NCBI names\n",
    "  results_ncbi = [1 if(name in doc) else 0 for name in offspring_names]\n",
    "\n",
    "  return results_ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "test100_csr7 = get_match_csr_v7(test100)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100_csr7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offspring_names.index(\"Solanum\"), offspring_names.index(\"Zea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect to be 1, 1, 0\n",
    "test100_csr7[1, 16432], test100_csr7[1, 4360], test100_csr7[1, 16431]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offspring_names.index(\"Arachis\"), offspring_names.index(\"Sesbania\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100_csr7[2, 21986], test100_csr7[2, 21477]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing getting counts for different timestamp bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct topX dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the top 100 genus names into a list\n",
    "topX = 100\n",
    "\n",
    "genus_count_df_topX = genus_count_df.nlargest(topX, 'Total')\n",
    "top100_names        = genus_count_df_top100['Genus names'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the csr indices for the top 100 names\n",
    "top100_idx = [idx for idx, name in enumerate(viridi_genus_names)\n",
    "              if name in top100_names]\n",
    "              \n",
    "# these are csr index, not for the top100_names list\n",
    "len(top100_idx), top100_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_csr[:,1].toarray().ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict with {top_name:{timestamp:count}\n",
    "top100_ts_count = {}\n",
    "ts_all          = documents[\"Bins_left\"] # the beginning ts of bins of all docs\n",
    "\n",
    "# Go through each column. Only look at the top 100\n",
    "for col_idx in tqdm(top100_idx):\n",
    "  # Get column values\n",
    "  col_val  = genus_csr[:,col_idx].toarray().ravel()\n",
    "  top_name = viridi_genus_names[col_idx]\n",
    "  top100_ts_count[top_name] = OrderedDict()\n",
    "  for idx, ts in enumerate(ts_all):\n",
    "    doc_val = col_val[idx]\n",
    "    if ts not in top100_ts_count[top_name]:\n",
    "      top100_ts_count[top_name][ts] = doc_val\n",
    "    else:\n",
    "      top100_ts_count[top_name][ts]+= doc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top100_ts_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_genus_count_df   = pd.DataFrame(top100_ts_count)\n",
    "ts_genus_count_file = work_dir / \"Table_5_1_genus_count_ts_bins.txt\"\n",
    "ts_genus_count_df.to_csv(ts_genus_count_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_genus_count_df.sort_index(inplace=True)\n",
    "ts_genus_count_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get min-max scaled count dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do min-max scaling\n",
    "#https://stackoverflow.com/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn\n",
    "ts_genus_count_minmax_df   = minmax_scaling(ts_genus_count_df,\n",
    "                                            columns=ts_genus_count_df.columns)\n",
    "ts_genus_count_minmax_file = work_dir / \\\n",
    "                                      \"Table_5_1_genus_count_ts_bins_minmax.txt\"\n",
    "ts_genus_count_minmax_df.to_csv(ts_genus_count_minmax_file, sep='\\t')\n",
    "ts_genus_count_minmax_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose  ts_genus_count_minmax_df so the genus are in rows, ts in columns.\n",
    "genus_ts = ts_genus_count_minmax_df.transpose()\n",
    "genus_ts.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(test_df, cmap=\"RdPu\", col_cluster=False)\n",
    "plt.gcf().set_size_inches(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 3\n",
    "new_yticks3, val_reordered3 = moving_avg_reorder(test_df, step_size)\n",
    "len(new_yticks3), val_reordered3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing functions for counting other levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids, test_names = get_level_names(\"family\")\n",
    "test_ids = test_ids[:10]\n",
    "test_names = test_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_taxids_csr, test_taxids_csr_c_sum = get_consolidated_counts(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_df = pd.DataFrame(list(zip(test_names, test_taxids_csr_c_sum)),\n",
    "                        columns=(\"Taxa names\", \"Total\"))\n",
    "test_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX = 5\n",
    "test_ts = get_topX_ts_bin_counts(topX, test_taxids_csr, \"family\", \n",
    "                                 test_count_df, test_names)\n",
    "test_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "test_ts.shape, boundsX.shape, boundsY.shape, len(xticklabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(test_ts, step_size, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: counts of taxa that belong to to a high level taxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the higher level taxon\n",
    "tax          = \"Brassicaceae\" \n",
    "# the lower offspring taxa level\n",
    "level_offspr = \"genus\"        \n",
    "# id of the higher leve\n",
    "tax_id, _    = get_name_dict(names_dmp_path, tax)\n",
    "# offsprings of the higher level taxa       l\n",
    "tax_offspr   = get_offsprings(tax_id, parent_child, []) \n",
    "len(tax_offspr), check_duplicate(tax_offspr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_ids and names of offsprings at the level_offspr\n",
    "tax_offspr_ids, tax_offspr_names = get_level_names(level_offspr, tax_offspr)\n",
    "print(tax_offspr_ids[:10], tax_offspr_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert higher level taxon offspring names into a dict \n",
    "tax_offspr_dict = {name:1 for name in tax_offspr_names}\n",
    "\n",
    "# Get indices of data for higher level taxon offspring names in the big csr\n",
    "# offspring_names is the list for all offspring of Viridiplantae, so will\n",
    "# will inclue offspring of any higher level taxon as long as it is below Viridi.\n",
    "tax_offspr_csr_idx  = [offspring_names.index(name) \n",
    "                       if name in viridi_offspr_dict else print(\"Not found:\", name)\n",
    "                       for name in tqdm(tax_offspr_names)]\n",
    "len(tax_offspr_csr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix, and column sum \n",
    "tax_offspr_csr = match_csr[:, tax_offspr_csr_idx]\n",
    "tax_offspr_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_offspr_csr_csum = np.squeeze(np.asarray(tax_offspr_csr.sum(axis=0)))\n",
    "tax_offspr_csr_csum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family counts\n",
    "tax_offspr_count_df = pd.DataFrame(\n",
    "                        list(zip(tax_offspr_names, tax_offspr_csr_csum)),\n",
    "                        columns=(\"Taxa names\", \"Total\"))\n",
    "tax_offspr_count_file = work_dir / f\"Table_5_1_{tax}_{level_offspr}_count.txt\"\n",
    "tax_offspr_count_df.to_csv(tax_offspr_count_file, sep='\\t')\n",
    "\n",
    "tax_offspr_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set topX and get counts in bins\n",
    "topX = 40\n",
    "tax_offspr_ts = get_topX_ts_bin_counts(topX, tax_offspr_csr, \"genus\", \n",
    "                                       tax_offspr_count_df, tax_offspr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounts and xticks\n",
    "boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "# Set step size and plot\n",
    "step_size = 2\n",
    "_, _  = moving_avg_reorder_and_plot(tax_offspr_ts, step_size, topX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topX = 20\n",
    "df   = genus_count_df\n",
    "\n",
    "# Sort value in ascending order\n",
    "df   = df.sort_values(\"Total\")\n",
    "\n",
    "# Get total counts and total for topX\n",
    "total      = sum(df_sorted['Total'])\n",
    "total_topX = \n",
    "df[\"Percent total\"] = df_sorted['Total']/total*100\n",
    "\n",
    "\n",
    "\n",
    "# Get the last topX rows from sorted dataframe\n",
    "df_sorted.plot.barh(x='Taxa names', y='Percent total', rot=0)\n",
    "plt.xlabel(\"Percent total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a62edfbcf39af3bfe98b03791e65c4337047c7eef8bfa85d65d3997033bead22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
