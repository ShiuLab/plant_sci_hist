{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 6.1: Arabodopsis-related topic over time__\n",
    "\n",
    "Goals here:\n",
    "- Model topics for docs mentioning Arabidopsis\n",
    "- Model Arabidopsis topics over time\n",
    "\n",
    "Issues:\n",
    "- 10/4/22: \n",
    "  - Run BERTopic and get `RuntimeError: CUDA error: no kernel image is available for execution on the device`.\n",
    "    - Reinstall torch following advices from [here](https://github.com/NVlabs/stylegan2-ada-pytorch/issues/6) for example\n",
    "    - `conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge`\n",
    "  - Once updating torch, run into problem that numba requires numpy <=1.21\n",
    "    - `conda install numpy=1.21`\n",
    "  - Get `AttributeError: 'BERTopic' object has no attribute 'generate_topic_labels'`\n",
    "    - Update BERTopic: `pip install --upgrade bertopic`\n",
    "  - Get `ImportError: cannot import name 'DatasetInfo' from 'huggingface_hub.hf_api'`\n",
    "    - Update huggingface_hub\n",
    "  - Because BERTopic is updated, the c-tf-idf matrix name is changed. This is not changed for earlier codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import\n",
    "\n",
    "In bertopic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, os, torch, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from xlsxwriter.workbook import Workbook\n",
    "from datetime import datetime\n",
    "from bisect import bisect\n",
    "from sklearn.preprocessing import normalize\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function torch.cuda.is_available() -> bool>, '1.12.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available, torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting working directory\n",
    "proj_dir   = Path.home() / \"projects/plant_sci_hist\"\n",
    "work_dir   = proj_dir / \"6_topic_model_arabidopsis/\"\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# plant science corpus with date and other info\n",
    "dir2        = proj_dir / \"2_text_classify//2_5_predict_pubmed\"\n",
    "corpus_file = dir2 / \"corpus_plant_421658.tsv.gz\"\n",
    "\n",
    "# Species match data\n",
    "dir5               = proj_dir / \"5_species_over_time/\"\n",
    "viridi_offspr_file = dir5 / \"viridiplantae_offspring_names.pickle\" \n",
    "match_csr_file     = dir5 / \"match_csr.pickle\"\n",
    "\n",
    "# BERT model to use\n",
    "model_name     = \"allenai/scibert_scivocab_uncased\"\n",
    "model_name_mod = \"-\".join(model_name.split(\"/\"))\n",
    "\n",
    "# the target term\n",
    "target = \"Arabidopsis\"\n",
    "\n",
    "## outputs for topic modeling\n",
    "# path for the corpus for the target (i.e., Arabidopsis)\n",
    "corpus_target_file = work_dir / \"corpus_arabidopsis.tsv.gz\"\n",
    "# generated during bertopic run\n",
    "docs_clean_file  = work_dir / f\"docs_clean_{target}.pickle\"\n",
    "emb_file         = work_dir / f\"embeddings_{target}_scibert.pickle\"\n",
    "topic_model_file = work_dir / f\"topic_model_{target}_{model_name_mod}\"\n",
    "topics_file      = work_dir / f\"topics_{target}_{model_name_mod}.pickle\"\n",
    "probs_file       = work_dir / f'probs_{target}_{model_name_mod}.pickle'\n",
    "topic_model_updated_file = work_dir / \\\n",
    "                              f\"topic_model_update_{target}_{model_name_mod}\"\n",
    "\n",
    "## outputs for topic model analysis\n",
    "top_50_terms_file = work_dir / f'top_50_terms_per_topic_{target}.pickle'\n",
    "top_50_terms_xlsx = work_dir / f'top_50_terms_per_topic_{target}.xlsx'\n",
    "topic_label_file  = work_dir / f\"topic_labels_{target}.txt\"\n",
    "rep_docs_file     = work_dir / f\"topic_rep_docs_{target}.tsv\"\n",
    "\n",
    "# Put the top 50 term info into different tsv files in the top_50 folder\n",
    "top_50_dir = work_dir / \"top_50\"\n",
    "top_50_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## output for topic over time analysis\n",
    "# Creat a folder to store c-tf-idf matrix for each timestamp bin\n",
    "ctfidf_dir = work_dir / \"ctfidf_over_time\"\n",
    "ctfidf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a folder to store topic over time graph for each topic\n",
    "tot_graph_dir = work_dir / 'tot_graphs'\n",
    "tot_graph_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# So PDF is saved in a format properly\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Get all Arabidopsis records___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get match_csr column that is for Arabidopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved csr\n",
    "with open(match_csr_file, \"rb\") as f:\n",
    "  match_csr = pickle.load(f)\n",
    "\n",
    "# load viridiplantae offspring names that is the same order as match_csr\n",
    "with open(viridi_offspr_file, \"rb\") as f:\n",
    "  viridi_offspr_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((421658, 26782), 26782)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimension\n",
    "match_csr.shape, len(viridi_offspr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22612"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx = viridi_offspr_names.index(target)\n",
    "target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, 421658, 50346)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get indicues of non-zero elements\n",
    "#https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html\n",
    "target_match_arr = match_csr[:,target_idx].toarray().ravel()\n",
    "target_match_idx = np.nonzero(target_match_arr)[0]\n",
    "\n",
    "# Check datatype, shape, and total (ok)\n",
    "type(target_match_arr), len(target_match_arr), np.sum(target_match_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50346, array([ 994, 1725, 2524, 3022, 3032]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_match_idx), target_match_idx[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the corpus dataframe for records mentioning Arabidopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read corpus file\n",
    "corpus_all = pd.read_csv(corpus_file, compression='gzip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Make sure the corpus contain Arabidopsis, test the 1st 1000\n",
    "#https://www.statology.org/pandas-select-rows-by-index/\n",
    "test1000     = corpus_all.loc[target_match_idx[:1000]]['txt']\n",
    "count_found = 0\n",
    "for txt in test1000:\n",
    "  if \"Arabidopsis\" in txt:\n",
    "    count_found += 1\n",
    "\n",
    "print(count_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50346, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dataframe for Arabidopsis\n",
    "corpus_target = corpus_all.loc[target_match_idx]\n",
    "corpus_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_target.to_csv(corpus_target_file, sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Topic modeling___\n",
    "\n",
    "Codes modified from `script_4_1_topic_model_v2.py` and `script_4_2_outlier_assign.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing docs\n",
      "  load processed docs\n"
     ]
    }
   ],
   "source": [
    "def clean_text(x, stop_words_dict):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    # Replace any non-alphanumric characters of any length\n",
    "    # Q: Not sure what the # character do.\n",
    "    x = re.sub(r'#[A-Za-z0-9]*', ' ', x)\n",
    "    # tokenize and rid of any token matching stop words\n",
    "    tokens = word_tokenize(x)\n",
    "    x = ' '.join([w for w in tokens if not w in stop_words_dict])\n",
    "    return x\n",
    "    \n",
    "print(\"Pre-processing docs\")\n",
    "if docs_clean_file.is_file():\n",
    "  print(\"  load processed docs\")\n",
    "  with open(docs_clean_file, \"rb\") as f:\n",
    "    docs_clean = pickle.load(f)\n",
    "else:\n",
    "  print(\"  read corpus and process docs\")\n",
    "  corpus_target_df = pd.read_csv(corpus_target_file, sep='\\t', compression='gzip')\n",
    "  \n",
    "  docs       = corpus_target_df['txt']\n",
    "  stop_words = stopwords.words('english')\n",
    "  stop_words_dict = {}\n",
    "  for i in stop_words:\n",
    "    stop_words_dict[i] = 1\n",
    "\n",
    "  docs_clean = []\n",
    "  for doc_idx in tqdm(range(len(docs))):\n",
    "    doc = docs[doc_idx]\n",
    "    docs_clean.append(clean_text(doc, stop_words_dict))\n",
    "  with open(docs_clean_file, \"wb\") as f:\n",
    "    pickle.dump(docs_clean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50346"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embeddings\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "if emb_file.is_file():\n",
    "  print(\"Load embeddings\")\n",
    "  with open(emb_file, \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "else:\n",
    "  print(\"Generate embeddings\")\n",
    "  emb_model  = SentenceTransformer(model_name)\n",
    "  embeddings = emb_model.encode(docs_clean, show_progress_bar=True)\n",
    "  # Output embeddings\n",
    "  with open(emb_file, \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN clustering setting\n",
    "min_cluster_size         = 100 # This is 500 for the full dataset run\n",
    "metric                   = 'euclidean' \n",
    "cluster_selection_method ='eom' \n",
    "prediction_data          = True \n",
    "min_samples              = 5\n",
    "\n",
    "# BERTopic setting\n",
    "calculate_probabilities = True\n",
    "n_neighbors             = 10  \n",
    "nr_topics               = 500\n",
    "n_gram_range            = (1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize HDBSCAN\n",
    "\n",
    "For reducing outliers, following [this instruction](https://maartengr.github.io/BERTopic/faq.html#how-do-i-reduce-topic-outliers)\n",
    "- Also see [HDBSCAN doc](https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#what-about-different-metrics)\n",
    "- Comparison of [distance metrics](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, \n",
    "                        metric=metric, \n",
    "                        cluster_selection_method=cluster_selection_method, \n",
    "                        prediction_data=prediction_data, \n",
    "                        min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intialize and train topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(hdbscan_model=hdbscan_model,\n",
    "                       calculate_probabilities=calculate_probabilities,\n",
    "                       n_gram_range=n_gram_range,\n",
    "                       nr_topics=nr_topics,\n",
    "                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 13:38:03,782 - BERTopic - Reduced dimensionality\n",
      "2022-10-05 13:38:16,070 - BERTopic - Clustered reduced embeddings\n",
      "2022-10-05 13:38:59,334 - BERTopic - Reduced number of topics from 59 to 59\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(docs_clean,\n",
    "                                          embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model, topics, and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already save the embeddings, so won't save it again\n",
    "topic_model.save(topic_model_file)\n",
    "\n",
    "with open(topics_file, \"wb\") as f:\n",
    "    pickle.dump(topics, f)\n",
    "\n",
    "with open(probs_file, \"wb\") as f:\n",
    "  pickle.dump(probs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model and probabilities\n",
    "\n",
    "This is more for running topic modeling on the much larger corpus. Here the targeted corpus is much smaller. But keep this anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model\n",
    "topic_model = BERTopic.load(topic_model_file)\n",
    "\n",
    "# load prob\n",
    "with open(probs_file, \"rb\") as f:\n",
    "  probs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>21778</td>\n",
       "      <td>-1_arabidopsis_plant_plants_genes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0_heterotrimeric_gprotein_rop_signaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1060</td>\n",
       "      <td>1_transformation_gene_tdna_plants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>834</td>\n",
       "      <td>2_promoter_gus_expression_gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>3_uvb_uvr8_radiation_uv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                     Name\n",
       "0     -1  21778        -1_arabidopsis_plant_plants_genes\n",
       "1      0    126  0_heterotrimeric_gprotein_rop_signaling\n",
       "2      1   1060        1_transformation_gene_tdna_plants\n",
       "3      2    834           2_promoter_gus_expression_gene\n",
       "4      3    132                  3_uvb_uvr8_radiation_uv"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Outlier assignments___\n",
    "\n",
    "Modified from `script_4_2_outlier_assign.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95 precentile probability value as threshold\n",
    "probability_threshold = np.percentile(probs, 95)\n",
    "new_topics = [np.argmax(prob) if max(prob) >= probability_threshold else -1 \n",
    "                                                            for prob in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06012394231915147"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unassigned doc number\n",
    "n_unassigned = pd.Series(new_topics).value_counts().loc[-1]\n",
    "n_unassigned/len(new_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update topic based on new topics\n",
    "topic_model.update_topics(docs_clean, new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update topic frequencies\n",
    "documents = pd.DataFrame({\"Document\": docs_clean, \"Topic\": new_topics})\n",
    "topic_model._update_topic_size(documents)\n",
    "topic_model.save(topic_model_updated_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get updated topic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model\n",
    "topic_model = BERTopic.load(topic_model_updated_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>3027</td>\n",
       "      <td>-1_arabidopsis_genes_protein_plants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>0_heterotrimeric_gprotein_protein_signaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1077</td>\n",
       "      <td>1_transformation_gene_tdna_system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>786</td>\n",
       "      <td>2_promoter_gus_expression_gene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>144</td>\n",
       "      <td>3_uvb_uvr8_radiation_uv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                         Name\n",
       "0     -1   3027          -1_arabidopsis_genes_protein_plants\n",
       "1      0    199  0_heterotrimeric_gprotein_protein_signaling\n",
       "2      1   1077            1_transformation_gene_tdna_system\n",
       "3      2    786               2_promoter_gus_expression_gene\n",
       "4      3    144                      3_uvb_uvr8_radiation_uv"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47319, 93.98760576808485)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total     = np.sum(topic_info['Count'].values)\n",
    "n_outlier   = topic_info['Count'][0]\n",
    "n_in_topics = n_total - n_outlier\n",
    "n_in_topics, n_in_topics/n_total*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Topic model analysis___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log10(topic_info[\"Count\"]), bins=200)\n",
    "plt.xlabel(\"log10(Count)\")\n",
    "plt.ylabel(\"Frquency\")\n",
    "plt.xlim(2.5,5)\n",
    "plt.savefig(work_dir / f\"fig6_topic_count_dist_{target}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-term matrix for top 50 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BERTopic in module bertopic._bertopic object:\n",
      "\n",
      "class BERTopic(builtins.object)\n",
      " |  BERTopic(language: str = 'english', top_n_words: int = 10, n_gram_range: Tuple[int, int] = (1, 1), min_topic_size: int = 10, nr_topics: Union[int, str] = None, low_memory: bool = False, calculate_probabilities: bool = False, diversity: float = None, seed_topic_list: List[List[str]] = None, embedding_model=None, umap_model: umap.umap_.UMAP = None, hdbscan_model: hdbscan.hdbscan_.HDBSCAN = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, ctfidf_model: sklearn.feature_extraction.text.TfidfTransformer = None, verbose: bool = False)\n",
      " |  \n",
      " |  BERTopic is a topic modeling technique that leverages BERT embeddings and\n",
      " |  c-TF-IDF to create dense clusters allowing for easily interpretable topics\n",
      " |  whilst keeping important words in the topic descriptions.\n",
      " |  \n",
      " |  The default embedding model is `all-MiniLM-L6-v2` when selecting `language=\"english\"`\n",
      " |  and `paraphrase-multilingual-MiniLM-L12-v2` when selecting `language=\"multilingual\"`.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      topics_ (List[int]) : The topics that are generated for each document after training or updating\n",
      " |                            the topic model. The most recent topics are tracked.\n",
      " |      probabilities_ (List[float]): The probability of the assigned topic per document. These are\n",
      " |                                    only calculated if a HDBSCAN model is used for the clustering step.\n",
      " |                                    When `calculate_probabilities=True`, then it is the probabilities\n",
      " |                                    of all topics per document.\n",
      " |      topic_sizes_ (Mapping[int, int]) : The size of each topic\n",
      " |      topic_mapper_ (TopicMapper) : A class for tracking topics and their mappings anytime they are\n",
      " |                                    merged, reduced, added, or removed.\n",
      " |      topic_representations_ (Mapping[int, Tuple[int, float]]) : The top n terms per topic and their respective\n",
      " |                                                                 c-TF-IDF values.\n",
      " |      c_tf_idf_ (csr_matrix) : The topic-term matrix as calculated through c-TF-IDF. To access its respective\n",
      " |                               words, run `.vectorizer_model.get_feature_names()`  or\n",
      " |                               `.vectorizer_model.get_feature_names_out()`\n",
      " |      topic_labels_ (Mapping[int, str]) : The default labels for each topic.\n",
      " |      custom_labels_ (List[str]) : Custom labels for each topic.\n",
      " |      topic_embeddings_ (np.ndarray) : The embeddings for each topic. It is calculated by taking the\n",
      " |                                       weighted average of word embeddings in a topic based on their c-TF-IDF values.\n",
      " |      representative_docs_ (Mapping[int, str]) : The representative documents for each topic.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  ```python\n",
      " |  from bertopic import BERTopic\n",
      " |  from sklearn.datasets import fetch_20newsgroups\n",
      " |  \n",
      " |  docs = fetch_20newsgroups(subset='all')['data']\n",
      " |  topic_model = BERTopic()\n",
      " |  topics, probabilities = topic_model.fit_transform(docs)\n",
      " |  ```\n",
      " |  \n",
      " |  If you want to use your own embedding model, use it as follows:\n",
      " |  \n",
      " |  ```python\n",
      " |  from bertopic import BERTopic\n",
      " |  from sklearn.datasets import fetch_20newsgroups\n",
      " |  from sentence_transformers import SentenceTransformer\n",
      " |  \n",
      " |  docs = fetch_20newsgroups(subset='all')['data']\n",
      " |  sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |  topic_model = BERTopic(embedding_model=sentence_model)\n",
      " |  ```\n",
      " |  \n",
      " |  Due to the stochastisch nature of UMAP, the results from BERTopic might differ\n",
      " |  and the quality can degrade. Using your own embeddings allows you to\n",
      " |  try out BERTopic several times until you find the topics that suit\n",
      " |  you best.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, language: str = 'english', top_n_words: int = 10, n_gram_range: Tuple[int, int] = (1, 1), min_topic_size: int = 10, nr_topics: Union[int, str] = None, low_memory: bool = False, calculate_probabilities: bool = False, diversity: float = None, seed_topic_list: List[List[str]] = None, embedding_model=None, umap_model: umap.umap_.UMAP = None, hdbscan_model: hdbscan.hdbscan_.HDBSCAN = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, ctfidf_model: sklearn.feature_extraction.text.TfidfTransformer = None, verbose: bool = False)\n",
      " |      BERTopic initialization\n",
      " |      \n",
      " |      Arguments:\n",
      " |          language: The main language used in your documents. The default sentence-transformers\n",
      " |                    model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of\n",
      " |                    supported languages see bertopic.backend.languages. Select\n",
      " |                    \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2`\n",
      " |                    sentence-tranformers model that supports 50+ languages.\n",
      " |          top_n_words: The number of words per topic to extract. Setting this\n",
      " |                       too high can negatively impact topic embeddings as topics\n",
      " |                       are typically best represented by at most 10 words.\n",
      " |          n_gram_range: The n-gram range for the CountVectorizer.\n",
      " |                        Advised to keep high values between 1 and 3.\n",
      " |                        More would likely lead to memory issues.\n",
      " |                        NOTE: This param will not be used if you pass in your own\n",
      " |                        CountVectorizer.\n",
      " |          min_topic_size: The minimum size of the topic. Increasing this value will lead\n",
      " |                          to a lower number of clusters/topics.\n",
      " |          nr_topics: Specifying the number of topics will reduce the initial\n",
      " |                     number of topics to the value specified. This reduction can take\n",
      " |                     a while as each reduction in topics (-1) activates a c-TF-IDF\n",
      " |                     calculation. If this is set to None, no reduction is applied. Use\n",
      " |                     \"auto\" to automatically reduce topics using HDBSCAN.\n",
      " |          low_memory: Sets UMAP low memory to True to make sure less memory is used.\n",
      " |                      NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP\n",
      " |                      this parameter will not be used.\n",
      " |          calculate_probabilities: Whether to calculate the probabilities of all topics\n",
      " |                                   per document instead of the probability of the assigned\n",
      " |                                   topic per document. This could slow down the extraction\n",
      " |                                   of topics if you have many documents (> 100_000). Set this\n",
      " |                                   only to True if you have a low amount of documents or if\n",
      " |                                   you do not mind more computation time.\n",
      " |                                   NOTE: If false you cannot use the corresponding\n",
      " |                                   visualization method `visualize_probabilities`.\n",
      " |          diversity: Whether to use MMR to diversify the resulting topic representations.\n",
      " |                     If set to None, MMR will not be used. Accepted values lie between\n",
      " |                     0 and 1 with 0 being not at all diverse and 1 being very diverse.\n",
      " |          seed_topic_list: A list of seed words per topic to converge around\n",
      " |          verbose: Changes the verbosity of the model, Set to True if you want\n",
      " |                   to track the stages of the model.\n",
      " |          embedding_model: Use a custom embedding model.\n",
      " |                           The following backends are currently supported\n",
      " |                             * SentenceTransformers\n",
      " |                             * Flair\n",
      " |                             * Spacy\n",
      " |                             * Gensim\n",
      " |                             * USE (TF-Hub)\n",
      " |                           You can also pass in a string that points to one of the following\n",
      " |                           sentence-transformers models:\n",
      " |                             * https://www.sbert.net/docs/pretrained_models.html\n",
      " |          umap_model: Pass in a UMAP model to be used instead of the default.\n",
      " |                      NOTE: You can also pass in any dimensionality reduction algorithm as long\n",
      " |                      as it has `.fit` and `.transform` functions.\n",
      " |          hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default\n",
      " |                         NOTE: You can also pass in any clustering algorithm as long as it has\n",
      " |                         `.fit` and `.predict` functions along with the `.labels_` variable.\n",
      " |          vectorizer_model: Pass in a custom `CountVectorizer` instead of the default model.\n",
      " |          ctfidf_model: Pass in a custom ClassTfidfTransformer instead of the default model.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a string representation of the current object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: Human readable representation of the most important model parameters.\n",
      " |               The parameters that represent models are ignored due to their length.\n",
      " |  \n",
      " |  find_topics(self, search_term: str, top_n: int = 5) -> Tuple[List[int], List[float]]\n",
      " |      Find topics most similar to a search_term\n",
      " |      \n",
      " |      Creates an embedding for search_term and compares that with\n",
      " |      the topic embeddings. The most similar topics are returned\n",
      " |      along with their similarity values.\n",
      " |      \n",
      " |      The search_term can be of any size but since it compares\n",
      " |      with the topic representation it is advised to keep it\n",
      " |      below 5 words.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          search_term: the term you want to use to search for topics\n",
      " |          top_n: the number of topics to return\n",
      " |      \n",
      " |      Returns:\n",
      " |          similar_topics: the most similar topics from high to low\n",
      " |          similarity: the similarity scores from high to low\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      You can use the underlying embedding model to find topics that\n",
      " |      best represent the search term:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics, similarity = topic_model.find_topics(\"sports\", top_n=5)\n",
      " |      ```\n",
      " |      \n",
      " |      Note that the search query is typically more accurate if the\n",
      " |      search_term consists of a phrase or multiple words.\n",
      " |  \n",
      " |  fit(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None)\n",
      " |      Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic().fit(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings, use it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  fit_transform(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None) -> Tuple[List[int], Optional[numpy.ndarray]]\n",
      " |      Fit the models on a collection of documents, generate topics, and return the docs with topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Returns:\n",
      " |          predictions: Topic predictions for each documents\n",
      " |          probabilities: The probability of the assigned topic per document.\n",
      " |                         If `calculate_probabilities` in BERTopic is set to True, then\n",
      " |                         it calculates the probabilities of all topics across all documents\n",
      " |                         instead of only the assigned topic. This, however, slows down\n",
      " |                         computation and may increase memory usage.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings, use it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  generate_topic_labels(self, nr_words: int = 3, topic_prefix: bool = True, word_length: int = None, separator: str = '_') -> List[str]\n",
      " |      Get labels for each topic in a user-defined format\n",
      " |      \n",
      " |      Arguments:\n",
      " |          original_labels:\n",
      " |          nr_words: Top `n` words per topic to use\n",
      " |          topic_prefix: Whether to use the topic ID as a prefix.\n",
      " |                      If set to True, the topic ID will be separated\n",
      " |                      using the `separator`\n",
      " |          word_length: The maximum length of each word in the topic label.\n",
      " |                      Some words might be relatively long and setting this\n",
      " |                      value helps to make sure that all labels have relatively\n",
      " |                      similar lengths.\n",
      " |          separator: The string with which the words and topic prefix will be\n",
      " |                  separated. Underscores are the default but a nice alternative\n",
      " |                  is `\", \"`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic_labels: A list of topic labels sorted from the lowest topic ID to the highest.\n",
      " |                      If the topic model was trained using HDBSCAN, the lowest topic ID is -1,\n",
      " |                      otherwise it is 0.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To create our custom topic labels, usage is rather straightforward:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \")\n",
      " |      ```\n",
      " |  \n",
      " |  get_params(self, deep: bool = False) -> Mapping[str, Any]\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Adapted from:\n",
      " |          https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178\n",
      " |      \n",
      " |      Arguments:\n",
      " |          deep: bool, default=True\n",
      " |                If True, will return the parameters for this estimator and\n",
      " |                contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          out: Parameter names mapped to their values.\n",
      " |  \n",
      " |  get_representative_docs(self, topic: int = None) -> List[str]\n",
      " |      Extract representative documents per topic\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want\n",
      " |                 the representative documents\n",
      " |      \n",
      " |      Returns:\n",
      " |          Representative documents of the chosen topic\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To extract the representative docs of all topics:\n",
      " |      \n",
      " |      ```python\n",
      " |      representative_docs = topic_model.get_representative_docs()\n",
      " |      ```\n",
      " |      \n",
      " |      To get the representative docs of a single topic:\n",
      " |      \n",
      " |      ```python\n",
      " |      representative_docs = topic_model.get_representative_docs(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic(self, topic: int) -> Union[Mapping[str, Tuple[str, float]], bool]\n",
      " |      Return top n words for a specific topic and their c-TF-IDF scores\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want its representation\n",
      " |      \n",
      " |      Returns:\n",
      " |          The top n words for a specific word and its respective c-TF-IDF scores\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic = topic_model.get_topic(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic_freq(self, topic: int = None) -> Union[pandas.core.frame.DataFrame, int]\n",
      " |      Return the the size of topics (descending order)\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want the frequency\n",
      " |      \n",
      " |      Returns:\n",
      " |          Either the frequency of a single topic or dataframe with\n",
      " |          the frequencies of all topics\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To extract the frequency of all topics:\n",
      " |      \n",
      " |      ```python\n",
      " |      frequency = topic_model.get_topic_freq()\n",
      " |      ```\n",
      " |      \n",
      " |      To get the frequency of a single topic:\n",
      " |      \n",
      " |      ```python\n",
      " |      frequency = topic_model.get_topic_freq(12)\n",
      " |      ```\n",
      " |  \n",
      " |  get_topic_info(self, topic: int = None) -> pandas.core.frame.DataFrame\n",
      " |      Get information about each topic including its ID, frequency, and name.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic: A specific topic for which you want the frequency\n",
      " |      \n",
      " |      Returns:\n",
      " |          info: The information relating to either a single topic or all topics\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      info_df = topic_model.get_topic_info()\n",
      " |      ```\n",
      " |  \n",
      " |  get_topics(self) -> Mapping[str, Tuple[str, float]]\n",
      " |      Return topics with top n words and their c-TF-IDF score\n",
      " |      \n",
      " |      Returns:\n",
      " |          self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      all_topics = topic_model.get_topics()\n",
      " |      ```\n",
      " |  \n",
      " |  hierarchical_topics(self, docs: List[int], linkage_function: Callable[[scipy.sparse._csr.csr_matrix], numpy.ndarray] = None, distance_function: Callable[[scipy.sparse._csr.csr_matrix], scipy.sparse._csr.csr_matrix] = None) -> pandas.core.frame.DataFrame\n",
      " |      Create a hierarchy of topics\n",
      " |      \n",
      " |      To create this hierarchy, BERTopic needs to be already fitted once.\n",
      " |      Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF\n",
      " |      representation using `scipy.cluster.hierarchy.linkage`.\n",
      " |      \n",
      " |      Based on that hierarchy, we calculate the topic representation at each\n",
      " |      merged step. This is a local representation, as we only assume that the\n",
      " |      chosen step is merged and not all others which typically improves the\n",
      " |      topic representation.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          linkage_function: The linkage function to use. Default is:\n",
      " |                          `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\n",
      " |          distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\n",
      " |                              `lambda x: 1 - cosine_similarity(x)`\n",
      " |      \n",
      " |      Returns:\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      A custom linkage function can be used as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from scipy.cluster import hierarchy as sch\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      \n",
      " |      # Hierarchical topics\n",
      " |      linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function)\n",
      " |      ```\n",
      " |  \n",
      " |  merge_topics(self, docs: List[str], topics_to_merge: List[Union[Iterable[int], int]]) -> None\n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics_to_merge: Either a list of topics or a list of list of topics\n",
      " |                          to merge. For example:\n",
      " |                              [1, 2, 3] will merge topics 1, 2 and 3\n",
      " |                              [[1, 2], [3, 4]] will merge topics 1 and 2, and\n",
      " |                              separately merge topics 3 and 4.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      If you want to merge topics 1, 2, and 3:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_to_merge = [1, 2, 3]\n",
      " |      topic_model.merge_topics(docs, topics_to_merge)\n",
      " |      ```\n",
      " |      \n",
      " |      or if you want to merge topics 1 and 2, and separately\n",
      " |      merge topics 3 and 4:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_to_merge = [[1, 2]\n",
      " |                          [3, 4]]\n",
      " |      topic_model.merge_topics(docs, topics_to_merge)\n",
      " |      ```\n",
      " |  \n",
      " |  partial_fit(self, documents: List[str], embeddings: numpy.ndarray = None, y: Union[List[int], numpy.ndarray] = None)\n",
      " |      Fit BERTopic on a subset of the data and perform online learning\n",
      " |      with batch-like data.\n",
      " |      \n",
      " |      Online topic modeling in BERTopic is performed by using dimensionality\n",
      " |      reduction and cluster algorithms that support a `partial_fit` method\n",
      " |      in order to incrementally train the topic model.\n",
      " |      \n",
      " |      Likewise, the `bertopic.vectorizers.OnlineCountVectorizer` is used\n",
      " |      to dynamically update its vocabulary when presented with new data.\n",
      " |      It has several parameters for modeling decay and updating the\n",
      " |      representations.\n",
      " |      \n",
      " |      In other words, although the main algorithm stays the same, the training\n",
      " |      procedure now works as follows:\n",
      " |      \n",
      " |      For each subset of the data:\n",
      " |      \n",
      " |      1. Generate embeddings with a pre-traing language model\n",
      " |      2. Incrementally update the dimensionality reduction algorithm with `partial_fit`\n",
      " |      3. Incrementally update the cluster algorithm with `partial_fit`\n",
      " |      4. Incrementally update the OnlineCountVectorizer and apply some form of decay\n",
      " |      \n",
      " |      Note that it is advised to use `partial_fit` with batches and\n",
      " |      not single documents for the best performance.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model\n",
      " |          y: The target class for (semi)-supervised modeling. Use -1 if no class for a\n",
      " |             specific instance is specified.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sklearn.cluster import MiniBatchKMeans\n",
      " |      from sklearn.decomposition import IncrementalPCA\n",
      " |      from bertopic.vectorizers import OnlineCountVectorizer\n",
      " |      from bertopic import BERTopic\n",
      " |      \n",
      " |      # Prepare documents\n",
      " |      docs = fetch_20newsgroups(subset=subset,  remove=('headers', 'footers', 'quotes'))[\"data\"]\n",
      " |      \n",
      " |      # Prepare sub-models that support online learning\n",
      " |      umap_model = IncrementalPCA(n_components=5)\n",
      " |      cluster_model = MiniBatchKMeans(n_clusters=50, random_state=0)\n",
      " |      vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01)\n",
      " |      \n",
      " |      topic_model = BERTopic(umap_model=umap_model,\n",
      " |                             hdbscan_model=cluster_model,\n",
      " |                             vectorizer_model=vectorizer_model)\n",
      " |      \n",
      " |      # Incrementally fit the topic model by training on 1000 documents at a time\n",
      " |      for index in range(0, len(docs), 1000):\n",
      " |          topic_model.partial_fit(docs[index: index+1000])\n",
      " |      ```\n",
      " |  \n",
      " |  reduce_topics(self, docs: List[str], nr_topics: int = 20) -> None\n",
      " |      Further reduce the number of topics to nr_topics.\n",
      " |      \n",
      " |      The number of topics is further reduced by calculating the c-TF-IDF matrix\n",
      " |      of the documents and then reducing them by iteratively merging the least\n",
      " |      frequent topic with the most similar one based on their c-TF-IDF matrices.\n",
      " |      The topics, their sizes, and representations are updated.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The docs you used when calling either `fit` or `fit_transform`\n",
      " |          nr_topics: The number of topics you want reduced to\n",
      " |      \n",
      " |      Updates:\n",
      " |          topics_ : Assigns topics to their merged representations.\n",
      " |          probabilities_ : Assigns probabilities to their merged representations.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      You can further reduce the topics by passing the documents with its\n",
      " |      topics and probabilities (if they were calculated):\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.reduce_topics(docs, nr_topics=30)\n",
      " |      ```\n",
      " |      \n",
      " |      You can then access the updated topics and probabilities with:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics = topic_model.topics_\n",
      " |      probabilities = topic_model.probabilities_\n",
      " |      ```\n",
      " |  \n",
      " |  save(self, path: str, save_embedding_model: bool = True) -> None\n",
      " |      Saves the model to the specified path\n",
      " |      \n",
      " |      Arguments:\n",
      " |          path: the location and name of the file you want to save\n",
      " |          save_embedding_model: Whether to save the embedding model in this class\n",
      " |                                as you might have selected a local model or one that\n",
      " |                                is downloaded automatically from the cloud.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.save(\"my_model\")\n",
      " |      ```\n",
      " |      \n",
      " |      or if you do not want the embedding_model to be saved locally:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.save(\"my_model\", save_embedding_model=False)\n",
      " |      ```\n",
      " |  \n",
      " |  set_topic_labels(self, topic_labels: Union[List[str], Mapping[int, str]]) -> None\n",
      " |      Set custom topic labels in your fitted BERTopic model\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_labels: If a list of topic labels, it should contain the same number\n",
      " |                      of labels as there are topics. This must be ordered\n",
      " |                      from the topic with the lowest ID to the highest ID,\n",
      " |                      including topic -1 if it exists.\n",
      " |                      If a dictionary of `topic ID`: `topic_label`, it can have\n",
      " |                      any number of topics as it will only map the topics found\n",
      " |                      in the dictionary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      First, we define our topic labels with `.get_topic_labels` in which\n",
      " |      we can customize our topic labels:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_labels = topic_model.get_topic_labels(nr_words=2,\n",
      " |                                                  topic_prefix=True,\n",
      " |                                                  word_length=10,\n",
      " |                                                  separator=\", \")\n",
      " |      ```\n",
      " |      \n",
      " |      Then, we pass these `topic_labels` to our topic model which\n",
      " |      can be accessed at any time with `.custom_labels_`:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.set_topic_labels(topic_labels)\n",
      " |      topic_model.custom_labels_\n",
      " |      ```\n",
      " |      \n",
      " |      You might want to change only a few topic labels instead of all of them.\n",
      " |      To do so, you can pass a dictionary where the keys are the topic IDs and\n",
      " |      its keys the topic labels:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"})\n",
      " |      topic_model.custom_labels_\n",
      " |      ```\n",
      " |  \n",
      " |  topics_over_time(self, docs: List[str], timestamps: Union[List[str], List[int]], nr_bins: int = None, datetime_format: str = None, evolution_tuning: bool = True, global_tuning: bool = True) -> pandas.core.frame.DataFrame\n",
      " |      Create topics over time\n",
      " |      \n",
      " |      To create the topics over time, BERTopic needs to be already fitted once.\n",
      " |      From the fitted models, the c-TF-IDF representations are calculate at\n",
      " |      each timestamp t. Then, the c-TF-IDF representations at timestamp t are\n",
      " |      averaged with the global c-TF-IDF representations in order to fine-tune the\n",
      " |      local representations.\n",
      " |      \n",
      " |      NOTE:\n",
      " |          Make sure to use a limited number of unique timestamps (<100) as the\n",
      " |          c-TF-IDF representation will be calculated at each single unique timestamp.\n",
      " |          Having a large number of unique timestamps can take some time to be calculated.\n",
      " |          Moreover, there aren't many use-cased where you would like to see the difference\n",
      " |          in topic representations over more than 100 different timestamps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          timestamps: The timestamp of each document. This can be either a list of strings or ints.\n",
      " |                      If it is a list of strings, then the datetime format will be automatically\n",
      " |                      inferred. If it is a list of ints, then the documents will be ordered by\n",
      " |                      ascending order.\n",
      " |          nr_bins: The number of bins you want to create for the timestamps. The left interval will\n",
      " |                   be chosen as the timestamp. An additional column will be created with the\n",
      " |                   entire interval.\n",
      " |          datetime_format: The datetime format of the timestamps if they are strings, eg %d/%m/%Y.\n",
      " |                           Set this to None if you want to have it automatically detect the format.\n",
      " |                           See strftime documentation for more information on choices:\n",
      " |                           https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
      " |          evolution_tuning: Fine-tune each topic representation at timestamp *t* by averaging its\n",
      " |                            c-TF-IDF matrix with the c-TF-IDF matrix at timestamp *t-1*. This creates\n",
      " |                            evolutionary topic representations.\n",
      " |          global_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix\n",
      " |                     with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
      " |                     topic representations that could not be found in the documents at timestamp *t*.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topics_over_time: A dataframe that contains the topic, words, and frequency of topic\n",
      " |                            at timestamp *t*.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      The timestamps variable represent the timestamp of each document. If you have over\n",
      " |      100 unique timestamps, it is advised to bin the timestamps as shown below:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      topics_over_time = topic_model.topics_over_time(docs, timestamps, nr_bins=20)\n",
      " |      ```\n",
      " |  \n",
      " |  topics_per_class(self, docs: List[str], classes: Union[List[int], List[str]], global_tuning: bool = True) -> pandas.core.frame.DataFrame\n",
      " |      Create topics per class\n",
      " |      \n",
      " |      To create the topics per class, BERTopic needs to be already fitted once.\n",
      " |      From the fitted models, the c-TF-IDF representations are calculate at\n",
      " |      each class c. Then, the c-TF-IDF representations at class c are\n",
      " |      averaged with the global c-TF-IDF representations in order to fine-tune the\n",
      " |      local representations. This can be turned off if the pure representation is\n",
      " |      needed.\n",
      " |      \n",
      " |      NOTE:\n",
      " |          Make sure to use a limited number of unique classes (<100) as the\n",
      " |          c-TF-IDF representation will be calculated at each single unique class.\n",
      " |          Having a large number of unique classes can take some time to be calculated.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          classes: The class of each document. This can be either a list of strings or ints.\n",
      " |          global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix\n",
      " |                     with the global c-TF-IDF matrix. Turn this off if you want to prevent words in\n",
      " |                     topic representations that could not be found in the documents at timestamp t.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topics_per_class: A dataframe that contains the topic, words, and frequency of topics\n",
      " |                            for each class.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      topics_per_class = topic_model.topics_per_class(docs, classes)\n",
      " |      ```\n",
      " |  \n",
      " |  transform(self, documents: Union[str, List[str]], embeddings: numpy.ndarray = None) -> Tuple[List[int], numpy.ndarray]\n",
      " |      After having fit a model, use transform to predict new instances\n",
      " |      \n",
      " |      Arguments:\n",
      " |          documents: A single document or a list of documents to fit on\n",
      " |          embeddings: Pre-trained document embeddings. These can be used\n",
      " |                      instead of the sentence-transformer model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          predictions: Topic predictions for each documents\n",
      " |          probabilities: The topic probability distribution which is returned by default.\n",
      " |                         If `calculate_probabilities` in BERTopic is set to False, then the\n",
      " |                         probabilities are not calculated to speed up computation and\n",
      " |                         decrease memory usage.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      \n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      topic_model = BERTopic().fit(docs)\n",
      " |      topics, probs = topic_model.transform(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to use your own embeddings:\n",
      " |      \n",
      " |      ```python\n",
      " |      from bertopic import BERTopic\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      \n",
      " |      # Create embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all')['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
      " |      \n",
      " |      # Create topic model\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      topics, probs = topic_model.transform(docs, embeddings)\n",
      " |      ```\n",
      " |  \n",
      " |  update_topics(self, docs: List[str], topics: List[int] = None, n_gram_range: Tuple[int, int] = None, vectorizer_model: sklearn.feature_extraction.text.CountVectorizer = None, ctfidf_model: bertopic.vectorizers._ctfidf.ClassTfidfTransformer = None)\n",
      " |      Updates the topic representation by recalculating c-TF-IDF with the new\n",
      " |      parameters as defined in this function.\n",
      " |      \n",
      " |      When you have trained a model and viewed the topics and the words that represent them,\n",
      " |      you might not be satisfied with the representation. Perhaps you forgot to remove\n",
      " |      stop_words or you want to try out a different n_gram_range. This function allows you\n",
      " |      to update the topic representation after they have been formed.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: A list of topics where each topic is related to a document in `docs`.\n",
      " |                  Use this variable to change or map the topics.\n",
      " |                  NOTE: Using a custom list of topic assignments may lead to errors if\n",
      " |                        topic reduction techniques are used afterwards. Make sure that\n",
      " |                        manually assigning topics is the last step in the pipeline\n",
      " |          n_gram_range: The n-gram range for the CountVectorizer.\n",
      " |          vectorizer_model: Pass in your own CountVectorizer from scikit-learn\n",
      " |          ctfidf_model: Pass in your own c-TF-IDF model to update the representations\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      In order to update the topic representation, you will need to first fit the topic\n",
      " |      model and extract topics from them. Based on these, you can update the representation:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.update_topics(docs, n_gram_range=(2, 3))\n",
      " |      ```\n",
      " |      \n",
      " |      You can also use a custom vectorizer to update the representation:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.feature_extraction.text import CountVectorizer\n",
      " |      vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
      " |      topic_model.update_topics(docs, vectorizer_model=vectorizer_model)\n",
      " |      ```\n",
      " |      \n",
      " |      You can also use this function to change or map the topics to something else.\n",
      " |      You can update them as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.update_topics(docs, my_updated_topics)\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_barchart(self, topics: List[int] = None, top_n_topics: int = 8, n_words: int = 5, custom_labels: bool = False, title: str = 'Topic Word Scores', width: int = 250, height: int = 250) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a barchart of selected topics\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize.\n",
      " |          top_n_topics: Only select the top n most frequent topics.\n",
      " |          n_words: Number of words to show in a topic\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using \n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          title: Title of the plot.\n",
      " |          width: The width of each figure.\n",
      " |          height: The height of each figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the barchart of selected topics\n",
      " |      simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_barchart()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_barchart()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_distribution(self, probabilities: numpy.ndarray, min_probability: float = 0.015, custom_labels: bool = False, width: int = 800, height: int = 600) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize the distribution of topic probabilities\n",
      " |      \n",
      " |      Arguments:\n",
      " |          probabilities: An array of probability scores\n",
      " |          min_probability: The minimum probability score to visualize.\n",
      " |                           All others are ignored.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                         `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      Make sure to fit the model before and only input the\n",
      " |      probabilities of a single document:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_distribution(topic_model.probabilities_[0])\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_distribution(topic_model.probabilities_[0])\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_documents(self, docs: List[str], topics: List[int] = None, embeddings: numpy.ndarray = None, reduced_embeddings: numpy.ndarray = None, sample: float = None, hide_annotations: bool = False, hide_document_hover: bool = False, custom_labels: bool = False, width: int = 1200, height: int = 750) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize documents and their topics in 2D\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_model: A fitted BERTopic instance.\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          topics: A selection of topics to visualize.\n",
      " |                  Not to be confused with the topics that you get from `.fit_transform`.\n",
      " |                  For example, if you want to visualize only topics 1 through 5:\n",
      " |                  `topics = [1, 2, 3, 4, 5]`.\n",
      " |          embeddings: The embeddings of all documents in `docs`.\n",
      " |          reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
      " |          sample: The percentage of documents in each topic that you would like to keep.\n",
      " |                  Value can be between 0 and 1. Setting this value to, for example,\n",
      " |                  0.1 (10% of documents in each topic) makes it easier to visualize\n",
      " |                  millions of documents as a subset is chosen.\n",
      " |          hide_annotations: Hide the names of the traces on top of each cluster.\n",
      " |          hide_document_hover: Hide the content of the documents when hovering over\n",
      " |                              specific points. Helps to speed up generation of visualization.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_documents(docs)\n",
      " |      ```\n",
      " |      \n",
      " |      Do note that this re-calculates the embeddings and reduces them to 2D.\n",
      " |      The advised and prefered pipeline for using this function is as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      from bertopic import BERTopic\n",
      " |      from umap import UMAP\n",
      " |      \n",
      " |      # Prepare embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
      " |      \n",
      " |      # Train BERTopic\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      \n",
      " |      # Reduce dimensionality of embeddings, this step is optional\n",
      " |      # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
      " |      \n",
      " |      # Run the visualization with the original embeddings\n",
      " |      topic_model.visualize_documents(docs, embeddings=embeddings)\n",
      " |      \n",
      " |      # Or, if you have reduced the original embeddings already:\n",
      " |      topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      <iframe src=\"../../getting_started/visualization/documents.html\"\n",
      " |      style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_heatmap(self, topics: List[int] = None, top_n_topics: int = None, n_clusters: int = None, custom_labels: bool = False, width: int = 800, height: int = 800) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a heatmap of the topic's similarity matrix\n",
      " |      \n",
      " |      Based on the cosine similarity matrix between topic embeddings,\n",
      " |      a heatmap is created showing the similarity between topics.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize.\n",
      " |          top_n_topics: Only select the top n most frequent topics.\n",
      " |          n_clusters: Create n clusters and order the similarity\n",
      " |                      matrix by those clusters.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the similarity matrix of\n",
      " |      topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_heatmap()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_heatmap()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_hierarchical_documents(self, docs: List[str], hierarchical_topics: pandas.core.frame.DataFrame, topics: List[int] = None, embeddings: numpy.ndarray = None, reduced_embeddings: numpy.ndarray = None, sample: Union[float, int] = None, hide_annotations: bool = False, hide_document_hover: bool = True, nr_levels: int = 10, custom_labels: bool = False, width: int = 1200, height: int = 750) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize documents and their topics in 2D at different levels of hierarchy\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The documents you used when calling either `fit` or `fit_transform`\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children\n",
      " |          topics: A selection of topics to visualize.\n",
      " |                  Not to be confused with the topics that you get from `.fit_transform`.\n",
      " |                  For example, if you want to visualize only topics 1 through 5:\n",
      " |                  `topics = [1, 2, 3, 4, 5]`.\n",
      " |          embeddings: The embeddings of all documents in `docs`.\n",
      " |          reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
      " |          sample: The percentage of documents in each topic that you would like to keep.\n",
      " |                  Value can be between 0 and 1. Setting this value to, for example,\n",
      " |                  0.1 (10% of documents in each topic) makes it easier to visualize\n",
      " |                  millions of documents as a subset is chosen.\n",
      " |          hide_annotations: Hide the names of the traces on top of each cluster.\n",
      " |          hide_document_hover: Hide the content of the documents when hovering over\n",
      " |                              specific points. Helps to speed up generation of visualizations.\n",
      " |          nr_levels: The number of levels to be visualized in the hierarchy. First, the distances\n",
      " |                  in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with\n",
      " |                  equal length. Then, for each list of distances, the merged topics are selected that\n",
      " |                  have a distance less or equal to the maximum distance of the selected list of distances.\n",
      " |                  NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to\n",
      " |                  the length of `hierarchical_topics`.\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                         `topic_model.set_topic_labels`.\n",
      " |                         NOTE: Custom labels are only generated for the original\n",
      " |                         un-merged topics.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics)\n",
      " |      ```\n",
      " |      \n",
      " |      Do note that this re-calculates the embeddings and reduces them to 2D.\n",
      " |      The advised and prefered pipeline for using this function is as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      from sklearn.datasets import fetch_20newsgroups\n",
      " |      from sentence_transformers import SentenceTransformer\n",
      " |      from bertopic import BERTopic\n",
      " |      from umap import UMAP\n",
      " |      \n",
      " |      # Prepare embeddings\n",
      " |      docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
      " |      sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
      " |      embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
      " |      \n",
      " |      # Train BERTopic and extract hierarchical topics\n",
      " |      topic_model = BERTopic().fit(docs, embeddings)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
      " |      \n",
      " |      # Reduce dimensionality of embeddings, this step is optional\n",
      " |      # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
      " |      \n",
      " |      # Run the visualization with the original embeddings\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings)\n",
      " |      \n",
      " |      # Or, if you have reduced the original embeddings already:\n",
      " |      topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\"\n",
      " |      style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_hierarchy(self, orientation: str = 'left', topics: List[int] = None, top_n_topics: int = None, custom_labels: bool = False, width: int = 1000, height: int = 600, hierarchical_topics: pandas.core.frame.DataFrame = None, linkage_function: Callable[[scipy.sparse._csr.csr_matrix], numpy.ndarray] = None, distance_function: Callable[[scipy.sparse._csr.csr_matrix], scipy.sparse._csr.csr_matrix] = None, color_threshold: int = 1) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize a hierarchical structure of the topics\n",
      " |      \n",
      " |      A ward linkage function is used to perform the\n",
      " |      hierarchical clustering based on the cosine distance\n",
      " |      matrix between topic embeddings.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topic_model: A fitted BERTopic instance.\n",
      " |          orientation: The orientation of the figure.\n",
      " |                      Either 'left' or 'bottom'\n",
      " |          topics: A selection of topics to visualize\n",
      " |          top_n_topics: Only select the top n most frequent topics\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |                     NOTE: Custom labels are only generated for the original\n",
      " |                     un-merged topics.\n",
      " |          width: The width of the figure. Only works if orientation is set to 'left'\n",
      " |          height: The height of the figure. Only works if orientation is set to 'bottom'\n",
      " |          hierarchical_topics: A dataframe that contains a hierarchy of topics\n",
      " |                              represented by their parents and their children.\n",
      " |                              NOTE: The hierarchical topic names are only visualized\n",
      " |                              if both `topics` and `top_n_topics` are not set.\n",
      " |          linkage_function: The linkage function to use. Default is:\n",
      " |                          `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)`\n",
      " |                          NOTE: Make sure to use the same `linkage_function` as used\n",
      " |                          in `topic_model.hierarchical_topics`.\n",
      " |          distance_function: The distance function to use on the c-TF-IDF matrix. Default is:\n",
      " |                          `lambda x: 1 - cosine_similarity(x)`\n",
      " |                          NOTE: Make sure to use the same `distance_function` as used\n",
      " |                          in `topic_model.hierarchical_topics`.\n",
      " |          color_threshold: Value at which the separation of clusters will be made which\n",
      " |                       will result in different colors for different clusters.\n",
      " |                       A higher value will typically lead in less colored clusters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the hierarchical structure of\n",
      " |      topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_hierarchy()\n",
      " |      ```\n",
      " |      \n",
      " |      If you also want the labels visualized of hierarchical topics,\n",
      " |      run the following:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Extract hierarchical topics and their representations\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
      " |      \n",
      " |      # Visualize these representations\n",
      " |      topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_hierarchy()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      <iframe src=\"../../getting_started/visualization/hierarchy.html\"\n",
      " |      style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe>\n",
      " |  \n",
      " |  visualize_term_rank(self, topics: List[int] = None, log_scale: bool = False, custom_labels: bool = False, width: int = 800, height: int = 500) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize the ranks of all terms across all topics\n",
      " |      \n",
      " |      Each topic is represented by a set of words. These words, however,\n",
      " |      do not all equally represent the topic. This visualization shows\n",
      " |      how many words are needed to represent a topic and at which point\n",
      " |      the beneficial effect of adding words starts to decline.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize. These will be colored\n",
      " |                  red where all others will be colored black.\n",
      " |          log_scale: Whether to represent the ranking on a log scale\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          fig: A plotly figure\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the ranks of all words across\n",
      " |      all topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_term_rank()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_term_rank()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |      \n",
      " |      Reference:\n",
      " |      \n",
      " |      This visualization was heavily inspired by the\n",
      " |      \"Term Probability Decline\" visualization found in an\n",
      " |      analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/).\n",
      " |      Reference to that specific analysis can be found\n",
      " |      [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html).\n",
      " |  \n",
      " |  visualize_topics(self, topics: List[int] = None, top_n_topics: int = None, width: int = 650, height: int = 650) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics, their sizes, and their corresponding words\n",
      " |      \n",
      " |      This visualization is highly inspired by LDAvis, a great visualization\n",
      " |      technique typically reserved for LDA.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics: A selection of topics to visualize\n",
      " |          top_n_topics: Only select the top n most frequent topics\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the topics simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topic_model.visualize_topics()\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics()\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_topics_over_time(self, topics_over_time: pandas.core.frame.DataFrame, top_n_topics: int = None, topics: List[int] = None, normalize_frequency: bool = False, custom_labels: bool = False, width: int = 1250, height: int = 450) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics over time\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics_over_time: The topics you would like to be visualized with the\n",
      " |                            corresponding topic representation\n",
      " |          top_n_topics: To visualize the most frequent topics instead of all\n",
      " |          topics: Select which topics you would like to be visualized\n",
      " |          normalize_frequency: Whether to normalize each topic's frequency individually\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A plotly.graph_objects.Figure including all traces\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the topics over time, simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_over_time = topic_model.topics_over_time(docs, timestamps)\n",
      " |      topic_model.visualize_topics_over_time(topics_over_time)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics_over_time(topics_over_time)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  visualize_topics_per_class(self, topics_per_class: pandas.core.frame.DataFrame, top_n_topics: int = 10, topics: List[int] = None, normalize_frequency: bool = False, custom_labels: bool = False, width: int = 1250, height: int = 900) -> plotly.graph_objs._figure.Figure\n",
      " |      Visualize topics per class\n",
      " |      \n",
      " |      Arguments:\n",
      " |          topics_per_class: The topics you would like to be visualized with the\n",
      " |                            corresponding topic representation\n",
      " |          top_n_topics: To visualize the most frequent topics instead of all\n",
      " |          topics: Select which topics you would like to be visualized\n",
      " |          normalize_frequency: Whether to normalize each topic's frequency individually\n",
      " |          custom_labels: Whether to use custom topic labels that were defined using\n",
      " |                     `topic_model.set_topic_labels`.\n",
      " |          width: The width of the figure.\n",
      " |          height: The height of the figure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A plotly.graph_objects.Figure including all traces\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      To visualize the topics per class, simply run:\n",
      " |      \n",
      " |      ```python\n",
      " |      topics_per_class = topic_model.topics_per_class(docs, classes)\n",
      " |      topic_model.visualize_topics_per_class(topics_per_class)\n",
      " |      ```\n",
      " |      \n",
      " |      Or if you want to save the resulting figure:\n",
      " |      \n",
      " |      ```python\n",
      " |      fig = topic_model.visualize_topics_per_class(topics_per_class)\n",
      " |      fig.write_html(\"path/to/file.html\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(path: str, embedding_model=None) from builtins.type\n",
      " |      Loads the model from the specified path\n",
      " |      \n",
      " |      Arguments:\n",
      " |          path: the location and name of the BERTopic file you want to load\n",
      " |          embedding_model: If the embedding_model was not saved to save space or to load\n",
      " |                           it in from the cloud, you can load it in by specifying it here.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      BERTopic.load(\"my_model\")\n",
      " |      ```\n",
      " |      \n",
      " |      or if you did not save the embedding model:\n",
      " |      \n",
      " |      ```python\n",
      " |      BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_topic_tree(hier_topics: pandas.core.frame.DataFrame, max_distance: float = None, tight_layout: bool = False) -> str\n",
      " |      Extract the topic tree such that it can be printed\n",
      " |      \n",
      " |      Arguments:\n",
      " |          hier_topics: A dataframe containing the structure of the topic tree.\n",
      " |                      This is the output of `topic_model.hierachical_topics()`\n",
      " |          max_distance: The maximum distance between two topics. This value is\n",
      " |                      based on the Distance column in `hier_topics`.\n",
      " |          tight_layout: Whether to use a tight layout (narrow width) for\n",
      " |                      easier readability if you have hundreds of topics.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tree that has the following structure when printed:\n",
      " |              .\n",
      " |              .\n",
      " |              health_medical_disease_patients_hiv\n",
      " |                  patients_medical_disease_candida_health\n",
      " |                      candida_yeast_infection_gonorrhea_infections  Topic: 48\n",
      " |                      patients_disease_cancer_medical_doctor\n",
      " |                           hiv_medical_cancer_patients_doctor  Topic: 34\n",
      " |                           pain_drug_patients_disease_diet  Topic: 26\n",
      " |                  health_newsgroup_tobacco_vote_votes  Topic: 9\n",
      " |      \n",
      " |          The blocks () indicate that the topic is one you can directly access\n",
      " |          from `topic_model.get_topic`. In other words, they are the original un-grouped topics.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Train model\n",
      " |      from bertopic import BERTopic\n",
      " |      topic_model = BERTopic()\n",
      " |      topics, probs = topic_model.fit_transform(docs)\n",
      " |      hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
      " |      \n",
      " |      # Print topic tree\n",
      " |      tree = topic_model.get_topic_tree(hierarchical_topics)\n",
      " |      print(tree)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse._csr.csr_matrix, (59, 3118226))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse matrix with topics as rows and features (i.e. terms) as columns, \n",
    "# values are c-Tf-idf\n",
    "# 10/4/22: The updated BERTopic refer to the c_tf_idf matrix differently with an\n",
    "#   extra '_' at the end\n",
    "#topic_term_matrix = topic_model.c_tf_idf\n",
    "topic_term_matrix = topic_model.c_tf_idf_\n",
    "type(topic_term_matrix), topic_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 58/58 [00:09<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# A list of features (terms)\n",
    "terms = topic_model.vectorizer_model.get_feature_names()\n",
    "\n",
    "# Get top 50 terms\n",
    "top_50 = {} # {topic:[top50_idx_list, top50_c-tf-idf_list, to50_feat_list]}\n",
    "\n",
    "# Skip the outlier topic, named the variable topic_plus1 because the topic\n",
    "# index is -1 from the index in the topic_term_marix.\n",
    "for topic_plus1 in tqdm(range(1, topic_term_matrix.shape[0])):\n",
    "  row     = topic_term_matrix.getrow(topic_plus1).toarray()[0].ravel()\n",
    "\n",
    "  # The following two lines sorted from low to high\n",
    "  t50_idx = list(row.argsort()[-50:])\n",
    "  t50_val = list(row[row.argsort()[-50:]])\n",
    "\n",
    "  t50_fea = [terms[i] for i in t50_idx]\n",
    "  top_50[topic_plus1-1] = [t50_idx, t50_val, t50_fea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the top 50 terms\n",
    "with open(top_50_terms_file, 'wb') as f:\n",
    "  pickle.dump(top_50, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = topic_model.generate_topic_labels(nr_words=10,\n",
    "                                                 topic_prefix=True,\n",
    "                                                 separator='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top terms for different topics into an xlsx file\n",
    "xlsx      = Workbook(top_50_terms_xlsx)\n",
    "\n",
    "# Do not output outlier\n",
    "topic_label_df = pd.DataFrame(topic_labels[1:])\n",
    "topic_label_df.columns = [\"label\"]\n",
    "topic_label_df.to_csv(topic_label_file, sep='\\t')\n",
    "\n",
    "worksheet = xlsx.add_worksheet(\"topic_label\")\n",
    "read_tsv = csv.reader(open(topic_label_file,'r',encoding='utf-8'),delimiter='\\t')\n",
    "for row, data in enumerate(read_tsv):\n",
    "  worksheet.write_row(row, 0, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incude the representative doc in a worksheet\n",
    "rep_docs    = topic_model.get_representative_docs()\n",
    "rep_docs_df = pd.DataFrame.from_dict(rep_docs, orient='index',\n",
    "                                  columns=['doc1', 'doc2', 'doc3'])\n",
    "rep_docs_df.to_csv(rep_docs_file, sep='\\t')\n",
    "\n",
    "worksheet = xlsx.add_worksheet(\"representative docs\")\n",
    "read_tsv = csv.reader(open(rep_docs_file,'r',encoding='utf-8'),delimiter='\\t')\n",
    "for row, data in enumerate(read_tsv):\n",
    "  worksheet.write_row(row, 0, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the top 50 term info into different tsv files in the top_50 folder\n",
    "# Output individual tsv files and put tsv into xlsx\n",
    "for topic in top_50:\n",
    "  topic_file = top_50_dir / f\"topic_{topic}.tsv\"\n",
    "  # The nested list has index, c-tf-idf, and feature as rows. So it is transposed\n",
    "  # to have the rows as columns. The iloc bit is to reverse the order so higher\n",
    "  # c-tf-idf entries are on top.\n",
    "  topic_df = pd.DataFrame(top_50[topic]).transpose().iloc[::-1]\n",
    "  topic_df.columns = [\"index\", \"c-tf-idf\", \"feature\"]\n",
    "  topic_df.to_csv(topic_file, sep='\\t')\n",
    "\n",
    "  # Save to xlsx\n",
    "  worksheet = xlsx.add_worksheet(f\"{topic}\")\n",
    "  read_tsv  = csv.reader(open(topic_file, 'r',encoding='utf-8'),delimiter='\\t')\n",
    "  for row, data in enumerate(read_tsv):\n",
    "    worksheet.write_row(row, 0, data)\n",
    "\n",
    "xlsx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Records per year and timestamp bins___\n",
    "\n",
    "Code modified from `script_4_4_topic_over_time.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot records per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get pandas series dates\n",
    "dates = corpus_target['Date']\n",
    "\n",
    "ts_for_plot = [] # timestamps for plotting\n",
    "docs_yr     = [] # a list of years for generating bar graph later\n",
    "early_doc   = {} # {year: count}\n",
    "for date in dates:\n",
    "  [yr, mo, da] = date.split('-') # year, month, day\n",
    "  # docs from the same year has the same timestamp\n",
    "  yr = int(yr)\n",
    "  docs_yr.append(yr)\n",
    "\n",
    "  # Create \n",
    "  if yr not in early_doc:\n",
    "    early_doc[yr] = 1\n",
    "  else:\n",
    "    early_doc[yr]+= 1\n",
    "\n",
    "  # Set year before 1970 as 1969.\n",
    "  if yr < 1970:  \n",
    "    yr = 1969\n",
    "\n",
    "  # Set year after 2020 as 2020. There are 58 instances\n",
    "  if yr > 2020:  \n",
    "    yr = 2020\n",
    "\n",
    "  dt   = datetime(yr, 1, 1)\n",
    "  ts   = dt.timestamp()\n",
    "  ts_for_plot.append(ts)\n",
    "\n",
    "# array with number of records per year\n",
    "docs_year_array = np.unique(np.array(docs_yr), return_counts=True)\n",
    "\n",
    "# dataframe with counts per year\n",
    "docs_year_df    = pd.DataFrame(docs_year_array[1], columns=[\"Count\"],\n",
    "                               index=docs_year_array[0])\n",
    "\n",
    "# without log\n",
    "plt.figure(figsize=(6,10))\n",
    "plt.barh(docs_year_array[0], docs_year_array[1])\n",
    "plt.ylabel('Year')\n",
    "plt.xlabel('Numbers of documents')\n",
    "plt.yticks(range(1920, 2021, 5))\n",
    "plt.savefig(work_dir / f'figure6_num_docs_per_year_{target}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logged\n",
    "plt.figure(figsize=(6,10))\n",
    "plt.barh(docs_year_array[0], np.log10(docs_year_array[1]+1))\n",
    "plt.ylabel('Year')\n",
    "\n",
    "# +1 so the x-axis value is not zero.\n",
    "plt.xlabel('log10(Numbers of documents + 1)')\n",
    "plt.yticks(range(1920, 2021, 5))\n",
    "plt.savefig(work_dir / 'figure6_num_docs_per_year_{target}_logged.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the binned timestamp values\n",
    "\n",
    "Divide into 20 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all dates into timestamps \n",
    "ts_for_bins = []\n",
    "for date in dates:\n",
    "  [yr, mo, da] = date.split('-') # year, month, day\n",
    "  dt   = datetime(int(yr), int(mo), int(da))\n",
    "  ts   = dt.timestamp()\n",
    "  ts_for_bins.append(ts)\n",
    "  \n",
    "ts_for_bins.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin size\n",
    "bin_num  = 20\n",
    "bin_size = int(len(ts_for_bins)/bin_num)\n",
    "\n",
    "# index values of every 1/bin_num*100 percentile (because the data is broken \n",
    "# into parts=bin_num)\n",
    "bin_idxs = [idx for idx in range(0, len(ts_for_bins), bin_size)]\n",
    "\n",
    "# timestamp values at bin_idxs\n",
    "bin_timestamps = [ts_for_bins[idx] for idx in bin_idxs]\n",
    "\n",
    "# Modify the last value to be the max timestamp value + 1. This is otherwise\n",
    "# because of the bin_size is rounded down the last value be smaller than the max\n",
    "# timestamp values. Also, +1 to the max value, otherwise, the last entries will\n",
    "# be in its own bin.\n",
    "max_timestamp      = max(ts_for_bins) + 1\n",
    "bin_timestamps[-1] = max_timestamp\n",
    "\n",
    "# dates correspond to the different timestamp\n",
    "bin_dates      = [datetime.fromtimestamp(ts) for ts in bin_timestamps]\n",
    "\n",
    "# Put idx, timestamp, and date into a dataframe and save it.\n",
    "bin_df         = pd.DataFrame(list(zip(bin_idxs, bin_timestamps, bin_dates)),\n",
    "            columns=['bin_start_idx', 'bin_start_timestamp', 'bin_start_date'])\n",
    "bin_df.to_csv(work_dir / f\"table6_bin_timestamp_date_{target}.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   bin_start_idx  bin_start_timestamp bin_start_date\n",
       " 0              0        -1.211148e+08     1966-03-01\n",
       " 1           2517         9.030672e+08     1998-08-14\n",
       " 2           5034         1.006405e+09     2001-11-22\n",
       " 3           7551         1.071551e+09     2003-12-16\n",
       " 4          10068         1.124942e+09     2005-08-25,\n",
       "     bin_start_idx  bin_start_timestamp      bin_start_date\n",
       " 16          40272         1.509509e+09 2017-11-01 00:00:00\n",
       " 17          42789         1.536034e+09 2018-09-04 00:00:00\n",
       " 18          45306         1.561176e+09 2019-06-22 00:00:00\n",
       " 19          47823         1.585714e+09 2020-04-01 00:00:00\n",
       " 20          50340         1.609477e+09 2021-01-01 00:00:01)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_df.head(), bin_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the binned timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50346"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign new timestamps based on the bin timestamp values\n",
    "ts_in_bins = []\n",
    "for date in dates:\n",
    "  [yr, mo, da] = date.split('-') # year, month, day\n",
    "  dt   = datetime(int(yr), int(mo), int(da))\n",
    "  ts   = dt.timestamp()\n",
    "\n",
    "  bin_idx = bisect(bin_timestamps, ts)\n",
    "\n",
    "  if bin_idx < len(bin_timestamps):\n",
    "    ts2     = bin_timestamps[bin_idx]\n",
    "  # Deal with the last bin\n",
    "  else:\n",
    "    ts2     = datetime(2022, 12, 31).timestamp()\n",
    "  ts_in_bins.append(ts2) \n",
    "\n",
    "len(ts_in_bins) # Expect 50346, the number of Arabidopsis docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Run topics_over_time___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs_clean\n",
    "topics           = topics\n",
    "timestamps       = ts_in_bins\n",
    "nr_bins          = 20\n",
    "global_tuning    = 0\n",
    "evolution_tuning = 1\n",
    "\n",
    "# Get the global (i.e., based on the whole timef rame) ctfidf values\n",
    "global_c_tf_idf = normalize(topic_model.c_tf_idf_, axis=1, norm='l1', copy=False)\n",
    "# Construct a documents dataframe\n",
    "documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics, \n",
    "                          \"Timestamps\": timestamps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, -1, dict, 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get topic names, ordered and then get their indices, 0-based\n",
    "all_topics = sorted(list(documents.Topic.unique()))\n",
    "all_topics_indices = {topic: index for index, topic in enumerate(all_topics)}\n",
    "\n",
    "# 53 topics, index=0 is the outlier\n",
    "len(all_topics), all_topics[0], type(all_topics_indices), all_topics_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Timestamps</th>\n",
       "      <th>Bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>somatic mosaicism plants special reference som...</td>\n",
       "      <td>1</td>\n",
       "      <td>903067199.0</td>\n",
       "      <td>(903067199.0, 903067201.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ growth arabidopsis thaliana embryos followin...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067199.0</td>\n",
       "      <td>(903067199.0, 903067201.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Topic   Timestamps  \\\n",
       "0  somatic mosaicism plants special reference som...      1  903067199.0   \n",
       "1  [ growth arabidopsis thaliana embryos followin...     -1  903067199.0   \n",
       "\n",
       "                         Bins  \n",
       "0  (903067199.0, 903067201.0]  \n",
       "1  (903067199.0, 903067201.0]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin values into discrete intervals\n",
    "ts_bins   = [] # a list of tuples showing the bin range (+/-1 of the unique val)\n",
    "for ts in timestamps:\n",
    "  ts_bins.append(pd.Interval(left=ts-1, right=ts+1))\n",
    "\n",
    "# Add the bin info as a new column in documents df\n",
    "documents[\"Bins\"] = ts_bins\n",
    "# Add timestamp column\n",
    "documents[\"Timestamps\"] = documents.apply(lambda row: row.Bins.left, 1)\n",
    "documents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.0306720e+08, 1.0064052e+09, 1.0715508e+09, 1.1249424e+09,\n",
       "       1.1708244e+09, 1.2120336e+09, 1.2517776e+09, 1.2876336e+09,\n",
       "       1.3179600e+09, 1.3488048e+09, 1.3765392e+09, 1.4023728e+09,\n",
       "       1.4294160e+09, 1.4564628e+09, 1.4830740e+09, 1.5095088e+09,\n",
       "       1.5360336e+09, 1.5611760e+09, 1.5857136e+09, 1.6094772e+09])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort documents in chronological order\n",
    "documents  = documents.sort_values(\"Timestamps\")\n",
    "\n",
    "# Unique timestamps\n",
    "ts_unique = documents.Timestamps.unique()\n",
    "ts_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>somatic mosaicism plants special reference som...</td>\n",
       "      <td>1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ growth arabidopsis thaliana embryos followin...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantitative dose-response growth development ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>expression antisense sense rna ankyrin repeat-...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flavonoid-specific staining arabidopsis thalia...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Topic   Timestamps\n",
       "0  somatic mosaicism plants special reference som...      1  903067200.0\n",
       "1  [ growth arabidopsis thaliana embryos followin...     -1  903067200.0\n",
       "2  quantitative dose-response growth development ...     -1  903067200.0\n",
       "3  expression antisense sense rna ankyrin repeat-...     -1  903067200.0\n",
       "4  flavonoid-specific staining arabidopsis thalia...     -1  903067200.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>somatic mosaicism plants special reference som...</td>\n",
       "      <td>1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>convergence abscisic acid , co2 , extracellula...</td>\n",
       "      <td>16</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>regulatory activity exerted saur-ac1 promoter ...</td>\n",
       "      <td>2</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>identification tobacco arabidopsis homologues ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>characterization three cdna species encoding p...</td>\n",
       "      <td>29</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>photoaffinity labeling arabidopsis thaliana pl...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>characterization arabidopsis lipoxygenase gene...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>novel blue light- abscisic acid-inducible gene...</td>\n",
       "      <td>52</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>selection arabidopsis cdnas partially correct ...</td>\n",
       "      <td>12</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>atmpks : gene family plant map kinases arabido...</td>\n",
       "      <td>-1</td>\n",
       "      <td>903067200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2511 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Document  Topic   Timestamps\n",
       "0     somatic mosaicism plants special reference som...      1  903067200.0\n",
       "1673  convergence abscisic acid , co2 , extracellula...     16  903067200.0\n",
       "1674  regulatory activity exerted saur-ac1 promoter ...      2  903067200.0\n",
       "1675  identification tobacco arabidopsis homologues ...     -1  903067200.0\n",
       "1676  characterization three cdna species encoding p...     29  903067200.0\n",
       "...                                                 ...    ...          ...\n",
       "831   photoaffinity labeling arabidopsis thaliana pl...     -1  903067200.0\n",
       "834   characterization arabidopsis lipoxygenase gene...     -1  903067200.0\n",
       "836   novel blue light- abscisic acid-inducible gene...     52  903067200.0\n",
       "856   selection arabidopsis cdnas partially correct ...     12  903067200.0\n",
       "829   atmpks : gene family plant map kinases arabido...     -1  903067200.0\n",
       "\n",
       "[2511 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = documents.Timestamps[0]\n",
    "documents.loc[documents.Timestamps == timestamp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run modified topics_over_time code\n",
    "\n",
    "evolution_tuning=1, global_tuning=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 3/20 [00:48<05:04, 17.90s/it]"
     ]
    }
   ],
   "source": [
    "# For each unique timestamp, create topic representations\n",
    "topics_over_time = []\n",
    "for index, timestamp in enumerate(tqdm(ts_unique)):\n",
    "  #print(index, timestamp)\n",
    "  # Calculate c-TF-IDF representation for a specific timestamp\n",
    "  selection = documents.loc[documents.Timestamps == timestamp, :]\n",
    "  \n",
    "  docs_per_toc = selection.groupby(['Topic'], as_index=False).agg(\n",
    "                                {'Document': ' '.join, \"Timestamps\": \"count\"})\n",
    "  c_tf_idf, words = topic_model._c_tf_idf(docs_per_toc, fit=False)\n",
    "  \n",
    "  if global_tuning or evolution_tuning:\n",
    "    c_tf_idf = normalize(c_tf_idf, axis=1, norm='l1', copy=False)\n",
    "\n",
    "  # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the \n",
    "  # c-TF-IDF matrix at timestamp t-1\n",
    "  if evolution_tuning and index != 0:\n",
    "    curr_toc         = sorted(list(docs_per_toc.Topic.values))\n",
    "    overlap_toc      = sorted(list(set(prev_toc).intersection(set(curr_toc))))\n",
    "    curr_overlap_idx = [curr_toc.index(topic) for topic in overlap_toc]\n",
    "    prev_overlap_idx = [prev_toc.index(topic) for topic in overlap_toc]\n",
    "\n",
    "    c_tf_idf.tolil()[curr_overlap_idx] = \\\n",
    "      ((c_tf_idf[curr_overlap_idx] + prev_c_tf_idf[prev_overlap_idx]) / 2.0).tolil()\n",
    "\n",
    "  # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation\n",
    "  # by simply taking the average of the two\n",
    "  #if global_tuning:\n",
    "  #  selected_topics = [all_topics_indices[topic] \\\n",
    "  #                                      for topic in docs_per_toc.Topic.values]\n",
    "  #  c_tf_idf = (global_c_tf_idf[selected_topics] + c_tf_idf) / 2.0\n",
    "\n",
    "  # Extract the words per topic\n",
    "  labels = sorted(list(docs_per_toc.Topic.unique()))\n",
    "  words_per_toc = topic_model._extract_words_per_topic(words, c_tf_idf, labels)\n",
    "  topic_frequency = pd.Series(docs_per_toc.Timestamps.values,\n",
    "                                index=docs_per_toc.Topic).to_dict()\n",
    "  # Fill dataframe with results\n",
    "  topics_at_timestamp = [(topic,\n",
    "                          \", \".join([words[0] for words in values][:5]),\n",
    "                          topic_frequency[topic],\n",
    "                          timestamp) for topic, values in words_per_toc.items()]\n",
    "  topics_over_time.extend(topics_at_timestamp)\n",
    "\n",
    "  if evolution_tuning:\n",
    "    prev_toc = sorted(list(docs_per_toc.Topic.values))\n",
    "    prev_c_tf_idf = c_tf_idf.copy()\n",
    "\n",
    "  # Save the word list if this is the 1st timestamp\n",
    "  if index == 0:\n",
    "    word_file = ctfidf_dir / f\"word_list_{len(words)}.pickle\"\n",
    "    with open(word_file, 'wb') as f:\n",
    "      pickle.dump(words, f)\n",
    "\n",
    "  # Save the ctfidf values\n",
    "  ctfidf_file = ctfidf_dir / f'ctfidf_{index}_no_globatune.pickle'\n",
    "  with open(ctfidf_file, \"wb\") as f:\n",
    "    pickle.dump(c_tf_idf, f)\n",
    "\n",
    "  # Save index and topic name for cross-referencing in ctfidf_file\n",
    "  toc_index_file = ctfidf_dir / f\"ctfidf_{index}_toc_index.tsv\"\n",
    "  docs_per_toc['Topic'].to_csv(toc_index_file, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = pd.DataFrame(topics_over_time, \n",
    "                          columns=[\"Topic\", \"Words\", \"Frequency\", \"Timestamp\"])\n",
    "tot_df_file = work_dir / \\\n",
    "                      f\"table6_topics_over_time_df_no_global_tune_{target}.tsv\"\n",
    "topics_over_time.to_csv(tot_df_file, sep='\\t')\n",
    "topics_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic over time plot\n",
    "\n",
    "Customize [BERTopic/bertopic/plotting/_topics_over_time.py](https://github.com/MaartenGr/BERTopic/blob/master/bertopic/plotting/_topics_over_time.py):\n",
    "- BERTopic version is 0.12.0, different from what I started with 0.9.4.\n",
    "- The function to modify is `visualize_topics_over_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tot_mod(topic_model, topics_over_time, topics=None, \n",
    "                      normalize_frequency=False, width=1000, height=400):\n",
    "\n",
    "  colors = [\"#56B4E9\", \"#E69F00\", \"#009E73\", \"#F0E442\", \"#D55E00\", \"#0072B2\", \n",
    "            \"#CC79A7\"]\n",
    "\n",
    "  # Select topics\n",
    "  if topics:\n",
    "    selected_topics = topics\n",
    "  else:\n",
    "    selected_topics = topic_model.get_topic_freq().Topic.values\n",
    "\n",
    "  # Prepare data\n",
    "  # topic_model.topic_names is now topic_model.topic_labels_\n",
    "  topic_names = {key: value[:40] + \"...\" if len(value) > 40 else value\n",
    "                       for key, value in topic_model.topic_labels_.items()}\n",
    "  topics_over_time[\"Name\"] = topics_over_time.Topic.map(topic_names)\n",
    "  data = topics_over_time.loc[\n",
    "    topics_over_time.Topic.isin(selected_topics), :].sort_values(\n",
    "        [\"Topic\", \"Timestamp\"])\n",
    "    \n",
    "  # Add traces\n",
    "  fig = go.Figure()\n",
    "  max_freq = 0  # set max frequency for plotting purpose\n",
    "  for index, topic in enumerate(data.Topic.unique()):\n",
    "    trace_data = data.loc[data.Topic == topic, :]\n",
    "    topic_name = trace_data.Name.values[0]\n",
    "    words = trace_data.Words.values\n",
    "    if normalize_frequency:\n",
    "      y = normalize(trace_data.Frequency.values.reshape(1, -1))[0]\n",
    "    else:\n",
    "      y = trace_data.Frequency\n",
    "\n",
    "    if max(y) > max_freq:\n",
    "      max_freq = max(y)\n",
    "    fig.add_trace(go.Scatter(x=trace_data.Timestamp, y=y, mode='lines+markers',\n",
    "      marker_color=colors[index % 7], hoverinfo=\"text\",\n",
    "      name=topic_name,\n",
    "      hovertext=[f'<b>Topic {topic}</b><br>Words: {word}' for word in words]))\n",
    "\n",
    "  # Set the xaxis values, just use topic=0 \n",
    "  unique_tss = data.loc[data.Topic == selected_topics[0], :].Timestamp.tolist()\n",
    "  datetimes  = [datetime.fromtimestamp(ts) for ts in unique_tss]\n",
    "  ymds       = [f'{dt.year}-{dt.month}-{dt.day}' for dt in datetimes]\n",
    "  \n",
    "  # At recent date it becomes too dense, so only show every 2 dates\n",
    "  skip_after = datetime(2011, 1, 1).timestamp()\n",
    "  skip_start_idx = bisect(unique_tss, skip_after)\n",
    "  unique_tss2 = unique_tss[:skip_start_idx] + unique_tss[skip_start_idx::2]\n",
    "  ymds2       = ymds[:skip_start_idx] + ymds[skip_start_idx::2]\n",
    "\n",
    "  # Styling of the visualization\n",
    "  toc_str    = \"-\".join([str(toc) for toc in topics])\n",
    "\n",
    "  fig.update_xaxes(showgrid=True)\n",
    "  fig.update_yaxes(showgrid=True)\n",
    "  fig.update_layout(\n",
    "    yaxis_title=\"Normalized Frequency\" if normalize_frequency else \"Frequency\",\n",
    "    title={'text': f\"<b>Topic(s): {toc_str}\",\n",
    "           'y': .95,\n",
    "           'x': 0.40,\n",
    "           'xanchor': 'center',\n",
    "           'yanchor': 'top',\n",
    "           'font': dict(size=22, color=\"Black\")},\n",
    "    template=\"simple_white\",\n",
    "    width=width,\n",
    "    height=height,\n",
    "    hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"),\n",
    "    legend=dict(title=\"<b>Global Topic Representation\"),\n",
    "    xaxis=dict(tickmode='array', tickvals=unique_tss2, ticktext=ymds2,\n",
    "               tickangle=-60))\n",
    "\n",
    "  # Add decade lines\n",
    "  for decade in range(1980, 2020, 10):\n",
    "    dt = datetime(decade, 1, 1)\n",
    "    ts = datetime.timestamp(dt)\n",
    "    fig.add_shape(type=\"line\", x0=ts, y0=0, x1=ts, y1=max_freq*1.05,\n",
    "                  line=dict(color=\"red\", width=3, dash='dot'))\n",
    "\n",
    "  # Save figure\n",
    "  fig.write_html(tot_graph_dir / f'tot_graph_topic_{toc_str}_no_globaltune.html')\n",
    "  fig.write_image(tot_graph_dir / f'tot_graph_topic_{toc_str}_no_globaltune.pdf')\n",
    "\n",
    "  return fig, unique_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +2 because zero-indexing and the 1st topic is the outlier (-1)\n",
    "num_topics = np.max(topics_over_time[\"Topic\"]) + 2\n",
    "\n",
    "# Go through all topics\n",
    "for topic in tqdm(range(0, num_topics)):\n",
    "  visualize_tot_mod(topic_model, topics_over_time, topics=[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic over time heatmap\n",
    "\n",
    "Codes lifted from `script_5_1_species_over_time.ipynb`\n",
    "- Ordered based on moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bounds_and_xtick(topX):\n",
    "  # Set heatmap x-axis\n",
    "  ts_begin = datetime(1971, 1, 1).timestamp()\n",
    "  boundsX  = np.insert(ts_unique, 0, ts_begin)\n",
    "\n",
    "  # Set heatmap y-axis\n",
    "  toc_nooutlier = np.arange(topX) \n",
    "  midpointsY = (toc_nooutlier[:-1] + toc_nooutlier[1:]) / 2\n",
    "  boundsY    = np.concatenate([[2*midpointsY[0]-midpointsY[1]], \n",
    "                              midpointsY, \n",
    "                              [2*midpointsY[-1]-midpointsY[-2]]])\n",
    "\n",
    "  # xtick labels\n",
    "  ts_unique_dts = [datetime.fromtimestamp(ts) for ts in ts_unique]\n",
    "  ts_unique_dts = [f\"{dt.year}-{dt.month}-{dt.day}\" for dt in ts_unique_dts]\n",
    "  xticklabels   = [\"1917-1-1\"] + ts_unique_dts\n",
    "\n",
    "  return boundsX, boundsY, xticklabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg_reorder_and_plot(values, step_size, level_offspr, level_parent, \n",
    "                                topX):\n",
    "  dict_max_val_ts = {} # {1st_timestamp_in_window:{max_value:[taxa]}}\n",
    "\n",
    "  #print(\"In moving_avg_reorder_and_plot:\", values.shape)\n",
    "  count = 0\n",
    "  for taxa in values.index:\n",
    "    # series for genus\n",
    "    val_series = values.loc[taxa]\n",
    "    val_max    = 0 # for setting value maximum\n",
    "    val_max_ts = 0 # for setting last_timestamp_in_window for value maximum\n",
    "\n",
    "    # Go through steps\n",
    "    for idx in range(0, val_series.shape[0], step_size):\n",
    "      vals     = val_series.iloc[idx:idx+step_size]\n",
    "      val_mean = np.mean(vals)\n",
    "      # new max found, set values\n",
    "      if val_mean > val_max:\n",
    "        val_max    = val_mean\n",
    "        val_max_ts = vals.index[0]\n",
    "      #print(val_max, val_max_ts)\n",
    "    \n",
    "    # set values in dict_max_val_ts\n",
    "    if val_max_ts not in dict_max_val_ts:\n",
    "      dict_max_val_ts[val_max_ts] = {val_max:[taxa]}\n",
    "    elif val_max not in dict_max_val_ts[val_max_ts]:\n",
    "      dict_max_val_ts[val_max_ts][val_max] = [taxa]\n",
    "    # for topx=100, step_size=2, one val_max is the same\n",
    "    else:\n",
    "      print(\"Same val_max:\", val_max_ts, val_max, taxa)\n",
    "      dict_max_val_ts[val_max_ts][val_max].append(taxa)\n",
    "\n",
    "  # Get the new yticklabels based on the ordered ts_max_val\n",
    "  # was doing list(dict_max_val_ts.keys()), but this gives:\n",
    "  #  TypeError: list indices must be integers or slices, not float\n",
    "  max_val_ts   = [key for key in dict_max_val_ts.keys()]\n",
    "  max_val_ts.sort()\n",
    "  max_val_ts.reverse()\n",
    "  new_yticks = []\n",
    "  for ts in max_val_ts:\n",
    "    #print(ts)\n",
    "    val_maxs = [key for key in dict_max_val_ts[ts].keys()]\n",
    "    val_maxs.sort()\n",
    "    for val_max in val_maxs:\n",
    "      #print(\"\", val_max, len(dict_max_val_ts[ts][val_max]))\n",
    "      #print(ts, val_max, dict_max_val_ts[ts][val_max])\n",
    "      for taxa in dict_max_val_ts[ts][val_max]:\n",
    "        new_yticks.append(taxa)\n",
    "  #print(len(new_yticks))\n",
    "\n",
    "  # Reorder dataframe\n",
    "  values_reordered = values.reindex(new_yticks)\n",
    "  #print(\"after reordering:\", values_reordered.shape)\n",
    "\n",
    "  # set bounts and xticks\n",
    "  boundsX, boundsY, xticklabels = set_bounds_and_xtick(topX)\n",
    "\n",
    "  plot_scaled_heatmap(boundsX, boundsY, xticklabels, new_yticks, \n",
    "    values_reordered, f\"window{step_size}\", level_offspr, level_parent, topX)\n",
    "\n",
    "  return new_yticks, values_reordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bertopic': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2e4e20fc18a85a340f79bd4194edb89aa809195ae81b8baa70bd7972a2a12f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
