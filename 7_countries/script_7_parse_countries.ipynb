{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Step 7: Get country info__\n",
    "\n",
    "Goal\n",
    "- Get country info out of each doc\n",
    "- Get # of docs per country\n",
    "- Get # of docs per continent\n",
    "- Get # of docs per country over time\n",
    "- Get # of docs per country per topic\n",
    "- Get # of docs per country per topic over time\n",
    "\n",
    "Issues:\n",
    "- 2/20/23: \n",
    "  - The corpus dataset from 2_5_predict_pubmed does not have author or affiliation info. This needs to be done from the very beginning when I process the pubmed records.\n",
    "  - In \n",
    "[MEDLINE/PubMed Data Element (Field) Descriptions](https://www.nlm.nih.gov/bsd/mms/medlineelements.html), there are several important info:\n",
    "    - The affiliation of the authors, corporate authors and investigators appear in this repeating field.\n",
    "      - 1988- The address of the first author's affiliation is included. The institution, city, and state including zip code for U.S. addresses, and country for countries outside of the United States, are included if provided in the journal; sometimes the street address is also included if provided in the journal.\n",
    "      - 1995-2013 The designation USA is added at the end of the address when the first author's affiliation is in the fifty United States or the District of Columbia.\n",
    "        - Q: Does this mean that this is not done for records before 1995?\n",
    "      - 1996- The primary author's electronic mail (e-mail) address is included at the end of the Affiliation field, if present in the journal.\n",
    "      - 2003- The complete first author address is entered as it appears in the article with no words omitted.\n",
    "      - October 2013- Quality control of this field ceased in order to accommodate the affiliations for all authors and contributors.\n",
    "      - December 2014- Multiple affiliations for each author or contributor are included.\n",
    "        - __Because of this, only 1st author info is considered.__\n",
    "  - For dealing with countries, there is the issue of historical country names, see [ISSO_3166-3](https://en.wikipedia.org/wiki/ISO_3166-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Set up___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom scipy.sparse import csr_matrix, lil_matrix, coo_matrix, dok_matrix\\nfrom time import time\\nfrom datetime import datetime\\nfrom dateutil.relativedelta import relativedelta\\nfrom collections import OrderedDict, Counter\\nfrom bisect import bisect\\nfrom mlxtend.preprocessing import minmax_scaling\\nfrom copy import deepcopy\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle, nltk, re, multiprocessing, pycountry, zipcodes, us\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from Bio import Entrez, Medline\n",
    "from time import sleep\n",
    "from uszipcode import SearchEngine\n",
    "\n",
    "'''\n",
    "from scipy.sparse import csr_matrix, lil_matrix, coo_matrix, dok_matrix\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import OrderedDict, Counter\n",
    "from bisect import bisect\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from copy import deepcopy\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 20220609\n",
    "\n",
    "# Setting working directory\n",
    "proj_dir   = Path.home() / \"projects/plant_sci_hist\"\n",
    "work_dir   = proj_dir / \"7_countries\"\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# plant science corpus with date and other info\n",
    "dir2        = proj_dir / \"2_text_classify//2_5_predict_pubmed\"\n",
    "corpus_file = dir2 / \"corpus_plant_421658.tsv.gz\"\n",
    "\n",
    "# timestamp bins\n",
    "dir44            = proj_dir / \"4_topic_model/4_4_over_time\"\n",
    "ts_for_bins_file = dir44 / \"table4_4_bin_timestamp_date.tsv\"\n",
    "\n",
    "# So PDF is saved in a format properly\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Get PubMed records___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read plant science corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(corpus_file, compression='gzip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PMID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>QualifiedName</th>\n",
       "      <th>txt</th>\n",
       "      <th>reg_article</th>\n",
       "      <th>y_prob</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>1975-12-11</td>\n",
       "      <td>Biochimica et biophysica acta</td>\n",
       "      <td>Identification of the 120 mus phase in the dec...</td>\n",
       "      <td>After a 500 mus laser flash a 120 mus phase in...</td>\n",
       "      <td>spinach</td>\n",
       "      <td>Identification of the 120 mus phase in the dec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>1975-11-20</td>\n",
       "      <td>Biochimica et biophysica acta</td>\n",
       "      <td>Cholinesterases from plant tissues. VI. Prelim...</td>\n",
       "      <td>Enzymes capable of hydrolyzing esters of thioc...</td>\n",
       "      <td>plant</td>\n",
       "      <td>Cholinesterases from plant tissues. VI. Prelim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PMID        Date                        Journal  \\\n",
       "0           3    61  1975-12-11  Biochimica et biophysica acta   \n",
       "1           4    67  1975-11-20  Biochimica et biophysica acta   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Identification of the 120 mus phase in the dec...   \n",
       "1  Cholinesterases from plant tissues. VI. Prelim...   \n",
       "\n",
       "                                            Abstract QualifiedName  \\\n",
       "0  After a 500 mus laser flash a 120 mus phase in...       spinach   \n",
       "1  Enzymes capable of hydrolyzing esters of thioc...         plant   \n",
       "\n",
       "                                                 txt  reg_article    y_prob  \\\n",
       "0  Identification of the 120 mus phase in the dec...            1  0.716394   \n",
       "1  Cholinesterases from plant tissues. VI. Prelim...            1  0.894874   \n",
       "\n",
       "   y_pred  \n",
       "0       1  \n",
       "1       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421658,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all PMIDs\n",
    "pmids = corpus.PMID.values\n",
    "pmids.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pubmed docs using PMIDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59267992/biopython-how-to-download-all-of-the-peptide-sequences-or-all-records-associat\n",
    "\n",
    "Entrez.email = 'shius@msu.edu'\n",
    "\n",
    "id_list  = [str(pmid) for pmid in pmids]\n",
    "post_xml = Entrez.epost(db='pubmed', id=','.join(id_list))\n",
    "results  = Entrez.read(post_xml)\n",
    "webenv   = results['WebEnv']\n",
    "qkey     = results['QueryKey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [10:20<00:00, 14.44s/it] \n"
     ]
    }
   ],
   "source": [
    "#http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec166\n",
    "\n",
    "step    = 10000\n",
    "for begin in tqdm(range(0, len(pmids), step)):\n",
    "  # first check if this file is present\n",
    "  medline_file = work_dir / f\"corpus_plant_421658_medline_{begin}.pickle\"\n",
    "  if medline_file.is_file():\n",
    "    continue\n",
    "\n",
    "  # file does not exist\n",
    "  subset   = pmids[begin:begin+step]\n",
    "\n",
    "  # Get Medline records for subset\n",
    "  handle  = Entrez.efetch(db='pubmed', id=subset, rettype='medline', \n",
    "                          retmode='text', webenv=webenv, query_key=qkey)\n",
    "  records  = Medline.parse(handle)\n",
    "  rec_list = list(records)\n",
    "\n",
    "  with open(medline_file, \"wb\") as f:\n",
    "    pickle.dump(rec_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process PubMed Medline docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:36<00:00,  1.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "421585"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read individuial pickle files and compile the full list\n",
    "all_rec = []\n",
    "for begin in tqdm(range(0, len(pmids), step)):\n",
    "  medline_file = work_dir / f\"corpus_plant_421658_medline_{begin}.pickle\"\n",
    "  with open(medline_file, \"rb\") as f:\n",
    "    rec_list = pickle.load(f)\n",
    "  all_rec.extend(rec_list)\n",
    "\n",
    "# The number of docs don't add up. Some records are not downloaded\n",
    "len(all_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check what's missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go thorugh all downloaded docs and get PMIDs\n",
    "def check_missing(pmids, all_rec):\n",
    "  '''\n",
    "  Args:\n",
    "    pmids (list): list of integer PMIDs\n",
    "    all_rec (list): list of dictionary of medline records\n",
    "  Return:\n",
    "    id_list_missed (list): list of items in pmids but not all_rec\n",
    "  '''\n",
    "\n",
    "  # Downloaded\n",
    "  pmids_dn = []\n",
    "  for rec in tqdm(all_rec):\n",
    "    pmids_dn.append(int(rec['PMID']))\n",
    "  \n",
    "  # Compare lists\n",
    "  #https://stackoverflow.com/questions/15455737/python-use-set-to-find-the-different-items-in-list\n",
    "  print(\"differnce:\",len(pmids)-len(pmids_dn))\n",
    "\n",
    "  pmids_ori_set = set(pmids)\n",
    "  pmids_dn_set  = set(pmids_dn)\n",
    "  missing = pmids_ori_set - pmids_dn_set\n",
    "  print(\"# missing:\", len(missing))\n",
    "\n",
    "  id_list_missed = [str(pmid) for pmid in missing]\n",
    "\n",
    "  return id_list_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 421585/421585 [00:00<00:00, 1032346.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differnce: 73\n",
      "# missing: 72\n",
      "Retrieved: 41\n"
     ]
    }
   ],
   "source": [
    "# Get the missing records and add to all_rec\n",
    "id_list_missed = check_missing(pmids, all_rec)\n",
    "\n",
    "# Get Medline records for subset straight without epost\n",
    "handle  = Entrez.efetch(db='pubmed', id=id_list_missed, rettype='medline', \n",
    "                        retmode='text')\n",
    "records  = Medline.parse(handle)\n",
    "rec_list = list(records)\n",
    "\n",
    "# Can only get 41, so some still missing\n",
    "print(\"Retrieved:\", len(rec_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the missing records as pickle\n",
    "medline_file = work_dir / \"corpus_plant_421658_medline_missed.pickle\"\n",
    "\n",
    "with open(medline_file, \"wb\") as f:\n",
    "  pickle.dump(rec_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 421626/421626 [00:00<00:00, 1136620.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differnce: 32\n",
      "# missing: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['33292416',\n",
       " '33098629',\n",
       " '33082138',\n",
       " '31186333',\n",
       " '33292726',\n",
       " '32545091',\n",
       " '25764421',\n",
       " '25764422',\n",
       " '25764423',\n",
       " '25764424',\n",
       " '25764425',\n",
       " '25764426',\n",
       " '25764427',\n",
       " '25764428',\n",
       " '25764429',\n",
       " '32333389',\n",
       " '25764431',\n",
       " '25764432',\n",
       " '25764436',\n",
       " '25764437',\n",
       " '25764439',\n",
       " '33067356',\n",
       " '32764131',\n",
       " '33380717',\n",
       " '33380719',\n",
       " '25830899',\n",
       " '33380725',\n",
       " '33380726',\n",
       " '33380728',\n",
       " '17342585',\n",
       " '33064827']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add to all_rec, then check again\n",
    "all_rec.extend(rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 421626/421626 [00:00<00:00, 1164415.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differnce: 32\n",
      "# missing: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "still_missing = check_missing(pmids, all_rec)\n",
    "len(still_missing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Search for country codes___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up country dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build {country_name or official_name: alpha_3 code}\n",
    "countries   = list(pycountry.countries)\n",
    "cname_to_a3 = {}\n",
    "\n",
    "for country in countries:\n",
    "  name_a2    = country.alpha_2\n",
    "  name_a3    = country.alpha_3\n",
    "  name_short = country.name\n",
    "\n",
    "  cname_to_a3[name_a2] = name_a3 # store this for situation like US\n",
    "  cname_to_a3[name_a3] = name_a3 # store this for sitiation like USA\n",
    "  cname_to_a3[name_short] = name_a3\n",
    "  \n",
    "  # put official name in\n",
    "  try:\n",
    "    name_offic = country.official_name\n",
    "    cname_to_a3[name_offic] = name_a3\n",
    "  except AttributeError:\n",
    "    #print(\"No official name:\", name_short)\n",
    "    name_offic = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also build a dictionary for historical countries\n",
    "countries_hist = list(pycountry.historic_countries)\n",
    "cname_hist_to_a3 = {}\n",
    "\n",
    "for country in countries_hist:\n",
    "\n",
    "  # the name in historical countries are the official names\n",
    "  name_offic = country.name\n",
    "  cname_hist_to_a3[name_offic] = name_a3\n",
    "  \n",
    "  name_short = name_offic.split(\",\")[0]\n",
    "  cname_hist_to_a3[name_short] = name_a3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For weird stuff\n",
    "suppl_dict = {\"Academia Sinica\":\"China\", \"UK\":\"GBR\", \"The Netherlands\":\"NLD\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for country info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 421626/421626 [00:53<00:00, 7907.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No country code: 137886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Without country a3\n",
    "# Before checking for US state: 24867\n",
    "\n",
    "country_info = {} # {pmid:[first_AU, first_AD, alpha_3]}\n",
    "\n",
    "count = 0\n",
    "for rec in tqdm(all_rec):\n",
    "  pmid = rec[\"PMID\"]\n",
    "  a3   = \"NA\" # set default value\n",
    "\n",
    "  # Deal with AU info\n",
    "  try:\n",
    "    AU = rec[\"AU\"]\n",
    "  except KeyError:\n",
    "    AU = \"NA\"  \n",
    "\n",
    "  # Deal with AD info\n",
    "  try:\n",
    "    AD      = rec[\"AD\"]\n",
    "    tokens  = AD[0][:-1].split(\", \")\n",
    "\n",
    "    # The last token contain email address, not country\n",
    "    if len(tokens) < 2:\n",
    "      #print(tokens)\n",
    "      pass\n",
    "    elif tokens[-1].find(\"@\") != -1:\n",
    "      country = tokens[-2]\n",
    "    else:\n",
    "      country = tokens[-1]\n",
    "\n",
    "    # in current contru name\n",
    "    if country in cname_to_a3:\n",
    "      a3 = cname_to_a3[country]\n",
    "\n",
    "    # historical country name\n",
    "    elif country in cname_hist_to_a3:\n",
    "      a3 = cname_hist_to_a3[country]\n",
    "    # manually defined\n",
    "    elif country in suppl_dict:\n",
    "      a3 = cname_to_a3[suppl_dict[country]]\n",
    "    elif len(country.split(\" \")) == 2:\n",
    "      # Deal with those with \"city zip_code\" by checking if it is a US zip code\n",
    "      # e.g., Columbus 43210\n",
    "      #https://uszipcode.readthedocs.io/\n",
    "      [state, zip_code] = country.split(\" \")\n",
    "      if zip_code.find(\"-\") != -1:\n",
    "        zip_code = zip_code.split(\"-\")[0]\n",
    "\n",
    "      # check state\n",
    "      if us.states.lookup(state) is not None: \n",
    "        a3 = \"USA\"\n",
    "      # check zip\n",
    "      elif zip_code.isdigit():\n",
    "        # With this, somehow 02167 is not found, actually, go to USPS, this is\n",
    "        # not found either\n",
    "        sr = SearchEngine()\n",
    "        z = sr.by_zipcode(zip_code)\n",
    "        if z is not None:\n",
    "          a3 = \"USA\"\n",
    "      else:\n",
    "        # UNKNOWN THAT NEED TO BE DEALT WITH\n",
    "        #print(country)\n",
    "        #break\n",
    "        pass\n",
    "    else:\n",
    "      # UNKNOWN THAT NEED TO BE DEALT WITH\n",
    "      pass\n",
    "\n",
    "  except KeyError:\n",
    "    AD = \"NA\"\n",
    "\n",
    "  #if a3 == \"NA\" and AD != \"NA\":\n",
    "  #  print(AD)\n",
    "  #  break\n",
    "  if a3 == \"NA\": count += 1\n",
    "  country_info[pmid] = [AU, AD, a3]\n",
    "\n",
    "print(\"No country code:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Put country code info into corpusm___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get continent info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/55910004/get-continent-name-from-country-using-pycountry\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___Test___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country(alpha_2='AW', alpha_3='ABW', flag='ðŸ‡¦ðŸ‡¼', name='Aruba', numeric='533')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pycountry.countries)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Fouly HM', 'Domier LL', \"D'Arcy CJ\"],\n",
       " ['Department of Plant Pathology, University of Illinois, Urbana 61801.'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = all_rec[4004]\n",
    "au = rec[\"AU\"]\n",
    "ad = rec[\"AD\"]\n",
    "au, ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find a record for 'urbana'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pycountry\u001b[39m.\u001b[39;49msubdivisions\u001b[39m.\u001b[39;49mlookup(\u001b[39m\"\u001b[39;49m\u001b[39mUrbana\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/pycountry/db.py:39\u001b[0m, in \u001b[0;36mlazy_load.<locals>.load_if_needed\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_lock:\n\u001b[1;32m     38\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load()\n\u001b[0;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/pycountry/db.py:147\u001b[0m, in \u001b[0;36mDatabase.lookup\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m value:\n\u001b[1;32m    145\u001b[0m             \u001b[39mreturn\u001b[39;00m candidate\n\u001b[0;32m--> 147\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not find a record for \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m value)\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find a record for 'urbana'"
     ]
    }
   ],
   "source": [
    "pycountry.subdivisions.lookup(\"Urbana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "country = pycountry.countries.get(name=\"Yugoslavia\")\n",
    "print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "country = pycountry.historic_countries.get(name=\"Yugoslavia\")\n",
    "print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Country(alpha_2='AI', alpha_3='AFI', alpha_4='AIDJ', name='French Afars and Issas', numeric='262', withdrawal_date='1977'),\n",
       " Country(alpha_2='AN', alpha_3='ANT', alpha_4='ANHH', name='Netherlands Antilles', numeric='530', withdrawal_date='1993-07-12'),\n",
       " Country(alpha_2='BQ', alpha_3='ATB', alpha_4='BQAQ', name='British Antarctic Territory', withdrawal_date='1979'),\n",
       " Country(alpha_2='BU', alpha_3='BUR', alpha_4='BUMM', name='Burma, Socialist Republic of the Union of', numeric='104', withdrawal_date='1989-12-05'),\n",
       " Country(alpha_2='BY', alpha_3='BYS', alpha_4='BYAA', name='Byelorussian SSR Soviet Socialist Republic', numeric='112', withdrawal_date='1992-06-15'),\n",
       " Country(alpha_2='CS', alpha_3='CSK', alpha_4='CSHH', name='Czechoslovakia, Czechoslovak Socialist Republic', numeric='200', withdrawal_date='1993-06-15'),\n",
       " Country(alpha_2='CS', alpha_3='SCG', alpha_4='CSXX', name='Serbia and Montenegro', numeric='891', withdrawal_date='2006-06-05'),\n",
       " Country(alpha_2='CT', alpha_3='CTE', alpha_4='CTKI', name='Canton and Enderbury Islands', numeric='128', withdrawal_date='1984'),\n",
       " Country(alpha_2='DD', alpha_3='DDR', alpha_4='DDDE', name='German Democratic Republic', numeric='278', withdrawal_date='1990-10-30'),\n",
       " Country(alpha_2='DY', alpha_3='DHY', alpha_4='DYBJ', name='Dahomey', numeric='204', withdrawal_date='1977'),\n",
       " Country(alpha_2='FQ', alpha_3='ATF', alpha_4='FQHH', comment='now split between AQ and TF', name='French Southern and Antarctic Territories', withdrawal_date='1979'),\n",
       " Country(alpha_2='FX', alpha_3='FXX', alpha_4='FXFR', name='France, Metropolitan', numeric='249', withdrawal_date='1997-07-14'),\n",
       " Country(alpha_2='GE', alpha_3='GEL', alpha_4='GEHH', comment='now split into Kiribati and Tuvalu', name='Gilbert and Ellice Islands', numeric='296', withdrawal_date='1979'),\n",
       " Country(alpha_2='HV', alpha_3='HVO', alpha_4='HVBF', name='Upper Volta, Republic of', numeric='854', withdrawal_date='1984'),\n",
       " Country(alpha_2='JT', alpha_3='JTN', alpha_4='JTUM', name='Johnston Island', numeric='396', withdrawal_date='1986'),\n",
       " Country(alpha_2='MI', alpha_3='MID', alpha_4='MIUM', name='Midway Islands', numeric='488', withdrawal_date='1986'),\n",
       " Country(alpha_2='NH', alpha_3='NHB', alpha_4='NHVU', name='New Hebrides', numeric='548', withdrawal_date='1980'),\n",
       " Country(alpha_2='NQ', alpha_3='ATN', alpha_4='NQAQ', name='Dronning Maud Land', numeric='216', withdrawal_date='1983'),\n",
       " Country(alpha_2='NT', alpha_3='NTZ', alpha_4='NTHH', comment='formerly between Saudi Arabia and Iraq', name='Neutral Zone', numeric='536', withdrawal_date='1993-07-12'),\n",
       " Country(alpha_2='PC', alpha_3='PCI', alpha_4='PCHH', comment='divided into FM, MH, MP, and PW', name='Pacific Islands (trust territory)', numeric='582', withdrawal_date='1986'),\n",
       " Country(alpha_2='PU', alpha_3='PUS', alpha_4='PUUM', name='US Miscellaneous Pacific Islands', numeric='849', withdrawal_date='1986'),\n",
       " Country(alpha_2='PZ', alpha_3='PCZ', alpha_4='PZPA', name='Panama Canal Zone', withdrawal_date='1980'),\n",
       " Country(alpha_2='RH', alpha_3='RHO', alpha_4='RHZW', name='Southern Rhodesia', numeric='716', withdrawal_date='1980'),\n",
       " Country(alpha_2='SK', alpha_3='SKM', alpha_4='SKIN', name='Sikkim', withdrawal_date='1975'),\n",
       " Country(alpha_2='SU', alpha_3='SUN', alpha_4='SUHH', name='USSR, Union of Soviet Socialist Republics', numeric='810', withdrawal_date='1992-08-30'),\n",
       " Country(alpha_2='TP', alpha_3='TMP', alpha_4='TPTL', comment='was Portuguese Timor', name='East Timor', numeric='626', withdrawal_date='2002-05-20'),\n",
       " Country(alpha_2='VD', alpha_3='VDR', alpha_4='VDVN', name='Viet-Nam, Democratic Republic of', withdrawal_date='1977'),\n",
       " Country(alpha_2='WK', alpha_3='WAK', alpha_4='WKUM', name='Wake Island', numeric='872', withdrawal_date='1986'),\n",
       " Country(alpha_2='YD', alpha_3='YMD', alpha_4='YDYE', name=\"Yemen, Democratic, People's Democratic Republic of\", numeric='720', withdrawal_date='1990-08-14'),\n",
       " Country(alpha_2='YU', alpha_3='YUG', alpha_4='YUCS', name='Yugoslavia, Socialist Federal Republic of', numeric='891', withdrawal_date='1993-07-28'),\n",
       " Country(alpha_2='ZR', alpha_3='ZAR', alpha_4='ZRCD', name='Zaire, Republic of', numeric='180', withdrawal_date='1997-07-14')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pycountry.historic_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "sr = SearchEngine()\n",
    "z = sr.by_zipcode(\"02167\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "exact_zip = zipcodes.matching('02167')\n",
    "print(exact_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(us.states.lookup('bleh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8f2ce42de0a7f55eda8992f7abb5929e3699213a135804d69e10d69508f13d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
